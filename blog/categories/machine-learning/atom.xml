<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: machine learning | Reading Space]]></title>
  <link href="http://hongchaozhang.github.io/blog/categories/machine-learning/atom.xml" rel="self"/>
  <link href="http://hongchaozhang.github.io/"/>
  <updated>2020-02-21T11:58:48+08:00</updated>
  <id>http://hongchaozhang.github.io/</id>
  <author>
    <name><![CDATA[Zhang Hongchao]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[自然语言理解（NLU）综述]]></title>
    <link href="http://hongchaozhang.github.io/blog/2019/05/29/overview-of-nlp-and-nlu/"/>
    <updated>2019-05-29T17:27:58+08:00</updated>
    <id>http://hongchaozhang.github.io/blog/2019/05/29/overview-of-nlp-and-nlu</id>
    <content type="html"><![CDATA[<!-- more -->


<p>这篇总结有点乱，权当留作自己看。</p>

<ul>
<li><a href="#%E4%BA%BA%E6%9C%BA%E5%AF%B9%E8%AF%9D%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%A8%A1%E5%9E%8B">人机对话的两种模型</a></li>
<li><a href="#nlu%E4%B8%8Enlp">NLU与NLP</a>

<ul>
<li><a href="#stanford-corenlp">Stanford CoreNLP</a></li>
<li><a href="#ios-naturallanguage-framework">iOS <em>NaturalLanguage</em> framework</a>

<ul>
<li><a href="#custom-machine-learning-models">Custom Machine Learning Models</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%90%86%E8%A7%A3%E5%B9%B3%E5%8F%B0">自然语言理解平台</a></li>
<li><a href="#google%E7%9A%84dialogflow">Google的Dialogflow</a>

<ul>
<li><a href="#dialogflow%E7%9A%84%E5%8E%86%E5%8F%B2">Dialogflow的历史</a></li>
<li><a href="#dialogflow%E7%9A%84%E4%B8%80%E4%BA%9B%E6%A6%82%E5%BF%B5">Dialogflow的一些概念</a></li>
<li><a href="#%E4%B8%A4%E7%A7%8D%E6%96%B9%E6%B3%95%E5%AE%9E%E7%8E%B0intents%E4%B9%8B%E9%97%B4%E7%9A%84entities%E5%85%B1%E4%BA%AB">两种方法实现intents之间的entities共享</a></li>
<li><a href="#training-tab-in-dialogflow-platform">“Training” tab in Dialogflow platform</a></li>
<li><a href="#integrations-analytics-and-fulfillment">“Integrations”, “Analytics” and “Fulfillment”</a></li>
<li><a href="#prebuild-agent-and-small-talk">“Prebuild Agent” and “Small Talk”</a></li>
<li><a href="#%E4%B8%80%E4%BA%9B%E4%BD%BF%E7%94%A8dialogflow%E7%9A%84%E4%BE%8B%E5%AD%90">一些使用Dialogflow的例子</a></li>
</ul>
</li>
<li><a href="#alexa-and-lex">Alexa and Lex</a></li>
<li><a href="#microsoft-luis-text-analytics-api">Microsoft LUIS Text Analytics API</a></li>
<li><a href="#facebook%E7%9A%84witai">Facebook的wit.ai</a></li>
<li><a href="#snips">Snips</a>

<ul>
<li><a href="#duckling">Duckling</a></li>
</ul>
</li>
<li><a href="#ibm%E7%9A%84watson">IBM的Watson</a></li>
<li><a href="#swiftnlc">SwiftNLC</a>

<ul>
<li><a href="#xunfei-command-recognition">Xunfei Command Recognition</a></li>
</ul>
</li>
<li><a href="#siri-and-sirikit">Siri and SiriKit</a>

<ul>
<li><a href="#sirikit-build-in-domain">SiriKit build-in domain</a></li>
<li><a href="#sirikit-custom-intent">SiriKit custom intent</a></li>
</ul>
</li>
</ul>


<p><a id="markdown-人机对话的两种模型" name="人机对话的两种模型"></a></p>

<h2>人机对话的两种模型</h2>

<p>自然语言理解（NLU）的最终目的是让计算机能够理解人类的语言，实现人机对话。目前，人机对话模型基本上有两种：</p>

<ol>
<li>基于意图(Intent-based)的对话：这是当NLP算法使用intents和entities进行对话时，通过识别用户声明中的名词和动词，然后与它的dictionary交叉引用，让bot可以执行有效的操作。目前，大部分平台都是基于这种模式进行训练的，包括谷歌的Dialogflow。</li>
<li>基于流程(Flow-based)的对话：基于流程的对话是智能通信的下一个级别。在这里，我们会给予两个人之间对话的许多不同样本的RNN（循环神经网络），创建的机器人将根据你训练的ML模型进行响应。 wit.ai是在这个领域取得巨大进展的少数网站之一。但这个太超前，不是我们考虑使用的对象，接下来也不会讨论。</li>
</ol>


<p>详细讨论参考：<a href="https://chatbotsmagazine.com/bot-talks-intent-based-vs-flow-base-conversations-798788dc9cf6">Bot Talks: Intent-Based vs. Flow-Base Conversations</a>。</p>

<p><a id="markdown-nlu与nlp" name="nlu与nlp"></a></p>

<h2>NLU与NLP</h2>

<p>NLP（Natural Language Processing）是NLU（Natural Language Understanding）的一个前期步骤：NLP用于对文本或者语音进行机器学习训练和识别时的特征提取阶段。</p>

<p><a id="markdown-stanford-corenlp" name="stanford-corenlp"></a></p>

<h3>Stanford CoreNLP</h3>

<p>可以去<a href="http://nlp.stanford.edu:8080/corenlp/process">Stanford CoreNLP</a>试一下效果。使用界面可以看一下：</p>

<p><img src="/images/NLU_Stanford_CoreNLP.png" alt="stanford CoreNLP" /></p>

<p><a id="markdown-ios-naturallanguage-framework" name="ios-naturallanguage-framework"></a></p>

<h3>iOS <em>NaturalLanguage</em> framework</h3>

<p>iOS的<em>NaturalLanguage</em>框架可以做的事情如下：</p>

<p><img src="/images/NLU_AppleNLP.png" alt="Apple nlp framework" /></p>

<p><a id="markdown-custom-machine-learning-models" name="custom-machine-learning-models"></a></p>

<h4>Custom Machine Learning Models</h4>

<p>用户可以自己训练模型，用于下列事情：</p>

<p>NSLinguisticTagger:</p>

<ul>
<li>Language identification</li>
<li>Tokenization in Word/Sentence/Paragraph</li>
<li>Part of speech</li>
<li>Lemmatization</li>
<li>Named entity recognition</li>
</ul>


<p>With custom models:</p>

<ol>
<li>Text classification

<ul>
<li>Sentiment classification</li>
<li>Topic classification</li>
<li>Domain classification</li>
</ul>
</li>
<li>Word tagging

<ul>
<li>Part of speech</li>
<li>Named entity</li>
<li>Slot parsing

<ul>
<li>Chunking</li>
</ul>
</li>
<li></li>
</ul>
</li>
</ol>


<p><a id="markdown-自然语言理解平台" name="自然语言理解平台"></a></p>

<h2>自然语言理解平台</h2>

<p>一些比较有名气的自然语言理解平台：</p>

<ul>
<li>Facebook’s Wit.ai,</li>
<li>IBM Watson’s Conversation Service,</li>
<li>Microsoft’s Language Understanding and Intelligence Service or</li>
<li>Google NLP API</li>
</ul>


<p>Wit.ai joined Facebook on 2015.1.5</p>

<p>一些常见产品及其背后的支撑技术：</p>

<ul>
<li>Amazon: Echo &lt;- Alexa &lt;- Lex</li>
<li>Apple: iPhone &lt;- Siri &lt;- SiriKit</li>
<li>Google: Android phone &lt;- Google Asistant &lt;- Dialogflow</li>
<li>Microsoft: Windows phone &lt;- Cortana &lt;- Luis</li>
</ul>


<p>下面选择一些平台做简单介绍。</p>

<p><a id="markdown-google的dialogflow" name="google的dialogflow"></a></p>

<h2>Google的Dialogflow</h2>

<blockquote><p><a href="https://dialogflow.com/">Google的Dialogflow</a>Give users new ways to interact with your product by building engaging voice and text-based conversational interfaces, such as voice apps and chatbots, powered by AI. Connect with users on your website, mobile app, the Google Assistant, Amazon Alexa, Facebook Messenger, and other popular platforms and devices.</p></blockquote>

<p><a id="markdown-dialogflow的历史" name="dialogflow的历史"></a></p>

<h3>Dialogflow的历史</h3>

<p>Dialogflow就是Speaktoit公司的api.ai。</p>

<ul>
<li>2011: Speaktoit developed an intelligent personal assistant for mobile phones</li>
<li>2014: Speaktoit released api.ai</li>
<li>2016: Google buys Speaktoit to power Google Assistant</li>
<li>2017: api.ai is renamed to Dialogflow</li>
</ul>


<p><a id="markdown-dialogflow的一些概念" name="dialogflow的一些概念"></a></p>

<h3>Dialogflow的一些概念</h3>

<ul>
<li>Agents: 一套module包含dialogflow及自然語言理解使用者的語義後，執行整個動作 action. Ex:如上圖 TestAgent</li>
<li>Intents: 使用者的意圖。意圖由開發人員配置。</li>
<li>Entities:重要的關鍵字眼(我真的不知道怎麼翻好，Google說這個字叫做實體?) 從用戶口中所提到的重要的關鍵字眼轉換成重要的資訊，籍此提供給Intent。例如：“訂飛機” ：這句話中還需要 城市 日期 等資訊，來能完成訂飛機這個動作，所以 城市 和 日期 就是Entities.</li>
<li>Fulfilment: 程式撰寫的地方。例如 訂飛機 還得串飛機公司的API才有可能完成訂購，所以程式邏緝就是寫在這裡。</li>
<li>Integrations: LINE, Google home etc..</li>
<li>Prebuilt Agents: dialogflow幫你預先訓練好的Agent，你可以拿來用。</li>
<li>Smalltalk: 也是dialogflow幫你預先訓練好的Agent，幫助你的chatbot對話更友善.</li>
</ul>


<p><a id="markdown-两种方法实现intents之间的entities共享" name="两种方法实现intents之间的entities共享"></a></p>

<h3>两种方法实现intents之间的entities共享</h3>

<ul>
<li>Context: 在线性对话中，完成讯息在Intent中的传送。</li>
<li>Followup Intent</li>
</ul>


<blockquote><p>对话生命周期，就是这个参数可以存多久。</p></blockquote>

<p><a id="markdown-training-tab-in-dialogflow-platform" name="training-tab-in-dialogflow-platform"></a></p>

<h3>“Training” tab in Dialogflow platform</h3>

<blockquote><p><a href="https://console.dialogflow.com/api-client/#/agent/dea6f73c-7c22-44b6-a1a8-45cdcd160bfc/training">官方关于Training的说明</a>：你将收到所有发送给agent的回覆讯息以及agent回覆的内容，如果你告诉你的agent一些回应文本，但它回应你不喜欢的输出，这就非常有用，若你稍后意识到忘记了某个关键字的同义词，并且用户正在使用这个关键字，那么也可能会有所帮助，可以去告诉你的代理在这种情况下应该做什么。</p></blockquote>

<p><a id="markdown-integrations-analytics-and-fulfillment" name="integrations-analytics-and-fulfillment"></a></p>

<h3>“Integrations”, “Analytics” and “Fulfillment”</h3>

<blockquote><p>在Training下方，你可以看到Integrations。在这里，可以管理你的agent去串接不同的服务，例如Google Assistant，Twitter，Slack，Messenger，Cortana，Alexa等等。 Integrations之后，还有Analytics，基本上用来显示建议名称，之后还有Fulfillment，如果你要调用一个API并实现一个webhook，这就是你会需要来的地方。</p></blockquote>

<p><a id="markdown-prebuild-agent-and-small-talk" name="prebuild-agent-and-small-talk"></a></p>

<h3>“Prebuild Agent” and “Small Talk”</h3>

<p>最后两个选项功能非常简单，但很有用。第一个是Prebuilt Agents，在这里，你可以import一个预先存在的代理框架，有很多例子，如食物传递机器人，音乐机器人，甚至（抱歉，但你真的需要知道这个）hotel预订机器人！最后一个选项是Small Talk，如果你将代理设计为像Siri或Google Assistant这样的每日伙伴(daily companion)，这个选项非常有用，Small Talk允许你添加常见问题的答案，我们都喜欢问我们的机器人，如”你几岁？”或”你住哪里？”，以及更热门的问题”你愿意嫁给我吗？”</p>

<p><a id="markdown-一些使用dialogflow的例子" name="一些使用dialogflow的例子"></a></p>

<h3>一些使用Dialogflow的例子</h3>

<ol>
<li>林建宏的7篇文章：如何使用Dialogflow建立Chatbot #1-#7</li>
<li><a href="https://medium.com/@wolkesau/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8dialogflow%E5%BB%BA%E7%AB%8Bchatbot-1-%E4%BB%8B%E7%B4%B9-62736bcdad95">用Dialogflow建立LINE Chatbot #1 介紹</a></li>
<li><a href="https://github.com/appcoda/ChatbotHotel">A Demo for booking hotel based on Dialogflow</a></li>
<li><a href="https://www.appcoda.com.tw/chatbot-dialogflow-ios/">聊天機器人教學：使用Dialogflow (API.AI)開發 iOS Chatbot App</a></li>
</ol>


<p><a id="markdown-alexa-and-lex" name="alexa-and-lex"></a></p>

<h2>Alexa and Lex</h2>

<blockquote><p>Echo to Alexa as iPhone to Siri
Lex is whats inside Alexa
Lex is part of AWS, but Alexa isn&rsquo;t</p>

<p>Alexa is more focusing on communication using voice. Hence it has some special requirements for utterances. For example, the number is not supported in the utterances. You need to use &lsquo;Show me three elements&rsquo; instead of &lsquo;Show me 3 elements&rsquo;. For acronym, we need to use &lsquo;n.b.a&rsquo; instead of &lsquo;NBA&rsquo;.</p>

<p>Lex is a platform which can power bot who accept text input.</p>

<p>Amazon Lex 让您可以将语音和文本聊天访问集成到现有应用程序中。Amazon Alexa 允许您使用 Amazon Echo 或任何启用 Alexa Voice Service 的设备为家庭或工作场所的用户提供免提语音接口。</p></blockquote>

<p><a id="markdown-microsoft-luis-text-analytics-api" name="microsoft-luis-text-analytics-api"></a></p>

<h2>Microsoft LUIS Text Analytics API</h2>

<ol>
<li>Detect language</li>
<li>Analyze sentiment</li>
<li>Extract key phrases

<ul>
<li>使用方法参考<a href="https://docs.microsoft.com/en-us/azure/cognitive-services/text-analytics/quickstarts/python">Python demo</a>。</li>
</ul>
</li>
<li>Identify entities

<ul>
<li>使用方法参考<a href="https://docs.microsoft.com/en-us/azure/cognitive-services/text-analytics/quickstarts/python">Python demo</a>。</li>
</ul>
</li>
</ol>


<p><a id="markdown-facebook的witai" name="facebook的witai"></a></p>

<h2>Facebook的wit.ai</h2>

<p>参见<a href="https://wit.ai/">官方文档</a>。</p>

<p><a id="markdown-snips" name="snips"></a></p>

<h2>Snips</h2>

<p>Snips的设计流程还是非常好的 ，包含基于正则表达式的确定性识别和基于机器学习的可能性识别。一些资料：</p>

<ul>
<li><a href="https://medium.com/snips-ai/an-introduction-to-snips-nlu-the-open-source-library-behind-snips-embedded-voice-platform-b12b1a60a41a">Post</a></li>
<li><a href="https://snips-nlu.readthedocs.io/en/latest/index.html">Document</a></li>
<li><a href="https://github.com/snipsco/snips-platform-swift">GitHub for iOS app demo</a></li>
<li><a href="https://console.snips.ai/assistants/proj_ageaw4b4d83">Snips assistent generation</a></li>
</ul>


<p><a id="markdown-duckling" name="duckling"></a></p>

<h3>Duckling</h3>

<p>Snips使用<a href="https://github.com/facebook/duckling">Duckling</a>进行下面的理解：</p>

<p>输入："the first Tuesday of October"
输出：{&ldquo;value&rdquo;:&ldquo;2017-10-03T00:00:00.000-07:00&rdquo;,&ldquo;grain&rdquo;:&ldquo;day&rdquo;}</p>

<blockquote><p>Note:
* Spin your own Duckling server or using wit.ai’s build entities.
* <a href="https://medium.com/wit-ai/open-sourcing-duckling-our-probabilistic-date-parser-4351ee66c4ba">Open Sourcing Duckling, our probabilistic (date) parser</a></p></blockquote>

<p><a id="markdown-ibm的watson" name="ibm的watson"></a></p>

<h2>IBM的Watson</h2>

<p>参看<a href="https://www.ibm.com/watson/services/natural-language-classifier/">官方文档</a>。</p>

<p><a id="markdown-swiftnlc" name="swiftnlc"></a></p>

<h2>SwiftNLC</h2>

<ul>
<li><a href="https://chatbotsmagazine.com/coreml-nlc-with-keras-tensorflow-and-apple-nslinguistictagger-1659021ea8e5">Offline Intent Understanding: CoreML NLC with Keras/TensorFlow and Apple NSLinguisticTagger</a></li>
<li><a href="https://heartbeat.fritz.ai/implementing-a-natural-language-classifier-in-ios-with-keras-core-ml-358f114c0b51">Implementing a Natural Language Classifier in iOS with Keras + Core ML</a></li>
</ul>


<p><a id="markdown-xunfei-command-recognition" name="xunfei-command-recognition"></a></p>

<h3>Xunfei Command Recognition</h3>

<p>例如，开发一个简单的语音拨号应用，可定义如下语法：</p>

<pre><code>&amp;lt;commands&amp;gt;:(找一下|打电话给) &amp;lt;name&amp;gt;;
&amp;lt;name&amp;gt;: 张三|李四;
</code></pre>

<p>该语法使识别引擎可以支持以下说法：找一下张三 、打电话给张三 、找一下李四 、打电话给李四。
凡是用户说出这个范围中的任意一句话，均可以被识别系统识别。如果用户说的话不在上述范围中，识别系统可能拒绝识别。</p>

<p><a id="markdown-siri-and-sirikit" name="siri-and-sirikit"></a></p>

<h2>Siri and SiriKit</h2>

<p><a id="markdown-sirikit-build-in-domain" name="sirikit-build-in-domain"></a></p>

<h3>SiriKit build-in domain</h3>

<p>SiriKit支持的build-in的domain包括：</p>

<ul>
<li>Messaging</li>
<li>Lists and Notes</li>
<li>Workouts</li>
<li>Payments</li>
<li>VoIP Calling</li>
<li>Visual Codes</li>
<li>Photos</li>
<li>Ride Booking</li>
<li>Car Commands</li>
<li>CarPlay</li>
<li>Restaurant Reservations</li>
</ul>


<p>基于build-in的domain，可以不经过任何机器模型训练达到下面的效果：</p>

<pre><code>“Hey Siri, send a UnicornChat message”
“To whom?”
“Celestra”
“What do you want to say to Celestra?”
“Let’s add more sparkle transitions” 
</code></pre>

<p>当然也可以用机器学习模型做一些事情。</p>

<blockquote><p>Domain model can be trained and used through NLP framework.</p></blockquote>

<p>我们也可以帮助Siri识别自定义的词典（WWDC2017 228_making_great_sirikit_experiences)。可以支持两种自定义的词典：</p>

<ul>
<li>App vocabulary: known to all your users and unique to your app, supplied in the plist file.</li>
<li>User vocabulary: known only to some specific users, supplied at the runtime.</li>
</ul>


<p>但是，注意：</p>

<ul>
<li>Need to update the user vocabulary if some info changes.</li>
<li>Need to reset the user vocabulary if the user reset the app, or log out.</li>
</ul>


<p><a id="markdown-sirikit-custom-intent" name="sirikit-custom-intent"></a></p>

<h3>SiriKit custom intent</h3>

<p>SiriKit的custom intent只是用来实现Siri Shortcut的，不能携带参数。</p>

<blockquote><p>Custom intent can only be used as shortcut and NO parameters will be extracted from the voice command.</p></blockquote>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Offline Natural Language Understanding Engine on iOS]]></title>
    <link href="http://hongchaozhang.github.io/blog/2019/05/22/offline-natural-language-understanding-engine-on-ios/"/>
    <updated>2019-05-22T16:04:28+08:00</updated>
    <id>http://hongchaozhang.github.io/blog/2019/05/22/offline-natural-language-understanding-engine-on-ios</id>
    <content type="html"><![CDATA[<!-- more -->


<ul>
<li><a href="#objective">Objective</a>

<ul>
<li><a href="#what-is-a-good-nlu-engine">What is a good NLU engine</a>

<ul>
<li><a href="#deterministic-behavior">Deterministic behavior</a></li>
<li><a href="#generalization-power">Generalization power</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#design-and-workflow">Design and Workflow</a></li>
<li><a href="#deterministic-intent-parser">Deterministic Intent Parser</a></li>
<li><a href="#probabilistic-intent-parser">Probabilistic Intent Parser</a>

<ul>
<li><a href="#intent-classification">Intent Classification</a>

<ul>
<li><a href="#model-training">Model Training</a></li>
<li><a href="#model-usage">Model Usage</a></li>
</ul>
</li>
<li><a href="#slot-filling">Slot Filling</a>

<ul>
<li><a href="#model-training-1">Model Training</a></li>
<li><a href="#model-usage-1">Model Usage</a></li>
</ul>
</li>
<li><a href="#model-size">Model Size</a></li>
<li><a href="#problems-to-be-solved">Problems to Be Solved</a>

<ul>
<li><a href="#intent-classification-model-has-no-probability-output">Intent Classification Model Has No Probability Output</a></li>
<li><a href="#slot-filling-model-tagges-the-label-by-words-not-phrase">Slot Filling Model Tagges the Label by Words, not Phrase</a></li>
</ul>
</li>
</ul>
</li>
</ul>


<p><a id="markdown-objective" name="objective"></a></p>

<h2>Objective</h2>

<p><img src="/images/NLUObjective.png" alt="nlu objective" /></p>

<p>We want an NLU Engine to understand the normal text command on Mobile. We hope the engine can know the command&rsquo;s intent and the info the command needs to execute.</p>

<p>Currently, there are many NLU related tools, like Google Dialogflow, Amazon Lex, Facebook Wit.ai, Microsoft Luis. However, they are all online tools. Considering the privacy problem, we are trying to build our own offline NLU Engine.</p>

<p><a id="markdown-what-is-a-good-nlu-engine" name="what-is-a-good-nlu-engine"></a></p>

<h3>What is a good NLU engine</h3>

<p>Let’s start by looking at a simple example, and see what you would expect from a good NLU engine.</p>

<p>First, we need some examples to train the NLU engine. Consider the following dataset, used to train a simple weather assistant with a few query examples:</p>

<ul>
<li>Give me the weather for [tomorrow](date)</li>
<li>Show me the [Paris](location) weather for [Sunday](date)</li>
</ul>


<p><a id="markdown-deterministic-behavior" name="deterministic-behavior"></a></p>

<h4>Deterministic behavior</h4>

<p>The first thing you want is that all the examples you give to train the model are correctly supported by the engine. This makes the system predictable and easy to use: if a query is not correctly parsed, then add it to the dataset and it will work right away.</p>

<p><a id="markdown-generalization-power" name="generalization-power"></a></p>

<h4>Generalization power</h4>

<p>Having this deterministic behavior is great for robustness and predictability, but a powerful NLU engine also needs to have some generalization power. You want the system not only to recognize patterns provided in the training set, but also all the possible variations that come from speaking naturally. If we go back to the previous dataset, it is reasonable to expect the NLU engine to parse a query like: “What’s the weather in Beijing right now?” even though it is not one of the training examples.</p>

<p><a id="markdown-design-and-workflow" name="design-and-workflow"></a></p>

<h2>Design and Workflow</h2>

<p><img src="/images/NLUDesign.png" alt="nlu design" /></p>

<p>In order to satisfy these objectives: deterministic behavior and generalization power, we built the processing pipeline described in the figure above. It receives a text as input, and outputs a structured response containing the intent and the list of slots. The main processing unit of the pipeline is the NLU engine. It contains two intent parsers which are called successively: a deterministic intent parser and a probabilistic one.</p>

<p>The deterministic parser relies on regular expressions to match intent and slots, which results in perfect behavior on training examples but doesn’t generalize. This parser is the first to be used because of its strictness.</p>

<p>The probabilistic parser is used whenever the first parser fails to find a match. It uses machine learning to generalize beyond the set of sentences seen at train time, thus making our NLU engine be able to cope with examples which are not in the scope of the training data set. This parser involves two successive steps: intent classification and slot filling. These two steps rely on trained machine learning models to classify intent and extract slots.</p>

<p><a id="markdown-deterministic-intent-parser" name="deterministic-intent-parser"></a></p>

<h2>Deterministic Intent Parser</h2>

<p>The Deterministic Intent Parse is the first step to be used. This parser relies on some regular expressions to match the intent and slots. If the new input has the same structure with one of the training examples, we will find its intent and slots by comparing the input with the matched regular expression.</p>

<p>The regular expressions are built based on the training examples. For a training case:</p>

<ul>
<li>What is the weather in [Alaska](location)</li>
</ul>


<p>We will build a regular expression:</p>

<ul>
<li>(what is the weather in)(?&lt;location1&gt;.+)</li>
</ul>


<p><a id="markdown-probabilistic-intent-parser" name="probabilistic-intent-parser"></a></p>

<h2>Probabilistic Intent Parser</h2>

<p>If the Deterministic Intent Parser fails to find the intent and slots, the Probabilistic Intent Parser will be used.</p>

<p>The Probabilistic Intent Parser has two steps:</p>

<ul>
<li>Intent Classification</li>
<li>Slot Filling</li>
</ul>


<p>The Intent Classification is to find the intent of the input command text, and the Slot Filling is to extract all the slots needed by the intent. These two steps are both based on trained machine models.</p>

<p>Apple has released CreateML for training natural language models, which also integrates the powerful NatrualLanguage framework functions, like Tokenization, Part of Speech, Lemmatization, Name Entity Recognition, etc. This will make the training process very simple, and the trained model will be more accurate and smaller.</p>

<p><a id="markdown-intent-classification" name="intent-classification"></a></p>

<h3>Intent Classification</h3>

<p><a id="markdown-model-training" name="model-training"></a></p>

<h4>Model Training</h4>

<p>For Intent Classification model training, we prepare the data set as follows (The size of the training data is 3282 falling into four intents.):</p>

<pre><code class="json">[
  {
    "text": "I would like the forecast in cupertino california  tomorrow", 
    "label": "searchWeatherForecast"
  }, 
  {
    "text": "Forecast in Maine USA next week", 
    "label": "searchWeatherForecast"
  }, 
...
...
  {
    "text": "Will I be able to wear open-toed shoes twenty three hours and seven minutes from now in Severn?", 
    "label": "searchWeatherForecastItem"
  }, 
  {
    "text": "Should I bring a raincoat to the Belgrade and Loreto areas of Oman at midnight?", 
    "label": "searchWeatherForecastItem"
  }, 
...
...
]
</code></pre>

<p>Apple has release CreateML framework for training machine learning models easily inside Swift playground and the trained model can be saved as mlmodel type. And the MLTextClassifier class from CreateML will benefit from Apple&rsquo;s NatrualLanguage framework for Tokenization, Part of Speech, Lemmatization, etc.</p>

<p>The training script is:</p>

<pre><code class="swift">let trainingDataPath = Bundle.main.path(forResource: "intentClassificationFile", ofType: "json", inDirectory: "Data/text/train")!
let trainingData = try! MLDataTable(contentsOf:  URL(fileURLWithPath: trainingDataPath))

// Initializing the classifier with a training data.
let classifier = try! MLTextClassifier(trainingData: trainingData, textColumn: "text", labelColumn: "label")

// Evaluating training &amp; validation accuracies.
let trainingAccuracy = (1.0 - classifier.trainingMetrics.classificationError) * 100
let validationAccuracy = (1.0 - classifier.validationMetrics.classificationError) * 100

// Initializing the properly labeled testing data from Resources folder.
let testingDataPath = Bundle.main.path(forResource: "intentClassificationFile", ofType: "json", inDirectory: "Data/text/test")!
let testingData = try! MLDataTable(contentsOf: URL(fileURLWithPath:testingDataPath))

// Counting the testing evaluation.
let evaluationMetrics = classifier.evaluation(on: testingData)
let evaluationAccuracy = (1.0 - evaluationMetrics.classificationError) * 100

// Confusion matrix in order to see which labels were classified wrongly.
let confusionMatrix = evaluationMetrics.confusion
print("Confusion matrix: \(confusionMatrix)")

// Metadata for saving the model.
let metadata = MLModelMetadata(author: "Hongchao Zhang",
                            shortDescription: "A model trained to classify weather related commands.",
                            version: "1.0")

// Saving the model. Remember to update the path.
try! classifier.write(to: URL(fileURLWithPath: "/Users/hozhang/Downloads/textClassifier.mlmodel"),
                    metadata: metadata)
</code></pre>

<p>We can get 99.23% training accuracy and 98.87% validation accuracy.</p>

<p><a id="markdown-model-usage" name="model-usage"></a></p>

<h4>Model Usage</h4>

<p>For the trained model of mlmodel type, we can use it in our iOS app through NLModel (from NatrualLanguage framework). The demo swift code may be like:</p>

<pre><code class="swift">let modelUrl = Bundle.main.url(forResource: "Data/text/textClassifier", withExtension: "mlmodel")
let compiledModelUrl = try! MLModel.compileModel(at: modelUrl!)
let classifier = try! NLModel(contentsOf: compiledModelUrl)

let text = requestText
let label = classifier.predictedLabel(for: text)

print("text: \(text)\nlabel:\(label ?? "Not detected!")")
</code></pre>

<blockquote><p><strong>How to use .mlmodel file?</strong></p>

<p>.mlmodel file needs to be compiled before using. There are two ways to do this: offline and online:</p>

<ol>
<li>offline: drag the mlmodel into your project, xcode will compile the .mlmodel for you before you build you app.</li>
<li>online: use <code>MLModel.compileModel</code> to compile your .mlmodel file at runtime. This is especially useful when your are at swift playground, where you cannot get xcode&rsquo;s help for comipling.</li>
</ol>
</blockquote>

<p><a id="markdown-slot-filling" name="slot-filling"></a></p>

<h3>Slot Filling</h3>

<p><a id="markdown-model-training-1" name="model-training-1"></a></p>

<h4>Model Training</h4>

<p>For Slot Filling model training, we prepare the data set as follows (The size of the training data is: 3282.):</p>

<pre><code class="json">[
  {
    "tokens": ["I", "would", "like", "the", "forecast", "in", "california", "tomorrow"], 
    "labels": ["none", "none", "none", "none", "none", "none", "location", "date"]
  }, 
  {
    "tokens": ["Forecast", "in", "Maine", "next week"], 
    "labels": ["none", "none", "location", "date"]
  }, 
...
...
]
</code></pre>

<p>Like Intent Classification model training, CreateML framework also makes it easy. Like MLTextClassifier, the MLWordTagger class from CreateML will also benefit from NatrualLanguage framework for Part of Speech, Lemmatization, Name Entity Recognition, etc.</p>

<p>The training script is:</p>

<pre><code class="swift">// Initializing the training data from Resources folder.
let trainingDataPath = Bundle.main.path(forResource: "slotParsingFile", ofType: "json", inDirectory: "Data/text/train")!
let trainingData = try! MLDataTable(contentsOf:  URL(fileURLWithPath: trainingDataPath))

// Initializing the classifier with a training data.
let classifier = try! MLWordTagger(trainingData: trainingData, tokenColumn: "tokens", labelColumn: "labels")

// Evaluating training &amp; validation accuracies.
let trainingAccuracy = (1.0 - classifier.trainingMetrics.taggingError) * 100
let validationAccuracy = (1.0 - classifier.validationMetrics.taggingError) * 100

// Initializing the properly labeled testing data from Resources folder.
let testingDataPath = Bundle.main.path(forResource: "slotParsingFile", ofType: "json", inDirectory: "Data/text/test")!
let testingData = try! MLDataTable(contentsOf: URL(fileURLWithPath:testingDataPath))

// Counting the testing evaluation.
let evaluationMetrics = classifier.evaluation(on: testingData)
let evaluationAccuracy = (1.0 - evaluationMetrics.taggingError) * 100

// Confusion matrix in order to see which labels were classified wrongly.
let confusionMatrix = evaluationMetrics.confusion
print("Confusion matrix: \(confusionMatrix)")

// Metadata for saving the model.
let metadata = MLModelMetadata(author: "Hongchao Zhang",
                                shortDescription: "A model trained to parse slots from weather related commands.",
                                version: "1.0")

// Saving the model. Remember to update the path.
try! classifier.write(to: URL(fileURLWithPath: "/Users/hozhang/Downloads/slotParsing.mlmodel"),
                        metadata: metadata)
</code></pre>

<p>We can get 99.64% training accuracy and 98.38% validation accuracy.</p>

<p><a id="markdown-model-usage-1" name="model-usage-1"></a></p>

<h4>Model Usage</h4>

<p>We can load the mlmodel into an NLTagger (from NatrualLanguage framework), and use the NLTagger to tag labels for each word of the input command text. The demo swift script is like:</p>

<pre><code class="swift">let weatherTagSchema = NLTagScheme("Weather")
let modelUrl = Bundle.main.url(forResource: "Data/text/slotParsing", withExtension: "mlmodel")
let compiledModelUrl = try! MLModel.compileModel(at: modelUrl!)
let taggerModel = try! NLModel(contentsOf: compiledModelUrl)

let weatherTagger = NLTagger(tagSchemes: [weatherTagSchema])
weatherTagger.setModels([taggerModel], forTagScheme: weatherTagSchema)

let text = requestText
weatherTagger.string = text
weatherTagger.enumerateTags(in: text.startIndex..&lt;text.endIndex, unit: .word, scheme: weatherTagSchema, options: []) { (tag, tokenRange) -&gt; Bool in
    if let tag = tag, tag.rawValue != "Whitespace" {
        print("\(text[tokenRange]): \(tag.rawValue)")
    }
    return true
}
</code></pre>

<p><strong> Reference </strong></p>

<ol>
<li><a href="https://developer.apple.com/documentation/createml/creating_a_text_classifier_model">Creating a Text Classifier Model</a>: Apple offical site for training and using machine learning models through CreateML framework.</li>
<li>WWDC video <a href="https://developer.apple.com/videos/play/wwdc2018/713/">Introducing Natural Language Framework</a>: This session introduces NLP framework and its relation with CreateML framework.</li>
</ol>


<p><a id="markdown-model-size" name="model-size"></a></p>

<h3>Model Size</h3>

<p>For the iOS app, we hope the machine learning model size is small enough. Apple&rsquo;s NatrualLanguage framework has done many optimizations on machine learning model size. The following data is from WWDC 2018 (session 713: Introducing NatrualLanguage Framework):</p>

<table>
<thead>
<tr>
<th>&ndash; </th>
<th> Open Source CRFSuite </th>
<th> Natural Language Framework</th>
</tr>
</thead>
<tbody>
<tr>
<td>Name Entity Recognition </td>
<td> 70MB  </td>
<td> 1.4MB</td>
</tr>
<tr>
<td>Chunking </td>
<td> 30MB </td>
<td> 1.8MB</td>
</tr>
</tbody>
</table>


<p>We can see that the model will be much smaller than that trained from an open source platform.</p>

<p>The size of the two models we trained is (The training data size is: 3282):</p>

<table>
<thead>
<tr>
<th>Model </th>
<th> Size</th>
</tr>
</thead>
<tbody>
<tr>
<td>Intent Classification </td>
<td> 41K</td>
</tr>
<tr>
<td>Slot Filling </td>
<td> 609K</td>
</tr>
</tbody>
</table>


<p>If your model is a neural network, you can reduce the size of your model by the following way:
<a href="https://developer.apple.com/documentation/coreml/reducing_the_size_of_your_core_ml_app">Reducing the Size of Your Core ML App</a>. You can control the precision of the neural network parameters, and thus the size of the trained model.</p>

<p>If still your model is large, you can
<a href="https://developer.apple.com/documentation/coreml/core_ml_api/downloading_and_compiling_a_model_on_the_user_s_device">Downloading and Compiling a Model on the User&rsquo;s Device</a> at runtime.</p>

<p><a id="markdown-problems-to-be-solved" name="problems-to-be-solved"></a></p>

<h3>Problems to Be Solved</h3>

<p>For Probabilistic Intent Parser, we still have some problems.</p>

<p><a id="markdown-intent-classification-model-has-no-probability-output" name="intent-classification-model-has-no-probability-output"></a></p>

<h4>Intent Classification Model Has No Probability Output</h4>

<p>We may need the probability to define the reliability of the estimated intent of an input command text.</p>

<p>However, the model trained through <code>MLTextClassifier</code> has no probability output API. If we really need the probability output, we can use other platforms to train the model, like tensorflow. That way, we will not benefit from NatrualLanguage framework and we need to consider these things by ourselves, like Tokenization, Part of Speech, Lemmatization, etc.</p>

<blockquote><p>Try other tools for training models with probability output, like Turi.</p></blockquote>

<p><a id="markdown-slot-filling-model-tagges-the-label-by-words-not-phrase" name="slot-filling-model-tagges-the-label-by-words-not-phrase"></a></p>

<h4>Slot Filling Model Tagges the Label by Words, not Phrase</h4>

<p>The NLTagger class only supply the following four tag level: word, sentence, paragraph, and document. There is no &ldquo;phrase&rdquo; tag level. For example, &ldquo;New York&rdquo; will be treated as &ldquo;New&rdquo; and &ldquo;York&rdquo;, and the tagged label will both be &ldquo;location&rdquo;. We need to compose them together manually.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[关于AR的一些使用场景]]></title>
    <link href="http://hongchaozhang.github.io/blog/2018/08/17/some-ideas-on-ar-usage/"/>
    <updated>2018-08-17T15:13:58+08:00</updated>
    <id>http://hongchaozhang.github.io/blog/2018/08/17/some-ideas-on-ar-usage</id>
    <content type="html"><![CDATA[<p>搜集一些有意思的AR应用。</p>

<!-- more -->


<p>主要参考：<a href="https://github.com/olucurious/Awesome-ARKit">Awesome ARKit</a>。这篇post里面还有很多实用的AR Tutorial和Resouces，如果自己动手，可以参考。</p>

<ul>
<li><a href="#app-store%E4%B8%8A%E9%9D%A2%E7%9A%84app">App Store上面的App</a>

<ul>
<li><a href="#%E7%A5%9E%E5%A5%87ar">神奇AR</a></li>
<li><a href="#ikea-place">IKEA Place</a></li>
<li><a href="#wallr">Wallr</a></li>
<li><a href="#horizon-explorer">Horizon Explorer</a></li>
<li><a href="#weare">WeAre</a></li>
<li><a href="#waazy---magic-ar-video-maker">Waazy - Magic AR Video Maker</a></li>
<li><a href="#human-anatomy-atlas-2019">Human Anatomy Atlas 2019</a></li>
</ul>
</li>
<li><a href="#github%E4%B8%8A%E9%9D%A2%E7%9A%84%E9%A1%B9%E7%9B%AE">Github上面的项目</a>

<ul>
<li><a href="#arkit-occlusion-demo">arkit-occlusion-demo</a></li>
<li><a href="#arvideokit">[ARVideoKit]()</a></li>
<li><a href="#arkit-smb-homage">arkit-smb-homage</a></li>
<li><a href="#arkit-corelocation">ARKit-CoreLocation</a></li>
<li><a href="#arkitnavigationdemo">ARKitNavigationDemo</a></li>
<li><a href="#fineme">FineMe</a></li>
<li><a href="#arkitspitfire">ARKitSpitfire</a></li>
</ul>
</li>
<li><a href="#resources">Resources</a>

<ul>
<li><a href="#poly">Poly</a></li>
</ul>
</li>
</ul>


<p><a id="markdown-app-store上面的app" name="app-store上面的app"></a></p>

<h2>App Store上面的App</h2>

<p><a id="markdown-神奇arhttpsitunesapplecomcnappar-arid1327719623mt8" name="神奇ar"></a></p>

<h3><a href="https://itunes.apple.com/cn/app/%E7%A5%9E%E5%A5%87ar-%E7%89%B9%E6%95%88ar%E7%9B%B8%E6%9C%BA%E5%92%8C%E7%9F%AD%E8%A7%86%E9%A2%91%E6%8B%8D%E6%91%84%E7%A5%9E%E5%99%A8/id1327719623?mt=8">神奇AR</a></h3>

<p>这个AR应用非常棒，号称“中国第一AR平台”。打开App，以为自己是打开了“抖音”呢。其模仿抖音的痕迹很重，但是神奇AR的视频不同于抖音里面的视频：都是真实世界和虚拟世界的深入互动。支持下载很多3D模型。</p>

<p><a href="https://itunes.apple.com/cn/app/%E7%A5%9E%E5%A5%87ar-%E7%89%B9%E6%95%88ar%E7%9B%B8%E6%9C%BA%E5%92%8C%E7%9F%AD%E8%A7%86%E9%A2%91%E6%8B%8D%E6%91%84%E7%A5%9E%E5%99%A8/id1327719623?mt=8">神奇AR</a>还有一个很好的应用：可以让你将照片在真实场景中打开，支持默认的排列方式，也可以自由摆放多张照片。但是图片一圈都会默认带有一圈白色的过渡带和阴影，无法去除。对于不是标准长方形的图片（比如圆形的图片，圆形之外都是透明的），显示效果不佳。</p>

<p>而且，是免费的。也许是因为其主打段视频社交吧，所以免费。</p>

<p>看官方介绍：</p>

<blockquote><p>AR视频:
用户可以利用AR模型、特效、图片、视频等拍摄一段30秒的短视频，发布在神奇AR的视频流中，或者分享到各大媒体平台，将自己的创意展现给更多的人，告诉大家如何使用AR。
玩转AR:
用户可以打开AR摄像头，通过简单的操作，将AR模型放在真实世界中，创造各种神奇的景象，用AR就能创造电影里才能出现的特技。
丰富的模型:
神奇AR是一个开放内容平台，直接对接优质的AR内容提供者，他可以通过神奇AR把自己的作品第一时间开放给用户，所以我们拥有全世界最丰富的AR内容。</p></blockquote>

<p><img src="/images/shenqiar.jpg" alt="神奇 AR" /></p>

<p><a id="markdown-ikea-placehttpsitunesapplecomusappikea-placeid1279244498mt8" name="ikea-place"></a></p>

<h3><a href="https://itunes.apple.com/us/app/ikea-place/id1279244498?mt=8">IKEA Place</a></h3>

<p>宜家的官方App，有丰富的宜家家具的3D模型，真实尺寸，可以提前放置到自己的房间，看看效果。</p>

<blockquote><p>IKEA Place lets you virtually &lsquo;place&rsquo; IKEA products in your space. The app includes 3D and true-to-scale models of everything from sofas and armchairs to footstools and coffee tables. IKEA Place gives you an accurate impression of the furniture’s size, design and functionality in your home so you can stop wondering and start doing.</p></blockquote>

<p><img src="/images/IKEAPlace.jpg" alt="ikea place" /></p>

<p><a id="markdown-wallrhttpsitunesapplecomusappwallrid1278372745" name="wallr"></a></p>

<h3><a href="https://itunes.apple.com/us/app/wallr/id1278372745">Wallr</a></h3>

<p>Wallr可以让你将图片放置到真实场景的墙面上。如果你想买画装饰墙面，可以试试。不过这个功能已经在<a href="https://itunes.apple.com/cn/app/%E7%A5%9E%E5%A5%87ar-%E7%89%B9%E6%95%88ar%E7%9B%B8%E6%9C%BA%E5%92%8C%E7%9F%AD%E8%A7%86%E9%A2%91%E6%8B%8D%E6%91%84%E7%A5%9E%E5%99%A8/id1327719623?mt=8">神奇AR</a>中实现了，而且是免费的。</p>

<p>同时放置多张图片需要花钱购买。</p>

<p><img src="/images/wallr.jpg" alt="wallr" /></p>

<p><a id="markdown-horizon-explorerhttpsitunesapplecomgbapphorizon-explorerid1326860431platformipadpreservescrollpositiontrueplatformipad" name="horizon-explorer"></a></p>

<h3><a href="https://itunes.apple.com/gb/app/horizon-explorer/id1326860431?platform=ipad&amp;preserveScrollPosition=true#platform/ipad">Horizon Explorer</a></h3>

<p>AR和地图、地理位置结合的一个应用。</p>

<p>展示你设备看到的地理位置的信息，包括距离、建筑物名称、地点名称等。并把路线在一张地图上面展示给你。</p>

<p>还可以将3D的地图展示给你，让你看看某个景点或者建筑物周围等地理信息。</p>

<p>来看官方介绍：</p>

<blockquote><p>Horizon Explorer shows you the horizon and skyline around you &amp; tells you what you&rsquo;re looking at.</p>

<p>Point your camera at a hill, village, lake or landmark and Horizon Explorer will tell you what you are looking at, how far away it is, and show you a map, and information about the point you&rsquo;re aiming at.</p>

<p>ARKit technology makes the labels and alignment much more stable than used to be possible.</p>

<p>Fly up high and see the terrain laid out below you to see what is over the hills around you, and get the lay of the land, then see the scale-model 3D map that you can walk around &amp; explore to find out what&rsquo;s behind hills, or investigate up close.</p>

<p>Tracking works best on top of a hill with an unobstructed view of your surroundings (close up trees, buildings, rocks etc. can confuse the tracking). You can drag the terrain with your finger to line up with the camera if the automatic tracking is not working very well. Or try waving your phone around in the air in a figure 8 to calibrate the compass.</p></blockquote>

<p><img src="/images/HorizonExplorer.jpg" alt="Horizon Explorer" /></p>

<p><a id="markdown-wearehttpsitunesapplecomcnappweareid1304227680platformiphonepreservescrollpositiontrueplatformiphoneplatformiphoneplatformiphone" name="weare"></a></p>

<h3><a href="https://itunes.apple.com/cn/app/weare/id1304227680?platform=iphone&amp;preserveScrollPosition=true&amp;platform=iphone#platform/iphone&amp;platform=iphone">WeAre</a></h3>

<p>这个应用可以让你选择一些照片，以设备为中心围成一圈，并缓慢移动。还可以播放视频、背景音乐和编辑3D文字。用作者的话说，“可以打造一个或温馨浪漫的回忆相册,或缥缈遥远的世界.”</p>

<p>同时还是源码可以参考：<a href="https://github.com/SherlockQi/HeavenMemoirs">HeavenMemoirs - AR相册</a></p>

<p><img src="/images/weare.jpg" alt="WeAre" /></p>

<p><a id="markdown-waazy---magic-ar-video-makerhttpsitunesapplecomusappwaazy-magic-ar-video-makerid1286992749" name="waazy---magic-ar-video-maker"></a></p>

<h3><a href="https://itunes.apple.com/us/app/waazy-magic-ar-video-maker/id1286992749">Waazy - Magic AR Video Maker</a></h3>

<p>没太看懂这个应用。感觉主要做社交视频分享。录制视频还需要AR Lens。直接看官方介绍吧：</p>

<blockquote><p>Waazy is an augmented reality short video clips shooting and sharing social network, making it possible to bring virtual characters and objects to the real world.</p>

<p>Features:
- Record cool moments with AR Lens
- Tons of free and awesome AR effects
- Can add multiple AR characters at the same time
- One tap to make all the characters dance together
- Easily move and rotate a virtual character with control pad
- Themes include fantasy, monster, fun, and landmarks
- Show your original AR videos to the world</p></blockquote>

<p><img src="/images/wazzy.jpg" alt="wazzy" /></p>

<p><a id="markdown-human-anatomy-atlas-2019httpsitunesapplecomappid1117998129" name="human-anatomy-atlas-2019"></a></p>

<h3><a href="https://itunes.apple.com/app/id1117998129">Human Anatomy Atlas 2019</a></h3>

<p>其实这个主要是展示人体内部结构的3D素材，借助AR技术投射到真实场景，没有很新鲜的AR应用场景。</p>

<p>这个应用非常专业，下载需要钱，App内还要购买。</p>

<blockquote><p>Human Anatomy Atlas offers thousands of models to help understand and communicate how the human body looks and works&ndash;and includes textbook-level definitions. Use it as a reference, instead of an anatomy textbook, or to create virtual lab experiences.
Includes over 10,000 anatomical models with descriptions in English, Spanish, French, German, Italian, Japanese, and Simplified Chinese.</p></blockquote>

<p><img src="/images/HumanAnatomyAtlas2019_1.jpg" alt="Human Anatomy Atlas 2019 1" /></p>

<p><img src="/images/HumanAnatomyAtlas2019_2.jpg" alt="Human Anatomy Atlas 2019 2" /></p>

<p><a id="markdown-github上面的项目" name="github上面的项目"></a></p>

<h2>Github上面的项目</h2>

<p><a id="markdown-arkit-occlusion-demohttpsgithubcombjarnelarkit-occlusion" name="arkit-occlusion-demo"></a></p>

<h3><a href="https://github.com/bjarnel/arkit-occlusion">arkit-occlusion-demo</a></h3>

<p>事先用一些虚拟平面将真实的墙面、柜子面、门等标记出来，就可以让虚拟的小球在房间里面来回反弹，就像撞到真实的墙上返回来一样。</p>

<p><img src="/images/occlusiongame.jpg" width="600" alt="occlusion game" /></p>

<p><a id="markdown-arvideokit" name="arvideokit"></a></p>

<h3><a href="">ARVideoKit</a></h3>

<p>一个用来录制AR视频的框架。</p>

<blockquote><p>An iOS Framework that enables developers to capture videos, photos, Live Photos, and GIFs with ARKit content.</p>

<p>In other words, you NO LONGER have to screen record/screenshot to capture videos and photos of your awesome ARKit apps!</p></blockquote>

<p>其实录屏/截屏不是也挺好的吗？！</p>

<blockquote><p>Key Features:</p>

<p>✅ Capture Photos from <code>ARSCNView</code>, <code>ARSKView</code>, and <code>SCNView</code></p>

<p>✅ Capture Live Photos &amp; GIFs from <code>ARSCNView</code>, <code>ARSKView</code>, and <code>SCNView</code></p>

<p>✅ Record Videos from <code>ARSCNView</code>, <code>ARSKView</code>, and <code>SCNView</code></p>

<p>✅ Pause/Resume video</p>

<p>✅ Allow device&rsquo;s Music playing in the background while recording a video</p>

<p>✅ Returns rendered and raw buffers in a protocol method for additional Image &amp; Video processing</p></blockquote>

<p><a id="markdown-arkit-smb-homagehttpsgithubcombjarnelarkit-smb-homage" name="arkit-smb-homage"></a></p>

<h3><a href="https://github.com/bjarnel/arkit-smb-homage">arkit-smb-homage</a></h3>

<p>在现实场景中玩超级玛丽。非常粗糙，但是创意还不错。</p>

<p><img src="/images/supermario_beginning.jpg" width="600" alt="super mario beginning" /></p>

<p><img src="/images/supermario_flag.jpg" width="600" alt="super mario flag" /></p>

<p><a id="markdown-arkit-corelocationhttpsgithubcomprojectdentarkit-corelocation" name="arkit-corelocation"></a></p>

<h3><a href="https://github.com/ProjectDent/ARKit-CoreLocation">ARKit-CoreLocation</a></h3>

<p>功能：</p>

<ol>
<li>这个库最主要的工作，是在试图整合ARKit和CoreLocation，以得到更加准确的定位，从而更好地应用于AR场景。</li>
<li>基于真实地理位置，标注出摄像头中某个建筑物或者景点的标注。这个功能类似于<a href="https://itunes.apple.com/gb/app/horizon-explorer/id1326860431?platform=ipad&amp;preserveScrollPosition=true#platform/ipad">Horizon Explorer</a>。</li>
</ol>


<p>TODO: 可以仔细看看此库附带的demo：</p>

<blockquote><p>The library and demo come with a bunch of additional features for configuration. It’s all fully documented to be sure to have a look around.</p></blockquote>

<p><img src="/images/arkit+corelocation.jpg" width="600" alt="arkit + corelocation" /></p>

<p><a id="markdown-arkitnavigationdemohttpsgithubcomchriswebb09arkitnavigationdemo" name="arkitnavigationdemo"></a></p>

<h3><a href="https://github.com/chriswebb09/ARKitNavigationDemo">ARKitNavigationDemo</a></h3>

<p>在地图上选择目的地，然后在真实场景中进行AR导航。</p>

<p>但是，这个项目也只是Demo一下，作者也很谦虚：</p>

<blockquote><p>When it loads to the map, tap a place on the map where you want to navigate to and press okay. The tap can be sluggish, so you might have to try once or twice before you get it. When the navigation screen loads, tap the screen, then give it a few seconds. You should see the nodes render.</p></blockquote>

<p>TODO: 这个项目中推荐的一些参考文献还是值得看一看的。</p>

<p><img src="/images/ARKitNavigationDemo.gif" alt="ARKitNavigationDemo" /></p>

<p><a id="markdown-finemehttpsgithubcommmoaayfindme" name="fineme"></a></p>

<h3><a href="https://github.com/mmoaay/Findme">FineMe</a></h3>

<p>可以让你的朋友根据你录制的路线图找到你：</p>

<ol>
<li>通过各种方法，记录你的起点，并让另一个人知道你的起点。比如可以通过分享起点照片，或者分享起点位置。</li>
<li>通过ARKit记录你走过的路线，并将路线分享给另一个人。</li>
<li>另一个人如果找到了你的起点，就可以根据你分享的路径找到你。</li>
</ol>


<p>但是，由于ARKit不稳定，此方法也不一定奏效。</p>

<p>作者试图通过定位和距离提高路线的稳定性，但不知效果如何，分别见于以下两个分支：</p>

<ul>
<li><a href="https://github.com/mmoaay/Findme/tree/feature/location_optimize">According to location</a></li>
<li><a href="https://github.com/mmoaay/Findme/tree/feature/distance_optimize">According to distance</a></li>
</ul>


<p><a id="markdown-arkitspitfirehttpsgithubcomchriswebb09arkitspitfire" name="arkitspitfire"></a></p>

<h3><a href="https://github.com/chriswebb09/ARKitSpitfire">ARKitSpitfire</a></h3>

<p>可以让一架3D飞机模型，根据提供的地理位置经纬度，调整姿态，并飞向那里。</p>

<p><img src="/images/ARKitSpitfire.gif" alt="ARKitSpitfire" /></p>

<p><a id="markdown-resources" name="resources"></a></p>

<h2>Resources</h2>

<p><a id="markdown-polyhttpsgithubcompiemontepoly" name="poly"></a></p>

<h3><a href="https://github.com/piemonte/Poly">Poly</a></h3>

<p><a href="https://github.com/piemonte/Poly">Poly</a>是一个iOS库，用来从<a href="https://developers.google.com/poly/">Google Poly</a>上下载3D模型，包含搜索、下载管理和缓存功能。</p>

<p><a href="https://developers.google.com/poly/develop/ios">iOS Quickstart</a>展示了如何在iOS中下载使用<a href="https://developers.google.com/poly/">Google Poly</a>上面的3D资源。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Introducing Create ML]]></title>
    <link href="http://hongchaozhang.github.io/blog/2018/06/15/introducing-create-ml/"/>
    <updated>2018-06-15T17:39:30+08:00</updated>
    <id>http://hongchaozhang.github.io/blog/2018/06/15/introducing-create-ml</id>
    <content type="html"><![CDATA[<!-- more -->


<ul>
<li><a href="#background">Background</a></li>
<li><a href="#create-ml">Create ML</a></li>
<li><a href="#main-advantage-easy-to-use">Main Advantage: Easy to Use</a>

<ul>
<li><a href="#create-ml%E5%92%8Cturi-create">Create ML和Turi Create</a></li>
</ul>
</li>
<li><a href="#transfer-learning">Transfer Learning</a></li>
<li><a href="#improving-accuracy">Improving Accuracy</a></li>
<li><a href="#references-and-materials">References and Materials</a></li>
</ul>


<p>Apple released Core ML in WWDC2017, and I took a note on <a href="../../../../2017/12/28/coreml-usage/">CoreML Usage</a>, including mlmodel training using Microsoft <a href="https://www.customvision.ai/">Custom Vision</a>.</p>

<p>This post is about the background of Create ML, its advantages, its relations with Turi. There is no code in this post. If you are looking for the usage of Create ML, refer to <a href="https://www.raywenderlich.com/196233/create-ml-tutorial-getting-started">Create ML Tutorial: Getting Started</a>.</p>

<p><a id="markdown-background" name="background"></a></p>

<h2>Background</h2>

<p>Before 2018, where can we get the mlmodel file used in iOS and macOS?</p>

<ul>
<li><a href="https://github.com/tf-coreml/tf-coreml">TensorFlow</a>: Train machine learning models and easily convert them to the Core ML Model format.</li>
<li><a href="https://www.customvision.ai/">Custom Vision</a> from Microsoft</li>
<li><a href="https://pypi.org/project/coremltools/">Core ML Tools</a>: Use this python package to convert models from machine learning toolboxes into the Core ML format.</li>
<li><a href="https://github.com/apache/incubator-mxnet/tree/master/tools/coreml">Apache MXNet</a>: Train machine learning models and convert them to the Core ML format.</li>
<li><a href="https://github.com/onnx/onnx-coreml">ONNX</a>: Convert ONNX models you have created to the Core ML Model format.</li>
<li>&hellip;</li>
</ul>


<p>TensorFlow doesn&rsquo;t support GPU on macOS from version 1.2.</p>

<p><img src="/images/tensorflow_not_support_gpu_on_macos.jpg" alt="tensor flow not support gpu on macos" /></p>

<p><strong>Core ML</strong>: Announced at WWDC 2017, and already supported by every major ML platform to convert existing models. But the existing models tend to be too big and/or too general.</p>

<p><strong>Turi Create</strong>: Acquired by Apple in 2016 ($200M), it lets you customize existing models with your own data. But … Python :[.</p>

<p><a id="markdown-create-ml" name="create-ml"></a></p>

<h2>Create ML</h2>

<p>Finally in WWDC2018, Apple announced <strong>Create ML</strong>, which can train machine learning models on macOS, able to use the GPU on macOS. The Create ML session and Turi Create session did not mention any word on each other, but obviousely, Create ML is based on Turi Create.</p>

<p>Based on Trui&rsquo;s model training, Create ML can make model training on macOS using GPU (maybe through Metal), and come up with models which can be used by Core ML framework.</p>

<p>With XCode Playground&rsquo;s updates, Apple gives CreateMLUI, a very easy way for model training: just need to drag your training data and test data into Playground.</p>

<p><a id="markdown-main-advantage-easy-to-use" name="main-advantage-easy-to-use"></a></p>

<h2>Main Advantage: Easy to Use</h2>

<p>Do model training using Swift in XCode.</p>

<blockquote><p><strong>Create ML</strong> is proof that Apple is committed to making it easier for you to use machine learning models in your apps. In this Create ML tutorial, you’ll learn how Create ML speeds up the workflow for improving your model by improving your data while also flattening the learning curve by doing it all in the comfort of Xcode and Swift.</p>

<p>&ndash;Refer to <a href="https://www.raywenderlich.com/196233/create-ml-tutorial-getting-started">Create ML Tutorial: Getting Started</a></p>

<p><strong>Create ML</strong>: Announced at WWDC 2018. ML in Xcode &amp; Swift! Currently includes only two of Turi Create’s seven task-focused toolkits, plus a generic classifier and regressor, and data tables. I see it as a trail of breadcrumbs leading you to the Turi Create gingerbread house, inhabited by a “good dog” instead of a witch! (Turi Create’s logo is a dog silhouette.)</p>

<p>&ndash;Refer to <a href="https://www.raywenderlich.com/196233/create-ml-tutorial-getting-started">Create ML Tutorial: Getting Started</a></p></blockquote>

<p>Refer to <a href="https://www.raywenderlich.com/196233/create-ml-tutorial-getting-started">Create ML Tutorial: Getting Started</a> to see how easy it is to use Create ML. There are some code comparasion between Create ML and Turi Create.</p>

<p><a id="markdown-create-ml和turi-create" name="create-ml和turi-create"></a></p>

<h3>Create ML和Turi Create</h3>

<p>Currently Create ML includes only two of Turi Create’s seven task-focused toolkits, plus a generic classifier and regressor, and data tables. Turi Create has five task-focused toolkits that aren’t (yet?) in Create ML:</p>

<ul>
<li>Recommender systems</li>
<li>Image similarity</li>
<li>Object detection</li>
<li>Style transfer</li>
<li>Activity classification</li>
</ul>


<p><a id="markdown-transfer-learning" name="transfer-learning"></a></p>

<h2>Transfer Learning</h2>

<p>The description of Transfer Learning from Apple Turi:</p>

<blockquote><p>It’s not uncommon for the task you want to solve to be related to something that has already been solved. Take, for example, the task of distinguishing cats from dogs. The famous ImageNet Challenge, for which CNN’s are the state-of-the-art, asks the trained model to categorize input into one of 1000 classes. Shouldn&rsquo;t features that distinguish between categories like lions and wolves also be useful for discriminating between cats and dogs?</p>

<p>The answer is a definitive yes. It is accomplished by simply removing the output layer of the Deep Neural Network for 1000 categories, and taking the signals that would have been propagating to the output layer and feeding them as features to a classifier for our new cats vs dogs task.</p>

<p>So, when you run the Turi Create image classifier, it breaks things down into something like this:</p>

<p>Stage 1: Create a CNN classifier on a large, general dataset. A good example is ImageNet, with 1000 categories and 1.2 million images. The models are already trained by researchers and are available for us to use.</p>

<p>Stage 2: The outputs of each layer in the CNN can be viewed as a meaningful vector representation of each image. Extract these feature vectors from the layer prior to the output layer on each image of your task.</p>

<p>Stage 3: Create a new classifier with those features as input for your own task.</p>

<p>At first glance, this seems even more complicated than just training the deep learning model. However, Stage 1 is reusable for many different problems, and once done, it doesn&rsquo;t have to be changed often.</p>

<p>In the end, this pipeline results in not needing to adjust hyper-parameters, faster training, and better performance even in cases where you don&rsquo;t have enough data to create a convention deep learning model. What&rsquo;s more, this technique is effective even if your Stage 3 classification task is relatively unrelated to the task Stage 1 is trained on. This idea was first explored by Donahue et al. (2013), and has since become one of the best ways to create image classifier models.</p>

<p>&ndash;Refer to <a href="https://apple.github.io/turicreate/docs/userguide/image_classifier/how-it-works.html#transfer-learning">truicreate transfer learning</a></p></blockquote>

<p>Some comments on transfer learning from web:</p>

<blockquote><p>What’s happening here? It’s called transfer learning, if you want to look it up. The underlying model — VisionFeaturePrint_Screen, which backs the Vision framework — was pre-trained on a ginormous dataset to recognize an enormous number of classes. It did this by learning what features to look for in an image, and how to combine these features to classify the image. Almost all of the training time for your dataset is the model extracting around 1000 features from your images. These could include low-level shapes and textures and higher-level shape of ears, distance between eyes, shape of snout. Then it spends a relatively tiny amount of time training a logistic regression model to separate your images into two classes. It’s similar to fitting a straight line to scattered points, but in 1000 dimensions instead of 2. But it’s still very quick to do: my run 1m 15s for feature extraction and 0.177886 seconds to train and apply the logistic regression.</p>

<p>Transfer learning only works successfully when features of your dataset are reasonably similar to features of the dataset that was used to train the model. A model pre-trained on ImageNet — a large collection of photos — might not transfer well to pencil drawings or microscopy images.</p>

<p>&ndash;Refer to <a href="https://www.raywenderlich.com/196233/create-ml-tutorial-getting-started">Create ML Tutorial: Getting Started</a></p></blockquote>

<p><a id="markdown-improving-accuracy" name="improving-accuracy"></a></p>

<h2>Improving Accuracy</h2>

<p>Refer to <a href="https://developer.apple.com/documentation/create_ml/improving_your_model_s_accuracy">Improving Your Model’s Accuracy</a> from Apple for improving training accuracy.</p>

<p>How to improve the model&rsquo;s training accuracy, validation accuracy and evaluation accuracy. <a href="https://www.raywenderlich.com/196233/create-ml-tutorial-getting-started">Create ML Tutorial: Getting Started</a> describes the three &lsquo;accuracy&rsquo;s.</p>

<p><a id="markdown-references-and-materials" name="references-and-materials"></a></p>

<h2>References and Materials</h2>

<ol>
<li><p>You might like to browse two fascinating articles about features from (mostly) Google Brain/Research:</p>

<ul>
<li><p><a href="https://distill.pub/2018/building-blocks/">The Building Blocks of Interpretability</a>: image feature extracting</p></li>
<li><p><a href="https://distill.pub/2017/feature-visualization/">Feature Visualization</a></p></li>
</ul>
</li>
<li><p><a href="https://www.kaggle.com/">Kaggle</a> is a repository of datasets contributed by members, often supplemented by notebooks that analyze and visualize the data. It runs model prediction competitions, which leads to the next link:</p>

<ul>
<li>Machine Learning Zero-to-Hero: Everything you need in order to compete on Kaggle for the first time, step-by-step!</li>
</ul>
</li>
<li><p><a href="https://www.raywenderlich.com/196233/create-ml-tutorial-getting-started">Create ML Tutorial: Getting Started</a> describes the usage of Create ML and Turi Create, including the history, code, data preparation, improving model accuracy and so on.</p></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[What is new in ARKit 2]]></title>
    <link href="http://hongchaozhang.github.io/blog/2018/06/13/what-is-new-in-n-arkit-2/"/>
    <updated>2018-06-13T13:24:59+08:00</updated>
    <id>http://hongchaozhang.github.io/blog/2018/06/13/what-is-new-in-n-arkit-2</id>
    <content type="html"><![CDATA[<!-- more -->


<ul>
<li><a href="#overview">Overview</a></li>
<li><a href="#new-features-in-arkit-2">New Features in ARKit 2</a>

<ul>
<li><a href="#saving-and-loading-maps">Saving and Loading Maps</a>

<ul>
<li><a href="#world-tracking-recap">World Tracking Recap:</a></li>
<li><a href="#world-tracking-enhancement">World Tracking Enhancement:</a></li>
<li><a href="#saving-and-loading-maps">Saving and loading maps:</a></li>
<li><a href="#how-to-get-a-good-map">How to get a good map</a></li>
</ul>
</li>
<li><a href="#environment-texturing">Environment Texturing</a></li>
<li><a href="#image-tracking">Image Tracking</a></li>
<li><a href="#3d-object-detection">3D Object Detection</a></li>
<li><a href="#face-tracking-enhancements">Face Tracking Enhancements</a></li>
</ul>
</li>
<li><a href="#some-other-wwdc-sessions-related-to-ar">Some other WWDC Sessions Related to AR</a>

<ul>
<li><a href="#integrating-apps-and-content-with-ar-quick-look">Integrating Apps and Content with AR Quick Look</a></li>
<li><a href="#inside-swiftshot-creating-an-ar-game">Inside SwiftShot: Creating an AR Game</a></li>
<li><a href="#creating-great-ar-experiences">Creating Great AR Experiences</a></li>
<li><a href="#understanding-arkit-tracking-and-detection">Understanding ARKit Tracking and Detection</a></li>
</ul>
</li>
<li><a href="#some-other-materials-for-a-better-ar-app">Some other materials for a better AR app:</a>

<ul>
<li><a href="#building-your-first-ar-experience">Building Your First AR Experience</a></li>
<li><a href="#managing-session-lifecycle-and-tracking-quality">Managing Session Lifecycle and Tracking Quality</a></li>
<li><a href="#human-interface-guidelines---augmented-reality">Human Interface Guidelines - Augmented Reality</a></li>
<li><a href="#handling-3d-interaction-and-ui-controls-in-augmented-reality">Handling 3D Interaction and UI Controls in Augmented Reality</a></li>
<li><a href="#creating-a-multiuser-ar-experience">Creating a Multiuser AR Experience</a></li>
<li><a href="#swiftshot-creating-a-game-for-augmented-reality">SwiftShot: Creating a Game for Augmented Reality</a></li>
<li><a href="#recognizing-images-in-an-ar-experience">Recognizing Images in an AR Experience</a></li>
<li><a href="#scanning-and-detecting-3d-objects">Scanning and Detecting 3D Objects</a></li>
</ul>
</li>
</ul>


<p><a id="markdown-overview" name="overview"></a></p>

<h2>Overview</h2>

<p>In ARKit 1, we have:</p>

<ul>
<li>Device positioning from world tracking process</li>
<li>Horizontal and vertical plane detection from world tracking process</li>
<li>Lighting estimation</li>
<li>AR face tracking</li>
</ul>


<p>In ARKit 2, we have:</p>

<ul>
<li>Saving and loading maps</li>
<li>Environment Texturing</li>
<li>Image detection and tracking</li>
<li>3D object tracking</li>
<li>Improved face tracking</li>
</ul>


<p><a id="markdown-new-features-in-arkit-2" name="new-features-in-arkit-2"></a></p>

<h2>New Features in ARKit 2</h2>

<p><a id="markdown-saving-and-loading-maps" name="saving-and-loading-maps"></a></p>

<h3>Saving and Loading Maps</h3>

<p><a id="markdown-world-tracking-recap" name="world-tracking-recap"></a></p>

<h4>World Tracking Recap:</h4>

<ul>
<li>Position and orientation of the device.</li>
<li>Physical scale in the scene.</li>
<li>3D feature points.</li>
<li>Relocalization (iOS 11.3): we can relocalize objects when your AR session is interrupted, like phone coming or going from background. This feature is implemented by storing the mapping <code>ARWorldMap</code> between real world and the coordinate system. However the mapping is not exposed to developers.</li>
</ul>


<p><a id="markdown-world-tracking-enhancement" name="world-tracking-enhancement"></a></p>

<h4>World Tracking Enhancement:</h4>

<ul>
<li><strong>Saving and loading maps</strong>: expose the <code>ARWorldMap</code> to developers.</li>
<li>Faster initialization and plane detection</li>
<li>Robust tracking and plane detection</li>
<li>More accurate extent and boundary Continuous autofocus</li>
<li>New 4:3 video formats (iPad is also 4:3)</li>
</ul>


<p><a id="markdown-saving-and-loading-maps" name="saving-and-loading-maps"></a></p>

<h4>Saving and loading maps:</h4>

<p><code>ARWorldmap</code> contains:</p>

<ul>
<li>Mapping of physical 3D space: for representing 3D feature points in the coordinate system.</li>
<li>Mutable list of named anchors: for restoring previous 3D environment (like lighting node anchor), and relocalizing previously added virtual objects.</li>
<li>Raw feature points and extent: for debugging and visualization.</li>
<li>Serialization: for storing and recovering from an file.</li>
</ul>


<p><img src="/images/arkit2_arworldmap.jpg" width="500" alt="arkit arworldmap" /></p>

<p>We can use the map in two different ways:</p>

<ul>
<li>Persistent: Restore previous AR scene for a new AR session. For example, you go to another room and come back or close the AR app and open it some time later.</li>
<li>Multiuser experience: We can share the map among devices through WiFi or bluetooth.</li>
</ul>


<p>The SwiftShot is an multiuser experience AR game:</p>

<p><img src="/images/swiftshot.jpg" alt="arkit2 swiftshot" /></p>

<p>and the following is a small piece of the demo:</p>

<p><img src="/images/arkit2_multiuser_experience_demo.gif" alt="swift shot game" /></p>

<p><a id="markdown-how-to-get-a-good-map" name="how-to-get-a-good-map"></a></p>

<h4>How to get a good map</h4>

<p>In order to share or restore the map, we need to get a good one first. A good map should be:</p>

<!-- * Important for relocalization -->


<ul>
<li>Multiple points of view: If we record the mapping from one point of view, and try to restore the coordinate system from another point of view, it will fail.</li>
<li>Static, well-textured environment.</li>
<li>Dense feature points on the map.</li>
</ul>


<p>We can use the <code>WorldMappingStatus</code> status from <code>ARFrame</code> to decide if the current map is good enough for sharing or storing:</p>

<pre><code class="swift">public enum WorldMappingStatus : Int {
   case notAvailable
   case limited
   case extending
   case mapped 
}
</code></pre>

<p><a id="markdown-environment-texturing" name="environment-texturing"></a></p>

<h3>Environment Texturing</h3>

<p>With the help of Environment Texturing, AR scene objects can reflect the environment texture on the surface of themselves, just like:</p>

<p><img src="/images/arkit2_environment_texturing_demo.jpg" alt="arkit2 environment texturing demo" /></p>

<p><a id="markdown-image-tracking" name="image-tracking"></a></p>

<h3>Image Tracking</h3>

<p>Moving objects can not be positioned in ARKit 1. In ARKit 2, specified images can be tracked in AR scene.</p>

<p><img src="/images/arkit2_image_tracking.gif" alt="arkit 2 image tracking" /></p>

<p>The classes in ARKit 2 for image tracking are:</p>

<p><img src="/images/arkit2_image_tracking_classes.jpg" alt="arkit 2 image tracking classes" /></p>

<p>The detected <code>ARImageAnchor</code>s have properties like:</p>

<pre><code class="swift">open class ARImageAnchor : ARAnchor, ARTrackable { 
    public var isTracked: Bool { get }
    open var transform: simd_float4x4 { get }
    open var referenceImage: ARReferenceImage { get }
}
</code></pre>

<p>The specified image should:</p>

<ul>
<li>Histogram should be broad</li>
<li>Not have multiple uniform color regions</li>
<li>Not have repeated structures</li>
</ul>


<p>The following is the demo:</p>

<p><img src="/images/arkit2_image_tracking_demo.gif" alt="arkit 2 image tracking demo" /></p>

<p>The inputs of the above demo are:</p>

<ul>
<li>an static image of the cat, the same as it is in the picture frame</li>
<li>an video of the cat</li>
</ul>


<p>The video is played at the position of the specified picture frame, with the same orientation of the picture frame.</p>

<p>There are two classes related to image tracking:</p>

<table>
<thead>
<tr>
<th>ARImageTrackingConfiguration </th>
<th> ARWorldTrackingConfiguration</th>
</tr>
</thead>
<tbody>
<tr>
<td>Has No World Origin </td>
<td> Has World Origin</td>
</tr>
<tr>
<td>After detecting the image, only do things inside the place of the image. </td>
<td> After detecting the image, place some virtual objects outside the detected image plane.</td>
</tr>
</tbody>
</table>


<p><a id="markdown-3d-object-detection" name="3d-object-detection"></a></p>

<h3>3D Object Detection</h3>

<p>3D object detection workflow is:</p>

<p><img src="/images/arkit2_3D_object_tracking_classes.jpg" alt="arkit2 3D object tracking classes" /></p>

<p>The <code>ARObjectAnchor</code> contains properties like:</p>

<pre><code class="swift">open class ARObjectAnchor : ARAnchor {
    open var transform: simd_float4x4 { get }
    open var referenceObject: ARReferenceObject { get }
}
</code></pre>

<p>and <code>ARReferenceObject</code> is the scanned 3D object:</p>

<pre><code class="swift">open class ARReferenceObject
    : NSObject, NSCopying, NSSecureCoding {
    open var name: String?
    open var center: simd_float3 { get }
    open var extent: simd_float3 { get }
    open var rawFeaturePoints: ARPointCloud { get }
}
</code></pre>

<blockquote><p>An <code>ARReferenceObject</code> contains only the spatial feature information needed for ARKit to recognize the real-world object, and is not a displayable 3D reconstruction of that object.</p></blockquote>

<p>In order to get the <code>ARReferenceObject</code>, we should scan the real object, and store the result as an file (.arobject) or an xcode asset catalog for ARKit to use. Fortunately, Apple supplies a demo for scanning 3D object to get the <code>ARReferenceObject</code>. Refer to: <a href="https://developer.apple.com/documentation/arkit/scanning_and_detecting_3d_objects">Scanning and Detecting 3D Objects</a> for detail and the rough steps of object scanning are:</p>

<p><img src="/images/arkit2_3D_object_scan.jpg" alt="arkit2 3D object scan" /></p>

<p>For scanned object in the real world, we can dynamically add some info around it (Museum is a good use case.), like the demo does:</p>

<p><img src="/images/arkit2_3D_object_tracking_demo.gif" alt="arkit2 object tracking demo" /></p>

<p><a id="markdown-face-tracking-enhancements" name="face-tracking-enhancements"></a></p>

<h3>Face Tracking Enhancements</h3>

<p>With face tracking, we can place something on it or around it.</p>

<p>Enhancements in ARKit 2:</p>

<ul>
<li>Gaze tracking</li>
<li>Tongue support</li>
</ul>


<blockquote><p>Gaze and Tongue can be input of the AR app.</p></blockquote>

<p>New changes in one screenshot:</p>

<p><img src="/images/what-is-new-in-arkit-2.jpg" alt="what-is-new-in-arkit-2" /></p>

<p><a id="markdown-some-other-wwdc-sessions-related-to-ar" name="some-other-wwdc-sessions-related-to-ar"></a></p>

<h2>Some other WWDC Sessions Related to AR</h2>

<p><a id="markdown-integrating-apps-and-content-with-ar-quick-lookhttpsdeveloperapplecomvideosplaywwdc2018603" name="integrating-apps-and-content-with-ar-quick-lookhttpsdeveloperapplecomvideosplaywwdc2018603"></a></p>

<h3><a href="https://developer.apple.com/videos/play/wwdc2018/603/">Integrating Apps and Content with AR Quick Look</a></h3>

<p>A deeper dive into a new feature in iOS that provides a way to preview any AR object from a USDZ file.</p>

<p><img src="/images/QLPreviewController.png" alt="QLPreviewController" /></p>

<ul>
<li>There’s a great sequence diagram presented (see above) (I wish more sessions would have these!) for previewing USDZ objects, of which the <code>QLPreviewController</code> plays a central role.</li>
<li>For web developers, it covers HTML samples for how to preview USDZ objects in Safari.</li>
<li>Then it goes into a deep dive on how to create the actual USDZ objects, with more examples on new AR texturing capabilities.</li>
<li>There’s also a quick overview on how to optimize the files, to keep the size down, and there’s a breakdown of the files that make up the USDZ format.</li>
</ul>


<p><a id="markdown-inside-swiftshot-creating-an-ar-gamehttpsdeveloperapplecomvideosplaywwdc2018605" name="inside-swiftshot-creating-an-ar-gamehttpsdeveloperapplecomvideosplaywwdc2018605"></a></p>

<h3><a href="https://developer.apple.com/videos/play/wwdc2018/605/">Inside SwiftShot: Creating an AR Game</a></h3>

<p>Covers world map sharing, networking, and the physics of how to build an AR game, as well as some design insight (I have limited game dev experience so I’ll do the best I can below).</p>

<ul>
<li>Pointers to remember with designing an AR game, such as “encouraging” the user to slowly move the device for world mapping!</li>
<li>It demonstrates the usage of image &amp; object detection, world map sharing, and iBeacons for the game.</li>
<li>Integrating <code>ARKit</code> with <code>SceneKit</code> and <code>Metal</code>, including the translation of physics data between each — position, velocity, and orientation.</li>
<li>Performance enhancement with the <code>BitStreamCodable</code> protocol.</li>
<li>A small look at how audio was integrated into the game.</li>
</ul>


<p><a id="markdown-creating-great-ar-experienceshttpsdeveloperapplecomvideosplaywwdc2018805" name="creating-great-ar-experienceshttpsdeveloperapplecomvideosplaywwdc2018805"></a></p>

<h3><a href="https://developer.apple.com/videos/play/wwdc2018/805/">Creating Great AR Experiences</a></h3>

<p>Best practises mainly from a UX &amp; design perspective (there are no code samples in this session).</p>

<ul>
<li>Logical dos and don’ts that may be useful, if you need help with thought towards product and empathy towards the user.</li>
<li>They emphasize the importance of using transitions between AR scapes.</li>
<li>Why AR is a special combination of touch and movement.</li>
<li>They advise that minimal battery impact should be a huge focus! This is a challenge, given that they recommend to render the FPS at 60 to avoid latency.</li>
<li>There’s a lengthy demonstration of creating an AR fireplace, with complex texturing, etc. It looks great, but unfortunately there were no coding samples accompanying the demo.</li>
</ul>


<p><a id="markdown-understanding-arkit-tracking-and-detectionhttpsdeveloperapplecomvideosplaywwdc2018610" name="understanding-arkit-tracking-and-detectionhttpsdeveloperapplecomvideosplaywwdc2018610"></a></p>

<h3><a href="https://developer.apple.com/videos/play/wwdc2018/610/">Understanding ARKit Tracking and Detection</a></h3>

<p>A good broad overview of all of the main AR concepts.</p>

<ul>
<li>This is such a good intro into not only AR on iOS, but AR in general, that it should have been part of 2017’s sessions when ARKit was first introduced. Better late than never. If you’re only going to watch one session, watch this one!</li>
<li>It recaps the main features of ARKit — <strong>orientation</strong>, <strong>world tracking</strong>, and <strong>plane detection</strong>, and demos all of these in depth with coding samples.</li>
<li>It then demos the new features of ARKit 2 — <strong>shared world mapping</strong>, <strong>image tracking</strong>, and <strong>object detection</strong> (which has been available in the Vision framework recapped above, but is now also accessible in ARKit).</li>
<li>A good explanation on a core AR principle, <strong>Visual Inertial Odometry</strong>, is given. Short of going into the actual physics equations behind it, this should give you a great understanding of VIO.</li>
</ul>


<p><a id="markdown-some-other-materials-for-a-better-ar-app" name="some-other-materials-for-a-better-ar-app"></a></p>

<h2>Some other materials for a better AR app:</h2>

<p><a id="markdown-building-your-first-ar-experiencehttpsdeveloperapplecomdocumentationarkitbuilding_your_first_ar_experience" name="building-your-first-ar-experiencehttpsdeveloperapplecomdocumentationarkitbuilding_your_first_ar_experience"></a></p>

<h3><a href="https://developer.apple.com/documentation/arkit/building_your_first_ar_experience">Building Your First AR Experience</a></h3>

<p>This document demos an app for basic usage of ARKit.</p>

<p><a id="markdown-managing-session-lifecycle-and-tracking-qualityhttpsdeveloperapplecomdocumentationarkitmanaging_session_lifecycle_and_tracking_quality" name="managing-session-lifecycle-and-tracking-qualityhttpsdeveloperapplecomdocumentationarkitmanaging_session_lifecycle_and_tracking_quality"></a></p>

<h3><a href="https://developer.apple.com/documentation/arkit/managing_session_lifecycle_and_tracking_quality">Managing Session Lifecycle and Tracking Quality</a></h3>

<p>Make your AR experience more robust by</p>

<ul>
<li>providing clear feedback, using <code>ARCamera.TrackingState</code>.</li>
<li>recovering from interruptions, using <code>ARCamera.TrackingState.Reason.relocalizing</code>.</li>
<li>resuming previous sessions, using <code>ARWorldMap</code>.</li>
</ul>


<p><a id="markdown-human-interface-guidelines---augmented-realityhttpsdeveloperapplecomdesignhuman-interface-guidelinesiossystem-capabilitiesaugmented-reality" name="human-interface-guidelines---augmented-realityhttpsdeveloperapplecomdesignhuman-interface-guidelinesiossystem-capabilitiesaugmented-reality"></a></p>

<h3><a href="https://developer.apple.com/design/human-interface-guidelines/ios/system-capabilities/augmented-reality/">Human Interface Guidelines - Augmented Reality</a></h3>

<p>This post describes how to rendering virtual objects, how to interact with virtual objects, how to handling interruptions. It is for UX.</p>

<p><a id="markdown-handling-3d-interaction-and-ui-controls-in-augmented-realityhttpsdeveloperapplecomdocumentationarkithandling_3d_interaction_and_ui_controls_in_augmented_reality" name="handling-3d-interaction-and-ui-controls-in-augmented-realityhttpsdeveloperapplecomdocumentationarkithandling_3d_interaction_and_ui_controls_in_augmented_reality"></a></p>

<h3><a href="https://developer.apple.com/documentation/arkit/handling_3d_interaction_and_ui_controls_in_augmented_reality">Handling 3D Interaction and UI Controls in Augmented Reality</a></h3>

<p>This document describes the best practices for visual feedback, gesture interactions, and realistic rendering in AR experiences. And a demo app is supplied.</p>

<p><img src="/images/arkit_demo_screenshot.jpg" alt="arkit demo" /></p>

<p><a id="markdown-creating-a-multiuser-ar-experiencehttpsdeveloperapplecomdocumentationarkitcreating_a_multiuser_ar_experience" name="creating-a-multiuser-ar-experiencehttpsdeveloperapplecomdocumentationarkitcreating_a_multiuser_ar_experience"></a></p>

<h3><a href="https://developer.apple.com/documentation/arkit/creating_a_multiuser_ar_experience">Creating a Multiuser AR Experience</a></h3>

<p>This document demos an app (with source code) on how to transmit ARKit world-mapping data between nearby devices with the <a href="https://developer.apple.com/documentation/multipeerconnectivity">MultipeerConnectivity</a> framework (introduced in iOS 7.0) to create a shared basis for AR experiences. MultipeerConnectivity supports peer-to-peer connectivity and the discovery of nearby devices. With MultipeerConnectivity, you can not only share <code>ARWorldMap</code>, but also some actions. This makes multiuser AR game possible.</p>

<p>However:</p>

<ul>
<li>Recording and transmitting a world map and relocalizing to a world map are time-consuming, bandwidth-intensive operations. A good design is needed for better performance.</li>
<li>The persons received the world map data need to move their device so they see a similar perspective (also sent by the host) helps ARKit process the received map and establish a shared frame of reference for the multiuser experience.</li>
</ul>


<p><a id="markdown-swiftshot-creating-a-game-for-augmented-realityhttpsdeveloperapplecomdocumentationarkitswiftshot_creating_a_game_for_augmented_reality" name="swiftshot-creating-a-game-for-augmented-realityhttpsdeveloperapplecomdocumentationarkitswiftshot_creating_a_game_for_augmented_reality"></a></p>

<h3><a href="https://developer.apple.com/documentation/arkit/swiftshot_creating_a_game_for_augmented_reality">SwiftShot: Creating a Game for Augmented Reality</a></h3>

<p>This document demos the SwiftShot game shown on WWDC 2018, including:</p>

<ul>
<li>Designing Gameplay for AR</li>
<li>Using Local Multipeer Networking and Sharing World Maps</li>
<li>Synchronizing Gameplay Actions</li>
<li>Solving Multiplayer Physics</li>
</ul>


<p><a id="markdown-recognizing-images-in-an-ar-experiencehttpsdeveloperapplecomdocumentationarkitrecognizing_images_in_an_ar_experience" name="recognizing-images-in-an-ar-experiencehttpsdeveloperapplecomdocumentationarkitrecognizing_images_in_an_ar_experience"></a></p>

<h3><a href="https://developer.apple.com/documentation/arkit/recognizing_images_in_an_ar_experience">Recognizing Images in an AR Experience</a></h3>

<p>Detect known 2D images in the user’s environment, and use their positions to place AR content.</p>

<p><a id="markdown-scanning-and-detecting-3d-objectshttpsdeveloperapplecomdocumentationarkitscanning_and_detecting_3d_objects" name="scanning-and-detecting-3d-objectshttpsdeveloperapplecomdocumentationarkitscanning_and_detecting_3d_objects"></a></p>

<h3><a href="https://developer.apple.com/documentation/arkit/scanning_and_detecting_3d_objects">Scanning and Detecting 3D Objects</a></h3>

<p>Record spatial features of real-world objects, then use the results to find those objects in the user’s environment and trigger AR content.</p>
]]></content>
  </entry>
  
</feed>
