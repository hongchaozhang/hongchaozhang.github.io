<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: machine learning | Reading Space]]></title>
  <link href="http://hongchaozhang.github.io/blog/categories/machine-learning/atom.xml" rel="self"/>
  <link href="http://hongchaozhang.github.io/"/>
  <updated>2019-05-24T11:51:50+08:00</updated>
  <id>http://hongchaozhang.github.io/</id>
  <author>
    <name><![CDATA[Zhang Hongchao]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Offline Natural Language Understanding Engine on iOS]]></title>
    <link href="http://hongchaozhang.github.io/blog/2019/05/22/offline-natural-language-understanding-engine-on-ios/"/>
    <updated>2019-05-22T16:04:28+08:00</updated>
    <id>http://hongchaozhang.github.io/blog/2019/05/22/offline-natural-language-understanding-engine-on-ios</id>
    <content type="html"><![CDATA[<!-- more -->


<h2>Objective</h2>

<p><img src="/images/NLUObjective.png" alt="nlu objective" /></p>

<p>We want an NLU Engine to understand the normal text command on Mobile. We hope the engine can know the command&rsquo;s intent and the info the command needs to execute.</p>

<p>Currently, there are many NLU related tools, like Google Dialogflow, Amazon Lex, Facebook Wit.ai, Microsoft Luis. However, they are all online tools. Considering the privacy problem, we are trying to build our own offline NLU Engine.</p>

<h3>What is a good NLU engine</h3>

<p>Let’s start by looking at a simple example, and see what you would expect from a good NLU engine.</p>

<p>First, we need some examples to train the NLU engine. Consider the following dataset, used to train a simple weather assistant with a few query examples:</p>

<ul>
<li>Give me the weather for [tomorrow](date)</li>
<li>Show me the [Paris](location) weather for [Sunday](date)</li>
</ul>


<h4>Deterministic behavior</h4>

<p>The first thing you want is that all the examples you give to train the model are correctly supported by the engine. This makes the system predictable and easy to use: if a query is not correctly parsed, then add it to the dataset and it will work right away.</p>

<h4>Generalization power</h4>

<p>Having this deterministic behavior is great for robustness and predictability, but a powerful NLU engine also needs to have some generalization power. You want the system not only to recognize patterns provided in the training set, but also all the possible variations that come from speaking naturally. If we go back to the previous dataset, it is reasonable to expect the NLU engine to parse a query like: “What’s the weather in Beijing right now?” even though it is not one of the training examples.</p>

<h2>Design and Workflow</h2>

<p><img src="/images/NLUDesign.png" alt="nlu design" /></p>

<p>In order to satisfy these objectives: deterministic behavior and generalization power, we built the processing pipeline described in the figure above. It receives a text as input, and outputs a structured response containing the intent and the list of slots. The main processing unit of the pipeline is the NLU engine. It contains two intent parsers which are called successively: a deterministic intent parser and a probabilistic one.</p>

<p>The deterministic parser relies on regular expressions to match intent and slots, which results in perfect behavior on training examples but doesn’t generalize. This parser is the first to be used because of its strictness.</p>

<p>The probabilistic parser is used whenever the first parser fails to find a match. It uses machine learning to generalize beyond the set of sentences seen at train time, thus making our NLU engine be able to cope with examples which are not in the scope of the training data set. This parser involves two successive steps: intent classification and slot filling. These two steps rely on trained machine learning models to classify intent and extract slots.</p>

<h2>Deterministic Intent Parser</h2>

<p>The Deterministic Intent Parse is the first step to be used. This parser relies on some regular expressions to match the intent and slots. If the new input has the same structure with one of the training examples, we will find its intent and slots by comparing the input with the matched regular expression.</p>

<p>The regular expressions are built based on the training examples. For a training case:</p>

<ul>
<li>What is the weather in [Alaska](location)</li>
</ul>


<p>We will build a regular expression:</p>

<ul>
<li>(what is the weather in)(?&lt;location1&gt;.+)</li>
</ul>


<h2>Probabilistic Intent Parser</h2>

<p>If the Deterministic Intent Parser fails to find the intent and slots, the Probabilistic Intent Parser will be used.</p>

<p>The Probabilistic Intent Parser has two steps:</p>

<ul>
<li>Intent Classification</li>
<li>Slot Filling</li>
</ul>


<p>The Intent Classification is to find the intent of the input command text, and the Slot Filling is to extract all the slots needed by the intent. These two steps are both based on trained machine models.</p>

<p>Apple has released CreateML for training natural language models, which also integrates the powerful NatrualLanguage framework functions, like Tokenization, Part of Speech, Lemmatization, Name Entity Recognition, etc. This will make the training process very simple, and the trained model will be more accurate and smaller.</p>

<h3>Intent Classification</h3>

<h4>Model Training</h4>

<p>For Intent Classification model training, we prepare the data set as follows (The size of the training data is 3282 falling into four intents.):</p>

<pre><code class="json">[
  {
    "text": "I would like the forecast in cupertino california  tomorrow", 
    "label": "searchWeatherForecast"
  }, 
  {
    "text": "Forecast in Maine USA next week", 
    "label": "searchWeatherForecast"
  }, 
...
...
  {
    "text": "Will I be able to wear open-toed shoes twenty three hours and seven minutes from now in Severn?", 
    "label": "searchWeatherForecastItem"
  }, 
  {
    "text": "Should I bring a raincoat to the Belgrade and Loreto areas of Oman at midnight?", 
    "label": "searchWeatherForecastItem"
  }, 
...
...
]
</code></pre>

<p>Apple has release CreateML framework for training machine learning models easily inside Swift playground and the trained model can be saved as mlmodel type. And the MLTextClassifier class from CreateML will benefit from Apple&rsquo;s NatrualLanguage framework for Tokenization, Part of Speech, Lemmatization, etc.</p>

<p>The training script is:</p>

<pre><code class="swift">let trainingDataPath = Bundle.main.path(forResource: "intentClassificationFile", ofType: "json", inDirectory: "Data/text/train")!
let trainingData = try! MLDataTable(contentsOf:  URL(fileURLWithPath: trainingDataPath))

// Initializing the classifier with a training data.
let classifier = try! MLTextClassifier(trainingData: trainingData, textColumn: "text", labelColumn: "label")

// Evaluating training &amp; validation accuracies.
let trainingAccuracy = (1.0 - classifier.trainingMetrics.classificationError) * 100
let validationAccuracy = (1.0 - classifier.validationMetrics.classificationError) * 100

// Initializing the properly labeled testing data from Resources folder.
let testingDataPath = Bundle.main.path(forResource: "intentClassificationFile", ofType: "json", inDirectory: "Data/text/test")!
let testingData = try! MLDataTable(contentsOf: URL(fileURLWithPath:testingDataPath))

// Counting the testing evaluation.
let evaluationMetrics = classifier.evaluation(on: testingData)
let evaluationAccuracy = (1.0 - evaluationMetrics.classificationError) * 100

// Confusion matrix in order to see which labels were classified wrongly.
let confusionMatrix = evaluationMetrics.confusion
print("Confusion matrix: \(confusionMatrix)")

// Metadata for saving the model.
let metadata = MLModelMetadata(author: "Hongchao Zhang",
                            shortDescription: "A model trained to classify weather related commands.",
                            version: "1.0")

// Saving the model. Remember to update the path.
try! classifier.write(to: URL(fileURLWithPath: "/Users/hozhang/Downloads/textClassifier.mlmodel"),
                    metadata: metadata)
</code></pre>

<p>We can get 99.23% training accuracy and 98.87% validation accuracy.</p>

<h4>Model Usage</h4>

<p>For the trained model of mlmodel type, we can use it in our iOS app through NLModel (from NatrualLanguage framework). The demo swift code may be like:</p>

<pre><code class="swift">let modelUrl = Bundle.main.url(forResource: "Data/text/textClassifier", withExtension: "mlmodel")
let compiledModelUrl = try! MLModel.compileModel(at: modelUrl!)
let classifier = try! NLModel(contentsOf: compiledModelUrl)

let text = requestText
let label = classifier.predictedLabel(for: text)

print("text: \(text)\nlabel:\(label ?? "Not detected!")")
</code></pre>

<h3>Slot Filling</h3>

<h4>Model Training</h4>

<p>For Slot Filling model training, we prepare the data set as follows (The size of the training data is: 3282.):</p>

<pre><code class="json">[
  {
    "tokens": ["I", "would", "like", "the", "forecast", "in", "california", "tomorrow"], 
    "labels": ["none", "none", "none", "none", "none", "none", "location", "date"]
  }, 
  {
    "tokens": ["Forecast", "in", "Maine", "next week"], 
    "labels": ["none", "none", "location", "date"]
  }, 
...
...
]
</code></pre>

<p>Like Intent Classification model training, CreateML framework also makes it easy. Like MLTextClassifier, the MLWordTagger class from CreateML will also benefit from NatrualLanguage framework for Part of Speech, Lemmatization, Name Entity Recognition, etc.</p>

<p>The training script is:</p>

<pre><code class="swift">// Initializing the training data from Resources folder.
let trainingDataPath = Bundle.main.path(forResource: "slotParsingFile", ofType: "json", inDirectory: "Data/text/train")!
let trainingData = try! MLDataTable(contentsOf:  URL(fileURLWithPath: trainingDataPath))

// Initializing the classifier with a training data.
let classifier = try! MLWordTagger(trainingData: trainingData, tokenColumn: "tokens", labelColumn: "labels")

// Evaluating training &amp; validation accuracies.
let trainingAccuracy = (1.0 - classifier.trainingMetrics.taggingError) * 100
let validationAccuracy = (1.0 - classifier.validationMetrics.taggingError) * 100

// Initializing the properly labeled testing data from Resources folder.
let testingDataPath = Bundle.main.path(forResource: "slotParsingFile", ofType: "json", inDirectory: "Data/text/test")!
let testingData = try! MLDataTable(contentsOf: URL(fileURLWithPath:testingDataPath))

// Counting the testing evaluation.
let evaluationMetrics = classifier.evaluation(on: testingData)
let evaluationAccuracy = (1.0 - evaluationMetrics.taggingError) * 100

// Confusion matrix in order to see which labels were classified wrongly.
let confusionMatrix = evaluationMetrics.confusion
print("Confusion matrix: \(confusionMatrix)")

// Metadata for saving the model.
let metadata = MLModelMetadata(author: "Hongchao Zhang",
                                shortDescription: "A model trained to parse slots from weather related commands.",
                                version: "1.0")

// Saving the model. Remember to update the path.
try! classifier.write(to: URL(fileURLWithPath: "/Users/hozhang/Downloads/slotParsing.mlmodel"),
                        metadata: metadata)
</code></pre>

<p>We can get 99.64% training accuracy and 98.38% validation accuracy.</p>

<h4>Model Usage</h4>

<p>We can load the mlmodel into an NLTagger (from NatrualLanguage framework), and use the NLTagger to tag labels for each word of the input command text. The demo swift script is like:</p>

<pre><code class="swift">let weatherTagSchema = NLTagScheme("Weather")
let modelUrl = Bundle.main.url(forResource: "Data/text/slotParsing", withExtension: "mlmodel")
let compiledModelUrl = try! MLModel.compileModel(at: modelUrl!)
let taggerModel = try! NLModel(contentsOf: compiledModelUrl)

let weatherTagger = NLTagger(tagSchemes: [weatherTagSchema])
weatherTagger.setModels([taggerModel], forTagScheme: weatherTagSchema)

let text = requestText
weatherTagger.string = text
weatherTagger.enumerateTags(in: text.startIndex..&lt;text.endIndex, unit: .word, scheme: weatherTagSchema, options: []) { (tag, tokenRange) -&gt; Bool in
    if let tag = tag, tag.rawValue != "Whitespace" {
        print("\(text[tokenRange]): \(tag.rawValue)")
    }
    return true
}
</code></pre>

<h3>Model Size</h3>

<p>For the iOS app, we hope the machine learning model size is small enough. Apple&rsquo;s NatrualLanguage framework has done many optimizations on machine learning model size. The following data is from WWDC 2018 (session 713: Introducing NatrualLanguage Framework):</p>

<table>
<thead>
<tr>
<th>&ndash; </th>
<th> Open Source CRFSuite </th>
<th> Natural Language Framework</th>
</tr>
</thead>
<tbody>
<tr>
<td>Name Entity Recognition </td>
<td> 70MB  </td>
<td> 1.4MB</td>
</tr>
<tr>
<td>Chunking </td>
<td> 30MB </td>
<td> 1.8MB</td>
</tr>
</tbody>
</table>


<p>We can see that the model will be much smaller than that trained from an open source platform.</p>

<p>The size of the two models we trained is (The training data size is: 3282):</p>

<table>
<thead>
<tr>
<th>Model </th>
<th> Size</th>
</tr>
</thead>
<tbody>
<tr>
<td>Intent Classification </td>
<td> 41K</td>
</tr>
<tr>
<td>Slot Filling </td>
<td> 609K</td>
</tr>
</tbody>
</table>


<h3>Problems to Be Solved</h3>

<p>For Probabilistic Intent Parser, we still have some problems.</p>

<h4>Intent Classification Model Has No Probability Output</h4>

<p>We may need the probability to define the reliability of the estimated intent of an input command text.</p>

<p>However, the model trained through <code>MLTextClassifier</code> has no probability output API. If we really need the probability output, we can use other platforms to train the model, like tensorflow. That way, we will not benefit from NatrualLanguage framework and we need to consider these things by ourselves, like Tokenization, Part of Speech, Lemmatization, etc.</p>

<h4>Slot Filling Model Tagges the Label by Words, not Phrase</h4>

<p>The NLTagger class only supply the following four tag level: word, sentence, paragraph, and document. There is no &ldquo;phrase&rdquo; tag level. For example, &ldquo;New York&rdquo; will be treated as &ldquo;New&rdquo; and &ldquo;York&rdquo;, and the tagged label will both be &ldquo;location&rdquo;. We need to compose them together manually.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[关于AR的一些使用场景]]></title>
    <link href="http://hongchaozhang.github.io/blog/2018/08/17/some-ideas-on-ar-usage/"/>
    <updated>2018-08-17T15:13:58+08:00</updated>
    <id>http://hongchaozhang.github.io/blog/2018/08/17/some-ideas-on-ar-usage</id>
    <content type="html"><![CDATA[<p>搜集一些有意思的AR应用。</p>

<!-- more -->


<p>主要参考：<a href="https://github.com/olucurious/Awesome-ARKit">Awesome ARKit</a>。这篇post里面还有很多实用的AR Tutorial和Resouces，如果自己动手，可以参考。</p>

<ul>
<li><a href="#app-store%E4%B8%8A%E9%9D%A2%E7%9A%84app">App Store上面的App</a>

<ul>
<li><a href="#%E7%A5%9E%E5%A5%87ar">神奇AR</a></li>
<li><a href="#ikea-place">IKEA Place</a></li>
<li><a href="#wallr">Wallr</a></li>
<li><a href="#horizon-explorer">Horizon Explorer</a></li>
<li><a href="#weare">WeAre</a></li>
<li><a href="#waazy---magic-ar-video-maker">Waazy - Magic AR Video Maker</a></li>
<li><a href="#human-anatomy-atlas-2019">Human Anatomy Atlas 2019</a></li>
</ul>
</li>
<li><a href="#github%E4%B8%8A%E9%9D%A2%E7%9A%84%E9%A1%B9%E7%9B%AE">Github上面的项目</a>

<ul>
<li><a href="#arkit-occlusion-demo">arkit-occlusion-demo</a></li>
<li><a href="#arvideokit">[ARVideoKit]()</a></li>
<li><a href="#arkit-smb-homage">arkit-smb-homage</a></li>
<li><a href="#arkit-corelocation">ARKit-CoreLocation</a></li>
<li><a href="#arkitnavigationdemo">ARKitNavigationDemo</a></li>
<li><a href="#fineme">FineMe</a></li>
<li><a href="#arkitspitfire">ARKitSpitfire</a></li>
</ul>
</li>
<li><a href="#resources">Resources</a>

<ul>
<li><a href="#poly">Poly</a></li>
</ul>
</li>
</ul>


<h2>App Store上面的App</h2>

<h3><a href="https://itunes.apple.com/cn/app/%E7%A5%9E%E5%A5%87ar-%E7%89%B9%E6%95%88ar%E7%9B%B8%E6%9C%BA%E5%92%8C%E7%9F%AD%E8%A7%86%E9%A2%91%E6%8B%8D%E6%91%84%E7%A5%9E%E5%99%A8/id1327719623?mt=8">神奇AR</a></h3>

<p>这个AR应用非常棒，号称“中国第一AR平台”。打开App，以为自己是打开了“抖音”呢。其模仿抖音的痕迹很重，但是神奇AR的视频不同于抖音里面的视频：都是真实世界和虚拟世界的深入互动。支持下载很多3D模型。</p>

<p><a href="https://itunes.apple.com/cn/app/%E7%A5%9E%E5%A5%87ar-%E7%89%B9%E6%95%88ar%E7%9B%B8%E6%9C%BA%E5%92%8C%E7%9F%AD%E8%A7%86%E9%A2%91%E6%8B%8D%E6%91%84%E7%A5%9E%E5%99%A8/id1327719623?mt=8">神奇AR</a>还有一个很好的应用：可以让你将照片在真实场景中打开，支持默认的排列方式，也可以自由摆放多张照片。但是图片一圈都会默认带有一圈白色的过渡带和阴影，无法去除。对于不是标准长方形的图片（比如圆形的图片，圆形之外都是透明的），显示效果不佳。</p>

<p>而且，是免费的。也许是因为其主打段视频社交吧，所以免费。</p>

<p>看官方介绍：</p>

<blockquote><p>AR视频:
用户可以利用AR模型、特效、图片、视频等拍摄一段30秒的短视频，发布在神奇AR的视频流中，或者分享到各大媒体平台，将自己的创意展现给更多的人，告诉大家如何使用AR。
玩转AR:
用户可以打开AR摄像头，通过简单的操作，将AR模型放在真实世界中，创造各种神奇的景象，用AR就能创造电影里才能出现的特技。
丰富的模型:
神奇AR是一个开放内容平台，直接对接优质的AR内容提供者，他可以通过神奇AR把自己的作品第一时间开放给用户，所以我们拥有全世界最丰富的AR内容。</p></blockquote>

<p><img src="/images/shenqiar.jpg" alt="神奇 AR" /></p>

<h3><a href="https://itunes.apple.com/us/app/ikea-place/id1279244498?mt=8">IKEA Place</a></h3>

<p>宜家的官方App，有丰富的宜家家具的3D模型，真实尺寸，可以提前放置到自己的房间，看看效果。</p>

<blockquote><p>IKEA Place lets you virtually &lsquo;place&rsquo; IKEA products in your space. The app includes 3D and true-to-scale models of everything from sofas and armchairs to footstools and coffee tables. IKEA Place gives you an accurate impression of the furniture’s size, design and functionality in your home so you can stop wondering and start doing.</p></blockquote>

<p><img src="/images/IKEAPlace.jpg" alt="ikea place" /></p>

<h3><a href="https://itunes.apple.com/us/app/wallr/id1278372745">Wallr</a></h3>

<p>Wallr可以让你将图片放置到真实场景的墙面上。如果你想买画装饰墙面，可以试试。不过这个功能已经在<a href="https://itunes.apple.com/cn/app/%E7%A5%9E%E5%A5%87ar-%E7%89%B9%E6%95%88ar%E7%9B%B8%E6%9C%BA%E5%92%8C%E7%9F%AD%E8%A7%86%E9%A2%91%E6%8B%8D%E6%91%84%E7%A5%9E%E5%99%A8/id1327719623?mt=8">神奇AR</a>中实现了，而且是免费的。</p>

<p>同时放置多张图片需要花钱购买。</p>

<p><img src="/images/wallr.jpg" alt="wallr" /></p>

<h3><a href="https://itunes.apple.com/gb/app/horizon-explorer/id1326860431?platform=ipad&amp;preserveScrollPosition=true#platform/ipad">Horizon Explorer</a></h3>

<p>AR和地图、地理位置结合的一个应用。</p>

<p>展示你设备看到的地理位置的信息，包括距离、建筑物名称、地点名称等。并把路线在一张地图上面展示给你。</p>

<p>还可以将3D的地图展示给你，让你看看某个景点或者建筑物周围等地理信息。</p>

<p>来看官方介绍：</p>

<blockquote><p>Horizon Explorer shows you the horizon and skyline around you &amp; tells you what you&rsquo;re looking at.</p>

<p>Point your camera at a hill, village, lake or landmark and Horizon Explorer will tell you what you are looking at, how far away it is, and show you a map, and information about the point you&rsquo;re aiming at.</p>

<p>ARKit technology makes the labels and alignment much more stable than used to be possible.</p>

<p>Fly up high and see the terrain laid out below you to see what is over the hills around you, and get the lay of the land, then see the scale-model 3D map that you can walk around &amp; explore to find out what&rsquo;s behind hills, or investigate up close.</p>

<p>Tracking works best on top of a hill with an unobstructed view of your surroundings (close up trees, buildings, rocks etc. can confuse the tracking). You can drag the terrain with your finger to line up with the camera if the automatic tracking is not working very well. Or try waving your phone around in the air in a figure 8 to calibrate the compass.</p></blockquote>

<p><img src="/images/HorizonExplorer.jpg" alt="Horizon Explorer" /></p>

<h3><a href="https://itunes.apple.com/cn/app/weare/id1304227680?platform=iphone&amp;preserveScrollPosition=true&amp;platform=iphone#platform/iphone&amp;platform=iphone">WeAre</a></h3>

<p>这个应用可以让你选择一些照片，以设备为中心围成一圈，并缓慢移动。还可以播放视频、背景音乐和编辑3D文字。用作者的话说，“可以打造一个或温馨浪漫的回忆相册,或缥缈遥远的世界.”</p>

<p>同时还是源码可以参考：<a href="https://github.com/SherlockQi/HeavenMemoirs">HeavenMemoirs - AR相册</a></p>

<p><img src="/images/weare.jpg" alt="WeAre" /></p>

<h3><a href="https://itunes.apple.com/us/app/waazy-magic-ar-video-maker/id1286992749">Waazy - Magic AR Video Maker</a></h3>

<p>没太看懂这个应用。感觉主要做社交视频分享。录制视频还需要AR Lens。直接看官方介绍吧：</p>

<blockquote><p>Waazy is an augmented reality short video clips shooting and sharing social network, making it possible to bring virtual characters and objects to the real world.</p>

<p>Features:
- Record cool moments with AR Lens
- Tons of free and awesome AR effects
- Can add multiple AR characters at the same time
- One tap to make all the characters dance together
- Easily move and rotate a virtual character with control pad
- Themes include fantasy, monster, fun, and landmarks
- Show your original AR videos to the world</p></blockquote>

<p><img src="/images/wazzy.jpg" alt="wazzy" /></p>

<h3><a href="https://itunes.apple.com/app/id1117998129">Human Anatomy Atlas 2019</a></h3>

<p>其实这个主要是展示人体内部结构的3D素材，借助AR技术投射到真实场景，没有很新鲜的AR应用场景。</p>

<p>这个应用非常专业，下载需要钱，App内还要购买。</p>

<blockquote><p>Human Anatomy Atlas offers thousands of models to help understand and communicate how the human body looks and works&ndash;and includes textbook-level definitions. Use it as a reference, instead of an anatomy textbook, or to create virtual lab experiences.
Includes over 10,000 anatomical models with descriptions in English, Spanish, French, German, Italian, Japanese, and Simplified Chinese.</p></blockquote>

<p><img src="/images/HumanAnatomyAtlas2019_1.jpg" alt="Human Anatomy Atlas 2019 1" /></p>

<p><img src="/images/HumanAnatomyAtlas2019_2.jpg" alt="Human Anatomy Atlas 2019 2" /></p>

<h2>Github上面的项目</h2>

<h3><a href="https://github.com/bjarnel/arkit-occlusion">arkit-occlusion-demo</a></h3>

<p>事先用一些虚拟平面将真实的墙面、柜子面、门等标记出来，就可以让虚拟的小球在房间里面来回反弹，就像撞到真实的墙上返回来一样。</p>

<p><img src="/images/occlusiongame.jpg" width="600" alt="occlusion game" /></p>

<h3><a href="">ARVideoKit</a></h3>

<p>一个用来录制AR视频的框架。</p>

<blockquote><p>An iOS Framework that enables developers to capture videos, photos, Live Photos, and GIFs with ARKit content.</p>

<p>In other words, you NO LONGER have to screen record/screenshot to capture videos and photos of your awesome ARKit apps!</p></blockquote>

<p>其实录屏/截屏不是也挺好的吗？！</p>

<blockquote><p>Key Features:</p>

<p>✅ Capture Photos from <code>ARSCNView</code>, <code>ARSKView</code>, and <code>SCNView</code></p>

<p>✅ Capture Live Photos &amp; GIFs from <code>ARSCNView</code>, <code>ARSKView</code>, and <code>SCNView</code></p>

<p>✅ Record Videos from <code>ARSCNView</code>, <code>ARSKView</code>, and <code>SCNView</code></p>

<p>✅ Pause/Resume video</p>

<p>✅ Allow device&rsquo;s Music playing in the background while recording a video</p>

<p>✅ Returns rendered and raw buffers in a protocol method for additional Image &amp; Video processing</p></blockquote>

<h3><a href="https://github.com/bjarnel/arkit-smb-homage">arkit-smb-homage</a></h3>

<p>在现实场景中玩超级玛丽。非常粗糙，但是创意还不错。</p>

<p><img src="/images/supermario_beginning.jpg" width="600" alt="super mario beginning" /></p>

<p><img src="/images/supermario_flag.jpg" width="600" alt="super mario flag" /></p>

<h3><a href="https://github.com/ProjectDent/ARKit-CoreLocation">ARKit-CoreLocation</a></h3>

<p>功能：</p>

<ol>
<li>这个库最主要的工作，是在试图整合ARKit和CoreLocation，以得到更加准确的定位，从而更好地应用于AR场景。</li>
<li>基于真实地理位置，标注出摄像头中某个建筑物或者景点的标注。这个功能类似于<a href="https://itunes.apple.com/gb/app/horizon-explorer/id1326860431?platform=ipad&amp;preserveScrollPosition=true#platform/ipad">Horizon Explorer</a>。</li>
</ol>


<p>TODO: 可以仔细看看此库附带的demo：</p>

<blockquote><p>The library and demo come with a bunch of additional features for configuration. It’s all fully documented to be sure to have a look around.</p></blockquote>

<p><img src="/images/arkit+corelocation.jpg" width="600" alt="arkit + corelocation" /></p>

<h3><a href="https://github.com/chriswebb09/ARKitNavigationDemo">ARKitNavigationDemo</a></h3>

<p>在地图上选择目的地，然后在真实场景中进行AR导航。</p>

<p>但是，这个项目也只是Demo一下，作者也很谦虚：</p>

<blockquote><p>When it loads to the map, tap a place on the map where you want to navigate to and press okay. The tap can be sluggish, so you might have to try once or twice before you get it. When the navigation screen loads, tap the screen, then give it a few seconds. You should see the nodes render.</p></blockquote>

<p>TODO: 这个项目中推荐的一些参考文献还是值得看一看的。</p>

<p><img src="/images/ARKitNavigationDemo.gif" alt="ARKitNavigationDemo" /></p>

<h3><a href="https://github.com/mmoaay/Findme">FineMe</a></h3>

<p>可以让你的朋友根据你录制的路线图找到你：</p>

<ol>
<li>通过各种方法，记录你的起点，并让另一个人知道你的起点。比如可以通过分享起点照片，或者分享起点位置。</li>
<li>通过ARKit记录你走过的路线，并将路线分享给另一个人。</li>
<li>另一个人如果找到了你的起点，就可以根据你分享的路径找到你。</li>
</ol>


<p>但是，由于ARKit不稳定，此方法也不一定奏效。</p>

<p>作者试图通过定位和距离提高路线的稳定性，但不知效果如何，分别见于以下两个分支：</p>

<ul>
<li><a href="https://github.com/mmoaay/Findme/tree/feature/location_optimize">According to location</a></li>
<li><a href="https://github.com/mmoaay/Findme/tree/feature/distance_optimize">According to distance</a></li>
</ul>


<h3><a href="https://github.com/chriswebb09/ARKitSpitfire">ARKitSpitfire</a></h3>

<p>可以让一架3D飞机模型，根据提供的地理位置经纬度，调整姿态，并飞向那里。</p>

<p><img src="/images/ARKitSpitfire.gif" alt="ARKitSpitfire" /></p>

<h2>Resources</h2>

<h3><a href="https://github.com/piemonte/Poly">Poly</a></h3>

<p><a href="https://github.com/piemonte/Poly">Poly</a>是一个iOS库，用来从<a href="https://developers.google.com/poly/">Google Poly</a>上下载3D模型，包含搜索、下载管理和缓存功能。</p>

<p><a href="https://developers.google.com/poly/develop/ios">iOS Quickstart</a>展示了如何在iOS中下载使用<a href="https://developers.google.com/poly/">Google Poly</a>上面的3D资源。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Introducing Create ML]]></title>
    <link href="http://hongchaozhang.github.io/blog/2018/06/15/introducing-create-ml/"/>
    <updated>2018-06-15T17:39:30+08:00</updated>
    <id>http://hongchaozhang.github.io/blog/2018/06/15/introducing-create-ml</id>
    <content type="html"><![CDATA[<!-- more -->


<p>Apple released Core ML in WWDC2017, and I took a note on <a href="../../../../2017/12/28/coreml-usage/">CoreML Usage</a>, including mlmodel training using Microsoft <a href="https://www.customvision.ai/">Custom Vision</a>.</p>

<p>This post is about the background of Create ML, its advantages, its relations with Turi. There is no code in this post. If you are looking for the usage of Create ML, refer to <a href="https://www.raywenderlich.com/196233/create-ml-tutorial-getting-started">Create ML Tutorial: Getting Started</a>.</p>

<h2>Background</h2>

<p>Before 2018, where can we get the mlmodel file used in iOS and macOS?</p>

<ul>
<li><a href="https://github.com/tf-coreml/tf-coreml">TensorFlow</a>: Train machine learning models and easily convert them to the Core ML Model format.</li>
<li><a href="https://www.customvision.ai/">Custom Vision</a> from Microsoft</li>
<li><a href="https://pypi.org/project/coremltools/">Core ML Tools</a>: Use this python package to convert models from machine learning toolboxes into the Core ML format.</li>
<li><a href="https://github.com/apache/incubator-mxnet/tree/master/tools/coreml">Apache MXNet</a>: Train machine learning models and convert them to the Core ML format.</li>
<li><a href="https://github.com/onnx/onnx-coreml">ONNX</a>: Convert ONNX models you have created to the Core ML Model format.</li>
<li>&hellip;</li>
</ul>


<p>TensorFlow doesn&rsquo;t support GPU on macOS from version 1.2.</p>

<p><img src="/images/tensorflow_not_support_gpu_on_macos.jpg" alt="tensor flow not support gpu on macos" /></p>

<p><strong>Core ML</strong>: Announced at WWDC 2017, and already supported by every major ML platform to convert existing models. But the existing models tend to be too big and/or too general.</p>

<p><strong>Turi Create</strong>: Acquired by Apple in 2016 ($200M), it lets you customize existing models with your own data. But … Python :[.</p>

<h2>Create ML</h2>

<p>Finally in WWDC2018, Apple announced <strong>Create ML</strong>, which can train machine learning models on macOS, able to use the GPU on macOS. The Create ML session and Turi Create session did not mention any word on each other, but obviousely, Create ML is based on Turi Create.</p>

<p>Based on Trui&rsquo;s model training, Create ML can make model training on macOS using GPU (maybe through Metal), and come up with models which can be used by Core ML framework.</p>

<p>With XCode Playground&rsquo;s updates, Apple gives CreateMLUI, a very easy way for model training: just need to drag your training data and test data into Playground.</p>

<h2>Main Advantage: Easy to Use</h2>

<p>Do model training using Swift in XCode.</p>

<blockquote><p><strong>Create ML</strong> is proof that Apple is committed to making it easier for you to use machine learning models in your apps. In this Create ML tutorial, you’ll learn how Create ML speeds up the workflow for improving your model by improving your data while also flattening the learning curve by doing it all in the comfort of Xcode and Swift.</p>

<p>&ndash;Refer to <a href="https://www.raywenderlich.com/196233/create-ml-tutorial-getting-started">Create ML Tutorial: Getting Started</a></p>

<p><strong>Create ML</strong>: Announced at WWDC 2018. ML in Xcode &amp; Swift! Currently includes only two of Turi Create’s seven task-focused toolkits, plus a generic classifier and regressor, and data tables. I see it as a trail of breadcrumbs leading you to the Turi Create gingerbread house, inhabited by a “good dog” instead of a witch! (Turi Create’s logo is a dog silhouette.)</p>

<p>&ndash;Refer to <a href="https://www.raywenderlich.com/196233/create-ml-tutorial-getting-started">Create ML Tutorial: Getting Started</a></p></blockquote>

<p>Refer to <a href="https://www.raywenderlich.com/196233/create-ml-tutorial-getting-started">Create ML Tutorial: Getting Started</a> to see how easy it is to use Create ML. There are some code comparasion between Create ML and Turi Create.</p>

<h3>Create ML和Turi Create</h3>

<p>Currently Create ML includes only two of Turi Create’s seven task-focused toolkits, plus a generic classifier and regressor, and data tables. Turi Create has five task-focused toolkits that aren’t (yet?) in Create ML:</p>

<ul>
<li>Recommender systems</li>
<li>Image similarity</li>
<li>Object detection</li>
<li>Style transfer</li>
<li>Activity classification</li>
</ul>


<h2>Transfer Learning</h2>

<p>The description of Transfer Learning from Apple Turi:</p>

<blockquote><p>It’s not uncommon for the task you want to solve to be related to something that has already been solved. Take, for example, the task of distinguishing cats from dogs. The famous ImageNet Challenge, for which CNN’s are the state-of-the-art, asks the trained model to categorize input into one of 1000 classes. Shouldn&rsquo;t features that distinguish between categories like lions and wolves also be useful for discriminating between cats and dogs?</p>

<p>The answer is a definitive yes. It is accomplished by simply removing the output layer of the Deep Neural Network for 1000 categories, and taking the signals that would have been propagating to the output layer and feeding them as features to a classifier for our new cats vs dogs task.</p>

<p>So, when you run the Turi Create image classifier, it breaks things down into something like this:</p>

<p>Stage 1: Create a CNN classifier on a large, general dataset. A good example is ImageNet, with 1000 categories and 1.2 million images. The models are already trained by researchers and are available for us to use.</p>

<p>Stage 2: The outputs of each layer in the CNN can be viewed as a meaningful vector representation of each image. Extract these feature vectors from the layer prior to the output layer on each image of your task.</p>

<p>Stage 3: Create a new classifier with those features as input for your own task.</p>

<p>At first glance, this seems even more complicated than just training the deep learning model. However, Stage 1 is reusable for many different problems, and once done, it doesn&rsquo;t have to be changed often.</p>

<p>In the end, this pipeline results in not needing to adjust hyper-parameters, faster training, and better performance even in cases where you don&rsquo;t have enough data to create a convention deep learning model. What&rsquo;s more, this technique is effective even if your Stage 3 classification task is relatively unrelated to the task Stage 1 is trained on. This idea was first explored by Donahue et al. (2013), and has since become one of the best ways to create image classifier models.</p>

<p>&ndash;Refer to <a href="https://apple.github.io/turicreate/docs/userguide/image_classifier/how-it-works.html#transfer-learning">truicreate transfer learning</a></p></blockquote>

<p>Some comments on transfer learning from web:</p>

<blockquote><p>What’s happening here? It’s called transfer learning, if you want to look it up. The underlying model — VisionFeaturePrint_Screen, which backs the Vision framework — was pre-trained on a ginormous dataset to recognize an enormous number of classes. It did this by learning what features to look for in an image, and how to combine these features to classify the image. Almost all of the training time for your dataset is the model extracting around 1000 features from your images. These could include low-level shapes and textures and higher-level shape of ears, distance between eyes, shape of snout. Then it spends a relatively tiny amount of time training a logistic regression model to separate your images into two classes. It’s similar to fitting a straight line to scattered points, but in 1000 dimensions instead of 2. But it’s still very quick to do: my run 1m 15s for feature extraction and 0.177886 seconds to train and apply the logistic regression.</p>

<p>Transfer learning only works successfully when features of your dataset are reasonably similar to features of the dataset that was used to train the model. A model pre-trained on ImageNet — a large collection of photos — might not transfer well to pencil drawings or microscopy images.</p>

<p>&ndash;Refer to <a href="https://www.raywenderlich.com/196233/create-ml-tutorial-getting-started">Create ML Tutorial: Getting Started</a></p></blockquote>

<h2>Improving Accuracy</h2>

<p>Refer to <a href="https://developer.apple.com/documentation/create_ml/improving_your_model_s_accuracy">Improving Your Model’s Accuracy</a> from Apple for improving training accuracy.</p>

<p>How to improve the model&rsquo;s training accuracy, validation accuracy and evaluation accuracy. <a href="https://www.raywenderlich.com/196233/create-ml-tutorial-getting-started">Create ML Tutorial: Getting Started</a> describes the three &lsquo;accuracy&rsquo;s.</p>

<h2>References and Materials</h2>

<ol>
<li><p>You might like to browse two fascinating articles about features from (mostly) Google Brain/Research:</p>

<ul>
<li><p><a href="https://distill.pub/2018/building-blocks/">The Building Blocks of Interpretability</a>: image feature extracting</p></li>
<li><p><a href="https://distill.pub/2017/feature-visualization/">Feature Visualization</a></p></li>
</ul>
</li>
<li><p><a href="https://www.kaggle.com/">Kaggle</a> is a repository of datasets contributed by members, often supplemented by notebooks that analyze and visualize the data. It runs model prediction competitions, which leads to the next link:</p>

<ul>
<li>Machine Learning Zero-to-Hero: Everything you need in order to compete on Kaggle for the first time, step-by-step!</li>
</ul>
</li>
<li><p><a href="https://www.raywenderlich.com/196233/create-ml-tutorial-getting-started">Create ML Tutorial: Getting Started</a> describes the usage of Create ML and Turi Create, including the history, code, data preparation, improving model accuracy and so on.</p></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[What is new in ARKit 2]]></title>
    <link href="http://hongchaozhang.github.io/blog/2018/06/13/what-is-new-in-n-arkit-2/"/>
    <updated>2018-06-13T13:24:59+08:00</updated>
    <id>http://hongchaozhang.github.io/blog/2018/06/13/what-is-new-in-n-arkit-2</id>
    <content type="html"><![CDATA[<!-- more -->


<h2>Overview</h2>

<p>In ARKit 1, we have:</p>

<ul>
<li>Device positioning from world tracking process</li>
<li>Horizontal and vertical plane detection from world tracking process</li>
<li>Lighting estimation</li>
<li>AR face tracking</li>
</ul>


<p>In ARKit 2, we have:</p>

<ul>
<li>Saving and loading maps</li>
<li>Environment Texturing</li>
<li>Image detection and tracking</li>
<li>3D object tracking</li>
<li>Improved face tracking</li>
</ul>


<h2>New Features in ARKit 2</h2>

<h3>Saving and Loading Maps</h3>

<h4>World Tracking Recap:</h4>

<ul>
<li>Position and orientation of the device.</li>
<li>Physical scale in the scene.</li>
<li>3D feature points.</li>
<li>Relocalization (iOS 11.3): we can relocalize objects when your AR session is interrupted, like phone coming or going from background. This feature is implemented by storing the mapping <code>ARWorldMap</code> between real world and the coordinate system. However the mapping is not exposed to developers.</li>
</ul>


<h4>World Tracking Enhancement:</h4>

<ul>
<li><strong>Saving and loading maps</strong>: expose the <code>ARWorldMap</code> to developers.</li>
<li>Faster initialization and plane detection</li>
<li>Robust tracking and plane detection</li>
<li>More accurate extent and boundary Continuous autofocus</li>
<li>New 4:3 video formats (iPad is also 4:3)</li>
</ul>


<h4>Saving and loading maps:</h4>

<p><code>ARWorldmap</code> contains:</p>

<ul>
<li>Mapping of physical 3D space: for representing 3D feature points in the coordinate system.</li>
<li>Mutable list of named anchors: for restoring previous 3D environment (like lighting node anchor), and relocalizing previously added virtual objects.</li>
<li>Raw feature points and extent: for debugging and visualization.</li>
<li>Serialization: for storing and recovering from an file.</li>
</ul>


<p><img src="/images/arkit2_arworldmap.jpg" width="500" alt="arkit arworldmap" /></p>

<p>We can use the map in two different ways:</p>

<ul>
<li>Persistent: Restore previous AR scene for a new AR session. For example, you go to another room and come back or close the AR app and open it some time later.</li>
<li>Multiuser experience: We can share the map among devices through WiFi or bluetooth.</li>
</ul>


<p>The SwiftShot is an multiuser experience AR game:</p>

<p><img src="/images/swiftshot.jpg" alt="arkit2 swiftshot" /></p>

<p>and the following is a small piece of the demo:</p>

<p><img src="/images/arkit2_multiuser_experience_demo.gif" alt="swift shot game" /></p>

<h4>How to get a good map</h4>

<p>In order to share or restore the map, we need to get a good one first. A good map should be:</p>

<!-- * Important for relocalization -->


<ul>
<li>Multiple points of view: If we record the mapping from one point of view, and try to restore the coordinate system from another point of view, it will fail.</li>
<li>Static, well-textured environment.</li>
<li>Dense feature points on the map.</li>
</ul>


<p>We can use the <code>WorldMappingStatus</code> status from <code>ARFrame</code> to decide if the current map is good enough for sharing or storing:</p>

<pre><code class="swift">public enum WorldMappingStatus : Int {
   case notAvailable
   case limited
   case extending
   case mapped 
}
</code></pre>

<h3>Environment Texturing</h3>

<p>With the help of Environment Texturing, AR scene objects can reflect the environment texture on the surface of themselves, just like:</p>

<p><img src="/images/arkit2_environment_texturing_demo.jpg" alt="arkit2 environment texturing demo" /></p>

<h3>Image Tracking</h3>

<p>Moving objects can not be positioned in ARKit 1. In ARKit 2, specified images can be tracked in AR scene.</p>

<p><img src="/images/arkit2_image_tracking.gif" alt="arkit 2 image tracking" /></p>

<p>The classes in ARKit 2 for image tracking are:</p>

<p><img src="/images/arkit2_image_tracking_classes.jpg" alt="arkit 2 image tracking classes" /></p>

<p>The detected <code>ARImageAnchor</code>s have properties like:</p>

<pre><code class="swift">open class ARImageAnchor : ARAnchor, ARTrackable { 
    public var isTracked: Bool { get }
    open var transform: simd_float4x4 { get }
    open var referenceImage: ARReferenceImage { get }
}
</code></pre>

<p>The specified image should:</p>

<ul>
<li>Histogram should be broad</li>
<li>Not have multiple uniform color regions</li>
<li>Not have repeated structures</li>
</ul>


<p>The following is the demo:</p>

<p><img src="/images/arkit2_image_tracking_demo.gif" alt="arkit 2 image tracking demo" /></p>

<p>The inputs of the above demo are:</p>

<ul>
<li>an static image of the cat, the same as it is in the picture frame</li>
<li>an video of the cat</li>
</ul>


<p>The video is played at the position of the specified picture frame, with the same orientation of the picture frame.</p>

<p>There are two classes related to image tracking:</p>

<table>
<thead>
<tr>
<th>ARImageTrackingConfiguration </th>
<th> ARWorldTrackingConfiguration</th>
</tr>
</thead>
<tbody>
<tr>
<td>Has No World Origin </td>
<td> Has World Origin</td>
</tr>
<tr>
<td>After detecting the image, only do things inside the place of the image. </td>
<td> After detecting the image, place some virtual objects outside the detected image plane.</td>
</tr>
</tbody>
</table>


<h3>3D Object Detection</h3>

<p>3D object detection workflow is:</p>

<p><img src="/images/arkit2_3D_object_tracking_classes.jpg" alt="arkit2 3D object tracking classes" /></p>

<p>The <code>ARObjectAnchor</code> contains properties like:</p>

<pre><code class="swift">open class ARObjectAnchor : ARAnchor {
    open var transform: simd_float4x4 { get }
    open var referenceObject: ARReferenceObject { get }
}
</code></pre>

<p>and <code>ARReferenceObject</code> is the scanned 3D object:</p>

<pre><code class="swift">open class ARReferenceObject
    : NSObject, NSCopying, NSSecureCoding {
    open var name: String?
    open var center: simd_float3 { get }
    open var extent: simd_float3 { get }
    open var rawFeaturePoints: ARPointCloud { get }
}
</code></pre>

<blockquote><p>An <code>ARReferenceObject</code> contains only the spatial feature information needed for ARKit to recognize the real-world object, and is not a displayable 3D reconstruction of that object.</p></blockquote>

<p>In order to get the <code>ARReferenceObject</code>, we should scan the real object, and store the result as an file (.arobject) or an xcode asset catalog for ARKit to use. Fortunately, Apple supplies a demo for scanning 3D object to get the <code>ARReferenceObject</code>. Refer to: <a href="https://developer.apple.com/documentation/arkit/scanning_and_detecting_3d_objects">Scanning and Detecting 3D Objects</a> for detail and the rough steps of object scanning are:</p>

<p><img src="/images/arkit2_3D_object_scan.jpg" alt="arkit2 3D object scan" /></p>

<p>For scanned object in the real world, we can dynamically add some info around it (Museum is a good use case.), like the demo does:</p>

<p><img src="/images/arkit2_3D_object_tracking_demo.gif" alt="arkit2 object tracking demo" /></p>

<h3>Face Tracking Enhancements</h3>

<p>With face tracking, we can place something on it or around it.</p>

<p>Enhancements in ARKit 2:</p>

<ul>
<li>Gaze tracking</li>
<li>Tongue support</li>
</ul>


<blockquote><p>Gaze and Tongue can be input of the AR app.</p></blockquote>

<p>New changes in one screenshot:</p>

<p><img src="/images/what-is-new-in-arkit-2.jpg" alt="what-is-new-in-arkit-2" /></p>

<h2>Some other WWDC Sessions Related to AR</h2>

<h3><a href="https://developer.apple.com/videos/play/wwdc2018/603/">Integrating Apps and Content with AR Quick Look</a></h3>

<p>A deeper dive into a new feature in iOS that provides a way to preview any AR object from a USDZ file.</p>

<p><img src="/images/QLPreviewController.png" alt="QLPreviewController" /></p>

<ul>
<li>There’s a great sequence diagram presented (see above) (I wish more sessions would have these!) for previewing USDZ objects, of which the <code>QLPreviewController</code> plays a central role.</li>
<li>For web developers, it covers HTML samples for how to preview USDZ objects in Safari.</li>
<li>Then it goes into a deep dive on how to create the actual USDZ objects, with more examples on new AR texturing capabilities.</li>
<li>There’s also a quick overview on how to optimize the files, to keep the size down, and there’s a breakdown of the files that make up the USDZ format.</li>
</ul>


<h3><a href="https://developer.apple.com/videos/play/wwdc2018/605/">Inside SwiftShot: Creating an AR Game</a></h3>

<p>Covers world map sharing, networking, and the physics of how to build an AR game, as well as some design insight (I have limited game dev experience so I’ll do the best I can below).</p>

<ul>
<li>Pointers to remember with designing an AR game, such as “encouraging” the user to slowly move the device for world mapping!</li>
<li>It demonstrates the usage of image &amp; object detection, world map sharing, and iBeacons for the game.</li>
<li>Integrating <code>ARKit</code> with <code>SceneKit</code> and <code>Metal</code>, including the translation of physics data between each — position, velocity, and orientation.</li>
<li>Performance enhancement with the <code>BitStreamCodable</code> protocol.</li>
<li>A small look at how audio was integrated into the game.</li>
</ul>


<h3><a href="https://developer.apple.com/videos/play/wwdc2018/805/">Creating Great AR Experiences</a></h3>

<p>Best practises mainly from a UX &amp; design perspective (there are no code samples in this session).</p>

<ul>
<li>Logical dos and don’ts that may be useful, if you need help with thought towards product and empathy towards the user.</li>
<li>They emphasize the importance of using transitions between AR scapes.</li>
<li>Why AR is a special combination of touch and movement.</li>
<li>They advise that minimal battery impact should be a huge focus! This is a challenge, given that they recommend to render the FPS at 60 to avoid latency.</li>
<li>There’s a lengthy demonstration of creating an AR fireplace, with complex texturing, etc. It looks great, but unfortunately there were no coding samples accompanying the demo.</li>
</ul>


<h3><a href="https://developer.apple.com/videos/play/wwdc2018/610/">Understanding ARKit Tracking and Detection</a></h3>

<p>A good broad overview of all of the main AR concepts.</p>

<ul>
<li>This is such a good intro into not only AR on iOS, but AR in general, that it should have been part of 2017’s sessions when ARKit was first introduced. Better late than never. If you’re only going to watch one session, watch this one!</li>
<li>It recaps the main features of ARKit — <strong>orientation</strong>, <strong>world tracking</strong>, and <strong>plane detection</strong>, and demos all of these in depth with coding samples.</li>
<li>It then demos the new features of ARKit 2 — <strong>shared world mapping</strong>, <strong>image tracking</strong>, and <strong>object detection</strong> (which has been available in the Vision framework recapped above, but is now also accessible in ARKit).</li>
<li>A good explanation on a core AR principle, <strong>Visual Inertial Odometry</strong>, is given. Short of going into the actual physics equations behind it, this should give you a great understanding of VIO.</li>
</ul>


<h2>Some other materials for a better AR app:</h2>

<h3><a href="https://developer.apple.com/documentation/arkit/building_your_first_ar_experience">Building Your First AR Experience</a></h3>

<p>This document demos an app for basic usage of ARKit.</p>

<h3><a href="https://developer.apple.com/documentation/arkit/managing_session_lifecycle_and_tracking_quality">Managing Session Lifecycle and Tracking Quality</a></h3>

<p>Make your AR experience more robust by</p>

<ul>
<li>providing clear feedback, using <code>ARCamera.TrackingState</code>.</li>
<li>recovering from interruptions, using <code>ARCamera.TrackingState.Reason.relocalizing</code>.</li>
<li>resuming previous sessions, using <code>ARWorldMap</code>.</li>
</ul>


<h3><a href="https://developer.apple.com/design/human-interface-guidelines/ios/system-capabilities/augmented-reality/">Human Interface Guidelines - Augmented Reality</a></h3>

<p>This post describes how to rendering virtual objects, how to interact with virtual objects, how to handling interruptions. It is for UX.</p>

<h3><a href="https://developer.apple.com/documentation/arkit/handling_3d_interaction_and_ui_controls_in_augmented_reality">Handling 3D Interaction and UI Controls in Augmented Reality</a></h3>

<p>This document describes the best practices for visual feedback, gesture interactions, and realistic rendering in AR experiences. And a demo app is supplied.</p>

<p><img src="/images/arkit_demo_screenshot.jpg" alt="arkit demo" /></p>

<h3><a href="https://developer.apple.com/documentation/arkit/creating_a_multiuser_ar_experience">Creating a Multiuser AR Experience</a></h3>

<p>This document demos an app (with source code) on how to transmit ARKit world-mapping data between nearby devices with the <a href="https://developer.apple.com/documentation/multipeerconnectivity">MultipeerConnectivity</a> framework (introduced in iOS 7.0) to create a shared basis for AR experiences. MultipeerConnectivity supports peer-to-peer connectivity and the discovery of nearby devices. With MultipeerConnectivity, you can not only share <code>ARWorldMap</code>, but also some actions. This makes multiuser AR game possible.</p>

<p>However:</p>

<ul>
<li>Recording and transmitting a world map and relocalizing to a world map are time-consuming, bandwidth-intensive operations. A good design is needed for better performance.</li>
<li>The persons received the world map data need to move their device so they see a similar perspective (also sent by the host) helps ARKit process the received map and establish a shared frame of reference for the multiuser experience.</li>
</ul>


<h3><a href="https://developer.apple.com/documentation/arkit/swiftshot_creating_a_game_for_augmented_reality">SwiftShot: Creating a Game for Augmented Reality</a></h3>

<p>This document demos the SwiftShot game shown on WWDC 2018, including:</p>

<ul>
<li>Designing Gameplay for AR</li>
<li>Using Local Multipeer Networking and Sharing World Maps</li>
<li>Synchronizing Gameplay Actions</li>
<li>Solving Multiplayer Physics</li>
</ul>


<h3><a href="https://developer.apple.com/documentation/arkit/recognizing_images_in_an_ar_experience">Recognizing Images in an AR Experience</a></h3>

<p>Detect known 2D images in the user’s environment, and use their positions to place AR content.</p>

<h3><a href="https://developer.apple.com/documentation/arkit/scanning_and_detecting_3d_objects">Scanning and Detecting 3D Objects</a></h3>

<p>Record spatial features of real-world objects, then use the results to find those objects in the user’s environment and trigger AR content.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[CNN与图像识别]]></title>
    <link href="http://hongchaozhang.github.io/blog/2018/05/02/cnn-and-image-classification/"/>
    <updated>2018-05-02T18:55:44+08:00</updated>
    <id>http://hongchaozhang.github.io/blog/2018/05/02/cnn-and-image-classification</id>
    <content type="html"><![CDATA[<ol>
<li>卷积神经网络用于图像识别的现状。</li>
<li>卷积与图像处理基础知识。</li>
<li>卷积神经网络每一层的可视化，了解神经网络内部的物理意义。</li>
<li>图像卷积滤波器与神经元和权重的关系。</li>
</ol>


<!-- more -->


<p><br></p>

<!-- TOC -->


<ul>
<li><a href="#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB%E8%83%8C%E6%99%AF">卷积神经网络与图像识别背景</a>

<ul>
<li><a href="#lenet%E6%A6%82%E8%BF%B0">LeNet概述</a></li>
<li><a href="#imagenet%E5%A4%A7%E8%A7%84%E6%A8%A1%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB%E6%8C%91%E6%88%98%E8%B5%9B">ImageNet大规模图像识别挑战赛</a></li>
</ul>
</li>
<li><a href="#%E5%8D%B7%E7%A7%AF%E4%B8%8E%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86">卷积与图像处理</a>

<ul>
<li><a href="#%E5%8D%B7%E7%A7%AF%E7%9A%84%E5%AE%9A%E4%B9%89">卷积的定义</a></li>
<li><a href="#%E5%9B%BE%E5%83%8F%E5%8D%B7%E7%A7%AF%E7%9A%84%E7%89%A9%E7%90%86%E6%84%8F%E4%B9%89">图像卷积的物理意义</a></li>
</ul>
</li>
<li><a href="#lenet%E8%AF%A6%E8%A7%A3">LeNet详解</a></li>
<li><a href="#%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB%E8%BF%87%E7%A8%8B%E5%8F%AF%E8%A7%86%E5%8C%96">手写数字识别过程可视化</a></li>
<li><a href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E7%A5%9E%E7%BB%8F%E5%85%83%E5%92%8C%E6%9D%83%E9%87%8D%E5%9C%A8%E5%93%AA%E9%87%8C">神经网络中的“神经元”和“权重”在哪里？</a></li>
</ul>


<!-- /TOC -->


<p><a id="markdown-卷积神经网络与图像识别背景" name="卷积神经网络与图像识别背景"></a></p>

<h2>卷积神经网络与图像识别背景</h2>

<p><a id="markdown-lenet概述" name="lenet概述"></a></p>

<h3>LeNet概述</h3>

<p>LeNet是最早用于深度学习了领域的卷积神经网络之一。Yann LeCun的这一杰作得名于他自1988年以来的系列成功迭代。彼时LeNet架构还主要被用于识别邮政编码等任务。LeNet的基本架构如下：</p>

<p><img src="/images/201805lenet.jpg" alt="lenet" /></p>

<p>近几年已经出现了很多建立在LeNet之上的新架构，但是基本概念还是来自于LeNet。</p>

<p>卷积神经网络始自1990年代起，我们已经认识了最早的LeNet，其他一些很有影响力的架构列举如下：</p>

<ul>
<li>1990s至2012：从90年代到2010年代早期，卷积神经网络都处于孵化阶段。随着数据量增大和计算能力提高，卷积神经网络能搞定的问题也越来越有意思了。</li>
<li>AlexNet(2012)：2012年，Alex Krizhevsky发布了AlexNet，是LeNet的更深、更宽版本，并且大比分赢得了当年的ImageNet大规模图像识别挑战赛(ILSVRC)。这是一次非常重要的大突破，现在普及的卷积神经网络应用都要感谢这一壮举。</li>
<li>ZF Net(2013)：2013年的ILSVRC赢家是Matthew Zeiler和Rob Fergus的卷积网络，被称作ZF Net，这是调整过架构超参数的AlexNet改进型。</li>
<li>GoogleNet(2014)：2014的ILSVRC胜者是来自Google的Szegedy et al.。其主要贡献是研发了Inception Module，它<strong>大幅减少了网络中的参数数量（四百万，相比AlexNet的六千万）</strong>。</li>
<li>VGGNet(2014)：当年的ILSVRC亚军是VGGNet，突出贡献是展示了网络的深度（层次数量）是良好表现的关键因素。</li>
<li>ResNet(2015)： Kaiming He研发的Residual Network是2015年的ILSVRC冠军，也代表了卷积神经网络的最高水平，同时还是实践的默认选择（2016年5月）。</li>
<li>DenseNet（2016年8月）： 由Gao Huang发表，Densely Connected Convolutional Network的每一层都直接与其他各层前向连接。DenseNet已经在五个高难度的物体识别基础集上，显式出非凡的进步。</li>
</ul>


<p><a id="markdown-imagenet大规模图像识别挑战赛" name="imagenet大规模图像识别挑战赛"></a></p>

<h3>ImageNet大规模图像识别挑战赛</h3>

<p>参考<a href="http://www.sohu.com/a/143751643_473283">一个时代的终结：ImageNet竞赛2017是最后一届，WebVision 竞赛或接</a>。</p>

<p>上面的变种卷积神经网络基本上都来自一项比赛（DenseNet除外）：<strong>ImageNet大规模图像识别挑战赛</strong>(ImageNet Large Scale Visual Recognition Competition，ILSVRC)。</p>

<p>ILSVRC是基于ImageNet图像库的一个图像识别比赛。ImageNet可以说是计算机视觉研究人员进行大规模物体识别和检测时，最先想到的视觉大数据来源。ImageNet 数据集最初由斯坦福大学李飞飞等人在CVPR 2009的一篇论文中推出，并被用于替代 PASCAL数据集（后者在数据规模和多样性上都不如 ImageNet）和LabelMe数据集（在标准化上不如ImageNet）。</p>

<p>ImageNet不但是计算机视觉发展的重要推动者，也是这一波深度学习热潮的关键驱动力之一。截至2016年，ImageNet中含有超过1500万由人手工注释的图片网址，也就是带标签的图片，标签说明了图片中的内容，超过2.2万个类别。</p>

<p>CVPR2017研讨会“超越ILSVRC”将宣布今年是 ImageNet 竞赛正式组织的最后一年，2016年ILSVRC 的图像识别错误率已经达到约2.9%，不仅远远超越人类（5.1%），今后再进行这类竞赛意义也不大了。这无疑标志着一个时代的结束，但也是新征程的开始：未来，计算机视觉的重点在图像理解，而作为ILSVRC替代者的候选人之一是苏黎世理工大学和谷歌等联合提出的 WebVision Challenge，也将于CVPR2017同期举办，内容侧重于学习和理解网络数据。</p>

<p>历届ILSVRC的作品，可以参考<a href="https://blog.csdn.net/kangroger/article/details/56522132">ILSVRC历届冠军论文笔记</a>，包含模型框架和识别率等。</p>

<p><a id="markdown-卷积与图像处理" name="卷积与图像处理"></a></p>

<h2>卷积与图像处理</h2>

<p><a id="markdown-卷积的定义" name="卷积的定义"></a></p>

<h3>卷积的定义</h3>

<p><img src="/images/201805convolution_definition.gif" alt="convolution definition demo" /></p>

<p>参考<a href="https://en.wikipedia.org/wiki/Convolution">Convolution</a>。</p>

<p><a id="markdown-图像卷积的物理意义" name="图像卷积的物理意义"></a></p>

<h3>图像卷积的物理意义</h3>

<p>卷积矩阵也叫“滤波器”、“核”或“特征探测器”。</p>

<p><img src="/images/201805kernel_convolution.jpg" alt="image convolution" /></p>

<p><img src="/images/201805image_convolution.jpg" alt="image convolutioon" /></p>

<p>参考<a href="https://en.wikipedia.org/wiki/Kernel_(image_processing">Kernel (image processing)</a>)。
￼</p>

<p><a id="markdown-lenet详解" name="lenet详解"></a></p>

<h2>LeNet详解</h2>

<p>参考<a href="http://cv-tricks.com/tensorflow-tutorial/training-convolutional-neural-network-for-image-classification/">Basics of Convolutional Neural network (CNN)</a>。</p>

<ol>
<li>Convolutional Layer</li>
<li>Pooling Layer</li>
<li>Fully Connected Layer</li>
<li>Understanding Training Process</li>
</ol>


<p><a href="https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/">An Intuitive Explanation of Convolutional Neural Networks</a>的讲解也不错，中文版在<a href="https://blog.csdn.net/lcy7289786/article/details/68958111">这里</a>。</p>

<p><a id="markdown-手写数字识别过程可视化" name="手写数字识别过程可视化"></a></p>

<h2>手写数字识别过程可视化</h2>

<p><img src="/images/201805cnn_visualization.jpg" alt="cnn visualization" /></p>

<p>官方网站：
<a href="http://scs.ryerson.ca/~aharley/vis/">http://scs.ryerson.ca/~aharley/vis/</a></p>

<p>3D可视化：
<a href="http://scs.ryerson.ca/~aharley/vis/conv/">http://scs.ryerson.ca/~aharley/vis/conv/</a></p>

<p>2D可视化：
<a href="http://scs.ryerson.ca/~aharley/vis/conv/flat.html">http://scs.ryerson.ca/~aharley/vis/conv/flat.html</a></p>

<p>相关论文：
<a href="http://scs.ryerson.ca/~aharley/vis/harley_vis_isvc15.pdf">http://scs.ryerson.ca/~aharley/vis/harley_vis_isvc15.pdf</a></p>

<p><a id="markdown-神经网络中的神经元和权重在哪里" name="神经网络中的神经元和权重在哪里"></a></p>

<h2>神经网络中的“神经元”和“权重”在哪里？</h2>

<p><img src="/images/201805convolution_weights.jpg" alt="convolution and weights" /></p>

<p>各个卷机滤波器的里面的每个位置的值，即是我们需要训练的权重（卷积滤波器的尺寸是需要我们提前指定好的），每个像素对应于一个神经元。</p>

<p>其中神经网络的基本概念可以参考<a href="http://playground.tensorflow.org/">TensorFlow Playground</a>。</p>
]]></content>
  </entry>
  
</feed>
