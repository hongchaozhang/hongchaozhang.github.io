<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: ios | Reading Space]]></title>
  <link href="http://hongchaozhang.github.io/blog/categories/ios/atom.xml" rel="self"/>
  <link href="http://hongchaozhang.github.io/"/>
  <updated>2018-12-10T18:16:34+08:00</updated>
  <id>http://hongchaozhang.github.io/</id>
  <author>
    <name><![CDATA[Zhang Hongchao]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Introducing Create ML]]></title>
    <link href="http://hongchaozhang.github.io/blog/2018/06/15/introducing-create-ml/"/>
    <updated>2018-06-15T17:39:30+08:00</updated>
    <id>http://hongchaozhang.github.io/blog/2018/06/15/introducing-create-ml</id>
    <content type="html"><![CDATA[<!-- more -->


<p>Apple released Core ML in WWDC2017, and I took a note on <a href="../../../../2017/12/28/coreml-usage/">CoreML Usage</a>, including mlmodel training using Microsoft <a href="https://www.customvision.ai/">Custom Vision</a>.</p>

<p>This post is about the background of Create ML, its advantages, its relations with Turi. There is no code in this post. If you are looking for the usage of Create ML, refer to <a href="https://www.raywenderlich.com/196233/create-ml-tutorial-getting-started">Create ML Tutorial: Getting Started</a>.</p>

<h2>Background</h2>

<p>Before 2018, where can we get the mlmodel file used in iOS and macOS?</p>

<ul>
<li><a href="https://github.com/tf-coreml/tf-coreml">TensorFlow</a>: Train machine learning models and easily convert them to the Core ML Model format.</li>
<li><a href="https://www.customvision.ai/">Custom Vision</a> from Microsoft</li>
<li><a href="https://pypi.org/project/coremltools/">Core ML Tools</a>: Use this python package to convert models from machine learning toolboxes into the Core ML format.</li>
<li><a href="https://github.com/apache/incubator-mxnet/tree/master/tools/coreml">Apache MXNet</a>: Train machine learning models and convert them to the Core ML format.</li>
<li><a href="https://github.com/onnx/onnx-coreml">ONNX</a>: Convert ONNX models you have created to the Core ML Model format.</li>
<li>&hellip;</li>
</ul>


<p>TensorFlow doesn&rsquo;t support GPU on macOS from version 1.2.</p>

<p><img src="/images/tensorflow_not_support_gpu_on_macos.jpg" alt="tensor flow not support gpu on macos" /></p>

<p><strong>Core ML</strong>: Announced at WWDC 2017, and already supported by every major ML platform to convert existing models. But the existing models tend to be too big and/or too general.</p>

<p><strong>Turi Create</strong>: Acquired by Apple in 2016 ($200M), it lets you customize existing models with your own data. But … Python :[.</p>

<h2>Create ML</h2>

<p>Finally in WWDC2018, Apple announced <strong>Create ML</strong>, which can train machine learning models on macOS, able to use the GPU on macOS. The Create ML session and Turi Create session did not mention any word on each other, but obviousely, Create ML is based on Turi Create.</p>

<p>Based on Trui&rsquo;s model training, Create ML can make model training on macOS using GPU (maybe through Metal), and come up with models which can be used by Core ML framework.</p>

<p>With XCode Playground&rsquo;s updates, Apple gives CreateMLUI, a very easy way for model training: just need to drag your training data and test data into Playground.</p>

<h2>Main Advantage: Easy to Use</h2>

<p>Do model training using Swift in XCode.</p>

<blockquote><p><strong>Create ML</strong> is proof that Apple is committed to making it easier for you to use machine learning models in your apps. In this Create ML tutorial, you’ll learn how Create ML speeds up the workflow for improving your model by improving your data while also flattening the learning curve by doing it all in the comfort of Xcode and Swift.</p>

<p>&ndash;Refer to <a href="https://www.raywenderlich.com/196233/create-ml-tutorial-getting-started">Create ML Tutorial: Getting Started</a></p>

<p><strong>Create ML</strong>: Announced at WWDC 2018. ML in Xcode &amp; Swift! Currently includes only two of Turi Create’s seven task-focused toolkits, plus a generic classifier and regressor, and data tables. I see it as a trail of breadcrumbs leading you to the Turi Create gingerbread house, inhabited by a “good dog” instead of a witch! (Turi Create’s logo is a dog silhouette.)</p>

<p>&ndash;Refer to <a href="https://www.raywenderlich.com/196233/create-ml-tutorial-getting-started">Create ML Tutorial: Getting Started</a></p></blockquote>

<p>Refer to <a href="https://www.raywenderlich.com/196233/create-ml-tutorial-getting-started">Create ML Tutorial: Getting Started</a> to see how easy it is to use Create ML. There are some code comparasion between Create ML and Turi Create.</p>

<h3>Create ML和Turi Create</h3>

<p>Currently Create ML includes only two of Turi Create’s seven task-focused toolkits, plus a generic classifier and regressor, and data tables. Turi Create has five task-focused toolkits that aren’t (yet?) in Create ML:</p>

<ul>
<li>Recommender systems</li>
<li>Image similarity</li>
<li>Object detection</li>
<li>Style transfer</li>
<li>Activity classification</li>
</ul>


<h2>Transfer Learning</h2>

<p>The description of Transfer Learning from Apple Turi:</p>

<blockquote><p>It’s not uncommon for the task you want to solve to be related to something that has already been solved. Take, for example, the task of distinguishing cats from dogs. The famous ImageNet Challenge, for which CNN’s are the state-of-the-art, asks the trained model to categorize input into one of 1000 classes. Shouldn&rsquo;t features that distinguish between categories like lions and wolves also be useful for discriminating between cats and dogs?</p>

<p>The answer is a definitive yes. It is accomplished by simply removing the output layer of the Deep Neural Network for 1000 categories, and taking the signals that would have been propagating to the output layer and feeding them as features to a classifier for our new cats vs dogs task.</p>

<p>So, when you run the Turi Create image classifier, it breaks things down into something like this:</p>

<p>Stage 1: Create a CNN classifier on a large, general dataset. A good example is ImageNet, with 1000 categories and 1.2 million images. The models are already trained by researchers and are available for us to use.</p>

<p>Stage 2: The outputs of each layer in the CNN can be viewed as a meaningful vector representation of each image. Extract these feature vectors from the layer prior to the output layer on each image of your task.</p>

<p>Stage 3: Create a new classifier with those features as input for your own task.</p>

<p>At first glance, this seems even more complicated than just training the deep learning model. However, Stage 1 is reusable for many different problems, and once done, it doesn&rsquo;t have to be changed often.</p>

<p>In the end, this pipeline results in not needing to adjust hyper-parameters, faster training, and better performance even in cases where you don&rsquo;t have enough data to create a convention deep learning model. What&rsquo;s more, this technique is effective even if your Stage 3 classification task is relatively unrelated to the task Stage 1 is trained on. This idea was first explored by Donahue et al. (2013), and has since become one of the best ways to create image classifier models.</p>

<p>&ndash;Refer to <a href="https://apple.github.io/turicreate/docs/userguide/image_classifier/how-it-works.html#transfer-learning">truicreate transfer learning</a></p></blockquote>

<p>Some comments on transfer learning from web:</p>

<blockquote><p>What’s happening here? It’s called transfer learning, if you want to look it up. The underlying model — VisionFeaturePrint_Screen, which backs the Vision framework — was pre-trained on a ginormous dataset to recognize an enormous number of classes. It did this by learning what features to look for in an image, and how to combine these features to classify the image. Almost all of the training time for your dataset is the model extracting around 1000 features from your images. These could include low-level shapes and textures and higher-level shape of ears, distance between eyes, shape of snout. Then it spends a relatively tiny amount of time training a logistic regression model to separate your images into two classes. It’s similar to fitting a straight line to scattered points, but in 1000 dimensions instead of 2. But it’s still very quick to do: my run 1m 15s for feature extraction and 0.177886 seconds to train and apply the logistic regression.</p>

<p>Transfer learning only works successfully when features of your dataset are reasonably similar to features of the dataset that was used to train the model. A model pre-trained on ImageNet — a large collection of photos — might not transfer well to pencil drawings or microscopy images.</p>

<p>&ndash;Refer to <a href="https://www.raywenderlich.com/196233/create-ml-tutorial-getting-started">Create ML Tutorial: Getting Started</a></p></blockquote>

<h2>Improving Accuracy</h2>

<p>Refer to <a href="https://developer.apple.com/documentation/create_ml/improving_your_model_s_accuracy">Improving Your Model’s Accuracy</a> from Apple for improving training accuracy.</p>

<p>How to improve the model&rsquo;s training accuracy, validation accuracy and evaluation accuracy. <a href="https://www.raywenderlich.com/196233/create-ml-tutorial-getting-started">Create ML Tutorial: Getting Started</a> describes the three &lsquo;accuracy&rsquo;s.</p>

<h2>References and Materials</h2>

<ol>
<li><p>You might like to browse two fascinating articles about features from (mostly) Google Brain/Research:</p>

<ul>
<li><p><a href="https://distill.pub/2018/building-blocks/">The Building Blocks of Interpretability</a>: image feature extracting</p></li>
<li><p><a href="https://distill.pub/2017/feature-visualization/">Feature Visualization</a></p></li>
</ul>
</li>
<li><p><a href="https://www.kaggle.com/">Kaggle</a> is a repository of datasets contributed by members, often supplemented by notebooks that analyze and visualize the data. It runs model prediction competitions, which leads to the next link:</p>

<ul>
<li>Machine Learning Zero-to-Hero: Everything you need in order to compete on Kaggle for the first time, step-by-step!</li>
</ul>
</li>
<li><p><a href="https://www.raywenderlich.com/196233/create-ml-tutorial-getting-started">Create ML Tutorial: Getting Started</a> describes the usage of Create ML and Turi Create, including the history, code, data preparation, improving model accuracy and so on.</p></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[What is new in ARKit 2]]></title>
    <link href="http://hongchaozhang.github.io/blog/2018/06/13/what-is-new-in-n-arkit-2/"/>
    <updated>2018-06-13T13:24:59+08:00</updated>
    <id>http://hongchaozhang.github.io/blog/2018/06/13/what-is-new-in-n-arkit-2</id>
    <content type="html"><![CDATA[<!-- more -->


<h2>Overview</h2>

<p>In ARKit 1, we have:</p>

<ul>
<li>Device positioning from world tracking process</li>
<li>Horizontal and vertical plane detection from world tracking process</li>
<li>Lighting estimation</li>
<li>AR face tracking</li>
</ul>


<p>In ARKit 2, we have:</p>

<ul>
<li>Saving and loading maps</li>
<li>Environment Texturing</li>
<li>Image detection and tracking</li>
<li>3D object tracking</li>
<li>Improved face tracking</li>
</ul>


<h2>New Features in ARKit 2</h2>

<h3>Saving and Loading Maps</h3>

<h4>World Tracking Recap:</h4>

<ul>
<li>Position and orientation of the device.</li>
<li>Physical scale in the scene.</li>
<li>3D feature points.</li>
<li>Relocalization (iOS 11.3): we can relocalize objects when your AR session is interrupted, like phone coming or going from background. This feature is implemented by storing the mapping <code>ARWorldMap</code> between real world and the coordinate system. However the mapping is not exposed to developers.</li>
</ul>


<h4>World Tracking Enhancement:</h4>

<ul>
<li><strong>Saving and loading maps</strong>: expose the <code>ARWorldMap</code> to developers.</li>
<li>Faster initialization and plane detection</li>
<li>Robust tracking and plane detection</li>
<li>More accurate extent and boundary Continuous autofocus</li>
<li>New 4:3 video formats (iPad is also 4:3)</li>
</ul>


<h4>Saving and loading maps:</h4>

<p><code>ARWorldmap</code> contains:</p>

<ul>
<li>Mapping of physical 3D space: for representing 3D feature points in the coordinate system.</li>
<li>Mutable list of named anchors: for restoring previous 3D environment (like lighting node anchor), and relocalizing previously added virtual objects.</li>
<li>Raw feature points and extent: for debugging and visualization.</li>
<li>Serialization: for storing and recovering from an file.</li>
</ul>


<p><img src="/images/arkit2_arworldmap.jpg" width="500" alt="arkit arworldmap" /></p>

<p>We can use the map in two different ways:</p>

<ul>
<li>Persistent: Restore previous AR scene for a new AR session. For example, you go to another room and come back or close the AR app and open it some time later.</li>
<li>Multiuser experience: We can share the map among devices through WiFi or bluetooth.</li>
</ul>


<p>The SwiftShot is an multiuser experience AR game:</p>

<p><img src="/images/swiftshot.jpg" alt="arkit2 swiftshot" /></p>

<p>and the following is a small piece of the demo:</p>

<p><img src="/images/arkit2_multiuser_experience_demo.gif" alt="swift shot game" /></p>

<h4>How to get a good map</h4>

<p>In order to share or restore the map, we need to get a good one first. A good map should be:</p>

<!-- * Important for relocalization -->


<ul>
<li>Multiple points of view: If we record the mapping from one point of view, and try to restore the coordinate system from another point of view, it will fail.</li>
<li>Static, well-textured environment.</li>
<li>Dense feature points on the map.</li>
</ul>


<p>We can use the <code>WorldMappingStatus</code> status from <code>ARFrame</code> to decide if the current map is good enough for sharing or storing:</p>

<pre><code class="swift">public enum WorldMappingStatus : Int {
   case notAvailable
   case limited
   case extending
   case mapped 
}
</code></pre>

<h3>Environment Texturing</h3>

<p>With the help of Environment Texturing, AR scene objects can reflect the environment texture on the surface of themselves, just like:</p>

<p><img src="/images/arkit2_environment_texturing_demo.jpg" alt="arkit2 environment texturing demo" /></p>

<h3>Image Tracking</h3>

<p>Moving objects can not be positioned in ARKit 1. In ARKit 2, specified images can be tracked in AR scene.</p>

<p><img src="/images/arkit2_image_tracking.gif" alt="arkit 2 image tracking" /></p>

<p>The classes in ARKit 2 for image tracking are:</p>

<p><img src="/images/arkit2_image_tracking_classes.jpg" alt="arkit 2 image tracking classes" /></p>

<p>The detected <code>ARImageAnchor</code>s have properties like:</p>

<pre><code class="swift">open class ARImageAnchor : ARAnchor, ARTrackable { 
    public var isTracked: Bool { get }
    open var transform: simd_float4x4 { get }
    open var referenceImage: ARReferenceImage { get }
}
</code></pre>

<p>The specified image should:</p>

<ul>
<li>Histogram should be broad</li>
<li>Not have multiple uniform color regions</li>
<li>Not have repeated structures</li>
</ul>


<p>The following is the demo:</p>

<p><img src="/images/arkit2_image_tracking_demo.gif" alt="arkit 2 image tracking demo" /></p>

<p>The inputs of the above demo are:</p>

<ul>
<li>an static image of the cat, the same as it is in the picture frame</li>
<li>an video of the cat</li>
</ul>


<p>The video is played at the position of the specified picture frame, with the same orientation of the picture frame.</p>

<p>There are two classes related to image tracking:</p>

<table>
<thead>
<tr>
<th>ARImageTrackingConfiguration </th>
<th> ARWorldTrackingConfiguration</th>
</tr>
</thead>
<tbody>
<tr>
<td>Has No World Origin </td>
<td> Has World Origin</td>
</tr>
<tr>
<td>After detecting the image, only do things inside the place of the image. </td>
<td> After detecting the image, place some virtual objects outside the detected image plane.</td>
</tr>
</tbody>
</table>


<h3>3D Object Detection</h3>

<p>3D object detection workflow is:</p>

<p><img src="/images/arkit2_3D_object_tracking_classes.jpg" alt="arkit2 3D object tracking classes" /></p>

<p>The <code>ARObjectAnchor</code> contains properties like:</p>

<pre><code class="swift">open class ARObjectAnchor : ARAnchor {
    open var transform: simd_float4x4 { get }
    open var referenceObject: ARReferenceObject { get }
}
</code></pre>

<p>and <code>ARReferenceObject</code> is the scanned 3D object:</p>

<pre><code class="swift">open class ARReferenceObject
    : NSObject, NSCopying, NSSecureCoding {
    open var name: String?
    open var center: simd_float3 { get }
    open var extent: simd_float3 { get }
    open var rawFeaturePoints: ARPointCloud { get }
}
</code></pre>

<blockquote><p>An <code>ARReferenceObject</code> contains only the spatial feature information needed for ARKit to recognize the real-world object, and is not a displayable 3D reconstruction of that object.</p></blockquote>

<p>In order to get the <code>ARReferenceObject</code>, we should scan the real object, and store the result as an file (.arobject) or an xcode asset catalog for ARKit to use. Fortunately, Apple supplies a demo for scanning 3D object to get the <code>ARReferenceObject</code>. Refer to: <a href="https://developer.apple.com/documentation/arkit/scanning_and_detecting_3d_objects">Scanning and Detecting 3D Objects</a> for detail and the rough steps of object scanning are:</p>

<p><img src="/images/arkit2_3D_object_scan.jpg" alt="arkit2 3D object scan" /></p>

<p>For scanned object in the real world, we can dynamically add some info around it (Museum is a good use case.), like the demo does:</p>

<p><img src="/images/arkit2_3D_object_tracking_demo.gif" alt="arkit2 object tracking demo" /></p>

<h3>Face Tracking Enhancements</h3>

<p>With face tracking, we can place something on it or around it.</p>

<p>Enhancements in ARKit 2:</p>

<ul>
<li>Gaze tracking</li>
<li>Tongue support</li>
</ul>


<blockquote><p>Gaze and Tongue can be input of the AR app.</p></blockquote>

<p>New changes in one screenshot:</p>

<p><img src="/images/what-is-new-in-arkit-2.jpg" alt="what-is-new-in-arkit-2" /></p>

<h2>Some other WWDC Sessions Related to AR</h2>

<h3><a href="https://developer.apple.com/videos/play/wwdc2018/603/">Integrating Apps and Content with AR Quick Look</a></h3>

<p>A deeper dive into a new feature in iOS that provides a way to preview any AR object from a USDZ file.</p>

<p><img src="/images/QLPreviewController.png" alt="QLPreviewController" /></p>

<ul>
<li>There’s a great sequence diagram presented (see above) (I wish more sessions would have these!) for previewing USDZ objects, of which the <code>QLPreviewController</code> plays a central role.</li>
<li>For web developers, it covers HTML samples for how to preview USDZ objects in Safari.</li>
<li>Then it goes into a deep dive on how to create the actual USDZ objects, with more examples on new AR texturing capabilities.</li>
<li>There’s also a quick overview on how to optimize the files, to keep the size down, and there’s a breakdown of the files that make up the USDZ format.</li>
</ul>


<h3><a href="https://developer.apple.com/videos/play/wwdc2018/605/">Inside SwiftShot: Creating an AR Game</a></h3>

<p>Covers world map sharing, networking, and the physics of how to build an AR game, as well as some design insight (I have limited game dev experience so I’ll do the best I can below).</p>

<ul>
<li>Pointers to remember with designing an AR game, such as “encouraging” the user to slowly move the device for world mapping!</li>
<li>It demonstrates the usage of image &amp; object detection, world map sharing, and iBeacons for the game.</li>
<li>Integrating <code>ARKit</code> with <code>SceneKit</code> and <code>Metal</code>, including the translation of physics data between each — position, velocity, and orientation.</li>
<li>Performance enhancement with the <code>BitStreamCodable</code> protocol.</li>
<li>A small look at how audio was integrated into the game.</li>
</ul>


<h3><a href="https://developer.apple.com/videos/play/wwdc2018/805/">Creating Great AR Experiences</a></h3>

<p>Best practises mainly from a UX &amp; design perspective (there are no code samples in this session).</p>

<ul>
<li>Logical dos and don’ts that may be useful, if you need help with thought towards product and empathy towards the user.</li>
<li>They emphasize the importance of using transitions between AR scapes.</li>
<li>Why AR is a special combination of touch and movement.</li>
<li>They advise that minimal battery impact should be a huge focus! This is a challenge, given that they recommend to render the FPS at 60 to avoid latency.</li>
<li>There’s a lengthy demonstration of creating an AR fireplace, with complex texturing, etc. It looks great, but unfortunately there were no coding samples accompanying the demo.</li>
</ul>


<h3><a href="https://developer.apple.com/videos/play/wwdc2018/610/">Understanding ARKit Tracking and Detection</a></h3>

<p>A good broad overview of all of the main AR concepts.</p>

<ul>
<li>This is such a good intro into not only AR on iOS, but AR in general, that it should have been part of 2017’s sessions when ARKit was first introduced. Better late than never. If you’re only going to watch one session, watch this one!</li>
<li>It recaps the main features of ARKit — <strong>orientation</strong>, <strong>world tracking</strong>, and <strong>plane detection</strong>, and demos all of these in depth with coding samples.</li>
<li>It then demos the new features of ARKit 2 — <strong>shared world mapping</strong>, <strong>image tracking</strong>, and <strong>object detection</strong> (which has been available in the Vision framework recapped above, but is now also accessible in ARKit).</li>
<li>A good explanation on a core AR principle, <strong>Visual Inertial Odometry</strong>, is given. Short of going into the actual physics equations behind it, this should give you a great understanding of VIO.</li>
</ul>


<h2>Some other materials for a better AR app:</h2>

<h3><a href="https://developer.apple.com/documentation/arkit/building_your_first_ar_experience">Building Your First AR Experience</a></h3>

<p>This document demos an app for basic usage of ARKit.</p>

<h3><a href="https://developer.apple.com/documentation/arkit/managing_session_lifecycle_and_tracking_quality">Managing Session Lifecycle and Tracking Quality</a></h3>

<p>Make your AR experience more robust by</p>

<ul>
<li>providing clear feedback, using <code>ARCamera.TrackingState</code>.</li>
<li>recovering from interruptions, using <code>ARCamera.TrackingState.Reason.relocalizing</code>.</li>
<li>resuming previous sessions, using <code>ARWorldMap</code>.</li>
</ul>


<h3><a href="https://developer.apple.com/design/human-interface-guidelines/ios/system-capabilities/augmented-reality/">Human Interface Guidelines - Augmented Reality</a></h3>

<p>This post describes how to rendering virtual objects, how to interact with virtual objects, how to handling interruptions. It is for UX.</p>

<h3><a href="https://developer.apple.com/documentation/arkit/handling_3d_interaction_and_ui_controls_in_augmented_reality">Handling 3D Interaction and UI Controls in Augmented Reality</a></h3>

<p>This document describes the best practices for visual feedback, gesture interactions, and realistic rendering in AR experiences. And a demo app is supplied.</p>

<p><img src="/images/arkit_demo_screenshot.jpg" alt="arkit demo" /></p>

<h3><a href="https://developer.apple.com/documentation/arkit/creating_a_multiuser_ar_experience">Creating a Multiuser AR Experience</a></h3>

<p>This document demos an app (with source code) on how to transmit ARKit world-mapping data between nearby devices with the <a href="https://developer.apple.com/documentation/multipeerconnectivity">MultipeerConnectivity</a> framework (introduced in iOS 7.0) to create a shared basis for AR experiences. MultipeerConnectivity supports peer-to-peer connectivity and the discovery of nearby devices. With MultipeerConnectivity, you can not only share <code>ARWorldMap</code>, but also some actions. This makes multiuser AR game possible.</p>

<p>However:</p>

<ul>
<li>Recording and transmitting a world map and relocalizing to a world map are time-consuming, bandwidth-intensive operations. A good design is needed for better performance.</li>
<li>The persons received the world map data need to move their device so they see a similar perspective (also sent by the host) helps ARKit process the received map and establish a shared frame of reference for the multiuser experience.</li>
</ul>


<h3><a href="https://developer.apple.com/documentation/arkit/swiftshot_creating_a_game_for_augmented_reality">SwiftShot: Creating a Game for Augmented Reality</a></h3>

<p>This document demos the SwiftShot game shown on WWDC 2018, including:</p>

<ul>
<li>Designing Gameplay for AR</li>
<li>Using Local Multipeer Networking and Sharing World Maps</li>
<li>Synchronizing Gameplay Actions</li>
<li>Solving Multiplayer Physics</li>
</ul>


<h3><a href="https://developer.apple.com/documentation/arkit/recognizing_images_in_an_ar_experience">Recognizing Images in an AR Experience</a></h3>

<p>Detect known 2D images in the user’s environment, and use their positions to place AR content.</p>

<h3><a href="https://developer.apple.com/documentation/arkit/scanning_and_detecting_3d_objects">Scanning and Detecting 3D Objects</a></h3>

<p>Record spatial features of real-world objects, then use the results to find those objects in the user’s environment and trigger AR content.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Swift Coding Conventions]]></title>
    <link href="http://hongchaozhang.github.io/blog/2018/01/26/swift-coding-convention/"/>
    <updated>2018-01-26T11:37:05+08:00</updated>
    <id>http://hongchaozhang.github.io/blog/2018/01/26/swift-coding-convention</id>
    <content type="html"><![CDATA[<p>Collection of some Swift coding conventions, which will make Swift code more maintainable, more readable.</p>

<!-- more -->


<p>The following is conventions I like or I will likely misuse. For a complete version, go to:</p>

<ol>
<li><a href="https://github.com/raywenderlich/swift-style-guide/">The Official raywenderlich.com Swift Style Guide.</a></li>
<li>Swift <a href="https://swift.org/documentation/api-design-guidelines/">API Design Guidelines</a></li>
</ol>


<!-- TOC -->


<ul>
<li><a href="#naming">Naming</a>

<ul>
<li><a href="#try-to-form-grammatical-english-phrases">Try to Form Grammatical English Phrases</a></li>
<li><a href="#mutatingnonmutating-methods-naming">Mutating/Nonmutating Methods Naming</a></li>
<li><a href="#boolean-methods-naming">Boolean Methods Naming</a></li>
<li><a href="#protocol-naming">Protocol Naming</a></li>
<li><a href="#avoid-abbreviations">Avoid Abbreviations</a></li>
<li><a href="#delegates">Delegates</a></li>
</ul>
</li>
<li><a href="#code-organization">Code Organization</a>

<ul>
<li><a href="#protocol-conformance">Protocol Conformance</a></li>
</ul>
</li>
<li><a href="#classes-and-structures">Classes and Structures</a>

<ul>
<li><a href="#use-of-self">Use of Self</a></li>
<li><a href="#constants">Constants</a></li>
</ul>
</li>
<li><a href="#control-flow">Control Flow</a>

<ul>
<li><a href="#golden-path">Golden Path</a>

<ul>
<li><a href="#failing-guards">Failing Guards</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#argument-labels">Argument Labels</a></li>
</ul>


<!-- /TOC -->


<p><a id="markdown-naming" name="naming"></a></p>

<h2>Naming</h2>

<p>Descriptive and consistent naming makes software easier to read and understand. Use the Swift naming conventions described in the <a href="https://swift.org/documentation/api-design-guidelines/">API Design Guidelines</a>. Some key principles include:</p>

<ol>
<li>prioritizing clarity over brevity</li>
<li>striving for fluent usage</li>
<li>using uppercase for types (and protocols), lowercase for everything else</li>
<li>boolean types should read like assertions</li>
<li>choosing good parameter names that serve as documentation</li>
<li>generally avoiding abbreviations</li>
<li>taking advantage of default parameters</li>
<li>labeling closure and tuple parameters</li>
<li>verb methods follow the -ed, -ing rule for the non-mutating version</li>
<li>noun methods follow the formX rule for the mutating version</li>
<li>protocols that describe what something is should read as nouns</li>
<li>protocols that describe a capability should end in -able or -ible</li>
<li>striving for clarity at the call site</li>
</ol>


<p><a id="markdown-try-to-form-grammatical-english-phrases" name="try-to-form-grammatical-english-phrases"></a></p>

<h3>Try to Form Grammatical English Phrases</h3>

<p><strong>Preferred:</strong></p>

<pre><code class="swift">x.insert(y, at: z)          // “x, insert y at z”
x.subViews(havingColor: y)  // “x's subviews having color y”
x.capitalizingNouns()       // “x, capitalizing nouns”
</code></pre>

<p><strong>Not Preferred:</strong></p>

<pre><code class="swift">x.insert(y, position: z)
x.subViews(color: y)
x.nounCapitalize()
</code></pre>

<p><a id="markdown-mutatingnonmutating-methods-naming" name="mutatingnonmutating-methods-naming"></a></p>

<h3>Mutating/Nonmutating Methods Naming</h3>

<p>When the operation is naturally described by a verb, use the verb’s imperative for the mutating method and apply the “ed” or “ing” suffix to name its nonmutating counterpart.</p>

<table>
<thead>
<tr>
<th> Mutating  </th>
<th> Nonmutating </th>
</tr>
</thead>
<tbody>
<tr>
<td> <code>x.sort()</code>    </td>
<td> <code>z = x.sorted()</code> </td>
</tr>
<tr>
<td> <code>x.append(y)</code> </td>
<td> <code>z = x.appending(y)</code> </td>
</tr>
</tbody>
</table>


<p><br>
When the operation is naturally described by a noun, use the noun for the nonmutating method and apply the “form” prefix to name its mutating counterpart.</p>

<table>
<thead>
<tr>
<th> Nonmutating   </th>
<th> Mutating </th>
</tr>
</thead>
<tbody>
<tr>
<td> <code>x = y.union(z)</code>  </td>
<td> <code>y.formUnion(z)</code> </td>
</tr>
<tr>
<td> <code>j = c.successor(i)</code>  </td>
<td> <code>c.formSuccessor(&amp;i)</code> </td>
</tr>
</tbody>
</table>


<p><a id="markdown-boolean-methods-naming" name="boolean-methods-naming"></a></p>

<h3>Boolean Methods Naming</h3>

<p>Uses of Boolean methods and properties should read as assertions about the receiver when the use is nonmutating, e.g. <code>x.isEmpty</code>, <code>line1.intersects(line2)</code>.</p>

<p><a id="markdown-protocol-naming" name="protocol-naming"></a></p>

<h3>Protocol Naming</h3>

<p>Protocols that describe what something is should read as nouns (e.g. <code>Collection</code>).</p>

<p>Protocols that describe a capability should be named using the suffixes -able, -ible, or -ing (e.g. <code>Equatable</code>, <code>ProgressReporting</code>).</p>

<p><a id="markdown-avoid-abbreviations" name="avoid-abbreviations"></a></p>

<h3>Avoid Abbreviations</h3>

<blockquote><p>The intended meaning for any abbreviation you use should be easily found by a <strong>web search</strong>.</p></blockquote>

<p><a id="markdown-delegates" name="delegates"></a></p>

<h3>Delegates</h3>

<p>When creating custom delegate methods, an unnamed first parameter should be the delegate source. (UIKit contains numerous examples of this.)</p>

<p><strong>Preferred:</strong></p>

<pre><code class="swift">func namePickerView(_ namePickerView: NamePickerView, didSelectName name: String)
func namePickerViewShouldReload(_ namePickerView: NamePickerView) -&gt; Bool
</code></pre>

<p><strong>Not Preferred:</strong></p>

<pre><code class="swift">func didSelectName(namePicker: NamePickerViewController, name: String)
func namePickerShouldReload() -&gt; Bool
</code></pre>

<p><a id="markdown-code-organization" name="code-organization"></a></p>

<h2>Code Organization</h2>

<p>Use extensions to organize your code into logical blocks of functionality. Each extension should be set off with a <code>// MARK: - comment</code> to keep things well-organized.</p>

<p><a id="markdown-protocol-conformance" name="protocol-conformance"></a></p>

<h3>Protocol Conformance</h3>

<p>In particular, when adding protocol conformance to a model, prefer adding a separate extension for the protocol methods. This keeps the related methods grouped together with the protocol and can simplify instructions to add a protocol to a class with its associated methods.</p>

<p><strong>Preferred:</strong></p>

<pre><code class="swift">class MyViewController: UIViewController {
  // class stuff here
}

// MARK: - UITableViewDataSource
extension MyViewController: UITableViewDataSource {
  // table view data source methods
}

// MARK: - UIScrollViewDelegate
extension MyViewController: UIScrollViewDelegate {
  // scroll view delegate methods
}
</code></pre>

<p><strong>Not Preferred:</strong></p>

<pre><code class="swift">class MyViewController: UIViewController, UITableViewDataSource, UIScrollViewDelegate {
  // all methods
}
</code></pre>

<p>For UIKit view controllers, consider grouping lifecycle, custom accessors, and IBAction in separate class extensions.</p>

<p><a id="markdown-classes-and-structures" name="classes-and-structures"></a></p>

<h2>Classes and Structures</h2>

<p><a id="markdown-use-of-self" name="use-of-self"></a></p>

<h3>Use of Self</h3>

<p>For conciseness, avoid using <code>self</code> since Swift does not require it to access an object&rsquo;s properties or invoke its methods.</p>

<p>Use <code>self</code> only when required by the compiler (in <code>@escaping</code> closures, or in initializers to disambiguate properties from arguments). In other words, if it compiles without <code>self</code> then omit it.</p>

<p><a id="markdown-constants" name="constants"></a></p>

<h3>Constants</h3>

<p>Constants are defined using the <code>let</code> keyword, and variables with the <code>var</code> keyword. Always use <code>let</code> instead of <code>var</code> if the value of the variable will not change.</p>

<blockquote><p>Tip: A good technique is to define everything using <code>let</code> and only change it to <code>var</code> if the compiler complains!</p></blockquote>

<p>You can define constants on a type rather than on an instance of that type using type properties. To declare a type property as a constant simply use <code>static let</code>. Type properties declared in this way are generally preferred over global constants because they are easier to distinguish from instance properties.</p>

<p><strong>Preferred:</strong></p>

<pre><code class="swift">enum Math {
  static let e = 2.718281828459045235360287
  static let root2 = 1.41421356237309504880168872
}

let hypotenuse = side * Math.root2
</code></pre>

<p><strong>Not Preferred:</strong></p>

<pre><code class="swift">let e = 2.718281828459045235360287  // pollutes global namespace
let root2 = 1.41421356237309504880168872

let hypotenuse = side * root2 // what is root2?
</code></pre>

<p><a id="markdown-control-flow" name="control-flow"></a></p>

<h2>Control Flow</h2>

<p><a id="markdown-golden-path" name="golden-path"></a></p>

<h3>Golden Path</h3>

<p>When coding with conditionals, the left-hand margin of the code should be the &ldquo;golden&rdquo; or &ldquo;happy&rdquo; path. That is, don&rsquo;t nest <code>if</code> statements. Multiple return statements are OK. The <code>guard</code> statement is built for this.</p>

<p><strong>Preferred:</strong></p>

<pre><code class="swift">func computeFFT(context: Context?, inputData: InputData?) throws -&gt; Frequencies {

  guard let context = context else {
    throw FFTError.noContext
  }
  guard let inputData = inputData else {
    throw FFTError.noInputData
  }

  // use context and input to compute the frequencies
  return frequencies
}
</code></pre>

<p><strong>Not Preferred:</strong></p>

<pre><code class="swift">func computeFFT(context: Context?, inputData: InputData?) throws -&gt; Frequencies {

  if let context = context {
    if let inputData = inputData {
      // use context and input to compute the frequencies

      return frequencies
    } else {
      throw FFTError.noInputData
    }
  } else {
    throw FFTError.noContext
  }
}
</code></pre>

<p>When multiple optionals are unwrapped either with <code>guard</code> or <code>if let</code>, minimize nesting by using the compound version when possible. Example:</p>

<p><strong>Preferred:</strong></p>

<pre><code class="swift">guard let number1 = number1,
      let number2 = number2,
      let number3 = number3 else {
  fatalError("impossible")
}
// do something with numbers
</code></pre>

<p><strong>Not Preferred:</strong></p>

<pre><code class="swift">if let number1 = number1 {
  if let number2 = number2 {
    if let number3 = number3 {
      // do something with numbers
    } else {
      fatalError("impossible")
    }
  } else {
    fatalError("impossible")
  }
} else {
  fatalError("impossible")
}
</code></pre>

<p><a id="markdown-failing-guards" name="failing-guards"></a></p>

<h4>Failing Guards</h4>

<p><code>guard</code> statements are required to exit in some way. Generally, this should be simple one line statement such as <code>return</code>, <code>throw</code>, <code>break</code>, <code>continue</code>, and <code>fatalError()</code>. Large code blocks should be avoided. If cleanup code is required for multiple exit points, consider using a <code>defer</code> block to avoid cleanup code duplication.</p>

<p><a id="markdown-argument-labels" name="argument-labels"></a></p>

<h2>Argument Labels</h2>

<ol>
<li>Good practice</li>
</ol>


<pre><code class="swift">func move(from start: Point, to end: Point)
x.move(from: x, to: y) 
</code></pre>

<ol>
<li><p>Omit all labels when arguments can’t be usefully distinguished, e.g. <code>min(number1, number2)</code>, <code>zip(sequence1, sequence2)</code>.</p></li>
<li><p>When the first argument forms part of a prepositional phrase, give it an argument label. The argument label should normally begin at the preposition, e.g. <code>x.removeBoxes(havingLength: 12)</code>.</p>

<ul>
<li>An exception for the principle above arises when the first two arguments represent parts of a single abstraction. In such cases, begin the argument label after the preposition, to keep the abstraction clear.</li>
</ul>
</li>
</ol>


<p><strong>Preferred:</strong></p>

<pre><code class="swift">a.moveTo(x: b, y: c)
a.fadeFrom(red: b, green: c, blue: d)
</code></pre>

<p><strong>Not Preferred:</strong></p>

<pre><code class="swift">a.move(toX: b, y: c)
a.fade(fromRed: b, green: c, blue: d)
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[DR Project]]></title>
    <link href="http://hongchaozhang.github.io/blog/2018/01/02/dr-project/"/>
    <updated>2018-01-02T13:39:28+08:00</updated>
    <id>http://hongchaozhang.github.io/blog/2018/01/02/dr-project</id>
    <content type="html"><![CDATA[<p>Key words: ARKit, CoreML, SceneKit</p>

<!-- more -->


<p><a href="https://github.com/hongchaozhang/ProjectDataToReality">DR (Data to Reality)</a> is a demo for projecting data into reality: Using <strong>CoreML</strong> for object recognition, and then get the recognized object data and project the data to reality, just above the recognized object. In this process, <strong>ARKit</strong> helps us to get the real world object coordinate to put the data at, and <strong>SceneKit</strong> helps us to render the data in reality.</p>

<p>This is a screenshot in demo:</p>

<p><img src="/images/DR-Screenshot-1.jpg" alt="project chart to reality" /></p>

<p>Refer to github <a href="https://github.com/hongchaozhang/ProjectDataToReality">Project Data to Reality</a> for demo project. In the github page, the following are told:</p>

<ol>
<li>Requirement</li>
<li>How to Run the Project</li>
<li>How to Use the Demo

<ol>
<li>Project Chart to Reality</li>
<li>Face Detection</li>
<li>Face Recognition</li>
</ol>
</li>
</ol>


<h2>Related techniques used</h2>

<ol>
<li><a href="../../../../2017/12/28/arkit-usage/">ARKit</a></li>
<li><a href="../../../../2017/12/28/coreml-usage/">CoreML</a></li>
<li><a href="../../../../2018/01/02/scenekit-usage/">SceneKit</a></li>
</ol>


<h2>Notes on the Demo</h2>

<p>As this is a rough demo, it need some enhancements:</p>

<ol>
<li>Only four kinds of fruits are supported: banana, orange, cucumber and strawberry. But for anything recognized by Inceptionv3.mlmodel, we can add a sphere and the name just at the world position of the object. (Set <code>showRecognizedResultNearby</code> to <code>true</code>.)</li>
<li>The chart data of the four kinds of fruits are images exported from other apps.</li>
<li>For face detection on iphone, rotate the device to left by 90 degrees to make it work on landscape. This is an issue need to be fixed.</li>
<li>Face recognition needs a trained face recognition model, called FaceRecognition.mlmodel.</li>
<li>Face recognition request doesn&rsquo;t crop the image from camera according to the face detection result. This should be done to make face recognition more robust.</li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[CoreML Usage]]></title>
    <link href="http://hongchaozhang.github.io/blog/2017/12/28/coreml-usage/"/>
    <updated>2017-12-28T17:25:39+08:00</updated>
    <id>http://hongchaozhang.github.io/blog/2017/12/28/coreml-usage</id>
    <content type="html"><![CDATA[<!-- more -->




<!-- TOC -->


<ul>
<li><a href="#about-coreml">About CoreML</a></li>
<li><a href="#coreml-and-vision">CoreML and Vision</a></li>
<li><a href="#model-usage">Model Usage</a></li>
<li><a href="#model-training">Model Training</a>

<ul>
<li><a href="#basic">Basic</a></li>
<li><a href="#advanced">Advanced</a></li>
</ul>
</li>
<li><a href="#coreml-pros-and-cons">CoreML Pros and Cons</a>

<ul>
<li><a href="#pros">Pros</a></li>
<li><a href="#cons">Cons</a></li>
</ul>
</li>
</ul>


<!-- /TOC -->


<p><a id="markdown-about-coreml" name="about-coreml"></a></p>

<h2>About CoreML</h2>

<ol>
<li>Support image processing for <em>Vision</em>.</li>
<li>Support NPL (natural language processing) for <em>Foundation</em>.</li>
<li>Support learned decision tree analyzing for <em>GameplayKit</em>.</li>
</ol>


<p><a id="markdown-coreml-and-vision" name="coreml-and-vision"></a></p>

<h2>CoreML and Vision</h2>

<ol>
<li>CoreML makes it even easier to use trained models in your apps.</li>
<li>Vision gives you easy access to Apple’s models for detecting faces, face landmarks, text, rectangles, barcodes, and objects.</li>
</ol>


<p>Because these two frameworks are built on Metal, they run efficiently on the device, so you don’t need to send your users’ data to a server.</p>

<p><a id="markdown-model-usage" name="model-usage"></a></p>

<h2>Model Usage</h2>

<p>When you load a trained machine learning model (.mlmodel) into xcode, the screenshot is like (take inceptionv3.mlmodel as an example):</p>

<p><img src="/images/mlmodel_in_xcode.png" alt="machine learning model imported to xcode" /></p>

<p>From <em>Model Class</em> (section A), we can see that xcode has <em>Automatically generated Swift model calss</em>. Click the right arrow to view the generated model class.</p>

<p>If the model class is not generated successfully, double check <em>Target Membership</em> (section B) to make sure the mlmodel file is added into the correct target.</p>

<p>From <em>Model Evaluation Parameters</em>(section C), we can see the input and output of the trained model.</p>

<p>The following is a sample usage of image classification model:</p>

<pre><code class="swift">
// create request
guard let selectedModel = try? VNCoreMLModel(for: Inceptionv3().model) else {
    fatalError("Could not load model. Ensure model has been drag and dropped (copied) to XCode Project. Also ensure the model is part of a target.")
}     

let classificationRequest = VNCoreMLRequest(model: selectedModel, completionHandler: classificationCompleteHandler)
classificationRequest.imageCropAndScaleOption = VNImageCropAndScaleOption.centerCrop // Crop from centre of images and scale to appropriate size.

...

// run request against an image
guard let pixbuff = (sceneView.session.currentFrame?.capturedImage) else { return }
let ciImage = CIImage(cvPixelBuffer: pixbuff)
// Note1: Not entirely sure if the ciImage is being interpreted as RGB, but for now it works with the Inception model.
// Note2: Also uncertain if the pixelBuffer should be rotated before handing off to Vision (VNImageRequestHandler) - regardless, for now, it still works well with the Inception model.

let imageRequestHandler = VNImageRequestHandler(ciImage: ciImage, options: [:])

do {
    try imageRequestHandler.perform([classificationRequest])
} catch {
    print(error)
}

...

// completion handler for coping with image classification results.
func classificationCompleteHandler(request: VNRequest, error: Error?) {
    if error != nil {
        print("Error: " + (error?.localizedDescription)!)
        return
    }

    guard let observations = request.results else {
        print("No results")
        return
    }

    // Get Classifications
    let classifications = observations[0...1] // top 2 results
        .flatMap({ $0 as? VNClassificationObservation })
        .filter({ $0.confidence &gt; 0.2 })
        .map({ "\($0.identifier) \(String(format:"- %.2f", $0.confidence))" })
        .joined(separator: "\n")

    print("image recognition: " + classifications)
}
</code></pre>

<p>Refer to <a href="https://developer.apple.com/machine-learning/">Build more intelligent apps with machine learning</a> for some official materials.</p>

<p>For some detailed usage step by step, refer to <a href="https://www.raywenderlich.com/164213/coreml-and-vision-machine-learning-in-ios-11-tutorial">Core ML and Vision: Machine Learning in iOS 11 Tutorial</a>.</p>

<p><a id="markdown-model-training" name="model-training"></a></p>

<h2>Model Training</h2>

<p><a id="markdown-basic" name="basic"></a></p>

<h3>Basic</h3>

<p><img src="/images/CustomVisionFromMicroSoft.png" alt="Custom Vision From MicroSoft" /></p>

<p>Microsoft <a href="https://www.customvision.ai/">Custom Vision</a> supplies a very friendly UI interface. You can upload you images and label them very easily. After training is done, you can export the model for mobile devices, including: mlmodel file for iOS platform, and TensorFlow model on Android platform.</p>

<p>Friendly UI Interface:</p>

<p><img src="/images/InterfaceOfCustomVision.png" alt="interface of microsoft custom vision" /></p>

<p>But there are some limitations, as <a href="https://www.customvision.ai/">Custom Vision</a> is still in preview process.</p>

<p><img src="/images/MicroSoftCustomVisionLimitation.png" alt="limitation of microsoft custom vision" /></p>

<p><a id="markdown-advanced" name="advanced"></a></p>

<h3>Advanced</h3>

<p><a href="https://github.com/apple/turicreate/tree/master/userguide/image_classifier">apple turicreate image classification</a> supplies more configurations for model training, like the partition of trainning data and verification data. But some Python experience is needed.</p>

<p><a id="markdown-coreml-pros-and-cons" name="coreml-pros-and-cons"></a></p>

<h2>CoreML Pros and Cons</h2>

<p><a id="markdown-pros" name="pros"></a></p>

<h3>Pros</h3>

<ol>
<li><p><strong>Easy to use.</strong> As described at the beginning of the post.</p></li>
<li><p><strong>High performance.</strong> As is said:</p>

<blockquote><p>“It was amazing to see the prediction results immediately without any time interval.”</p></blockquote></li>
</ol>


<p><a id="markdown-cons" name="cons"></a></p>

<h3>Cons</h3>

<p><strong>Lack of federated learning.</strong> As is said:</p>

<blockquote><p>There are no provisions within Core ML for model retraining or federated learning, where data collected from the field is used to improve the accuracy of the model. That’s something you would have to implement by hand, most likely by asking app users to opt in for data collection and using that data to retrain the model for a future edition of the app.</p></blockquote>

<p>Refer to <a href="https://www.infoworld.com/article/3200885/machine-learning/apples-core-ml-the-pros-and-cons.html">Apple’s Core ML: The pros and cons</a>.</p>
]]></content>
  </entry>
  
</feed>
