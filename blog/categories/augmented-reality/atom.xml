<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: augmented reality | Reading Space]]></title>
  <link href="http://hongchaozhang.github.io/blog/categories/augmented-reality/atom.xml" rel="self"/>
  <link href="http://hongchaozhang.github.io/"/>
  <updated>2020-05-24T22:47:19+08:00</updated>
  <id>http://hongchaozhang.github.io/</id>
  <author>
    <name><![CDATA[Zhang Hongchao]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[关于AR的一些使用场景]]></title>
    <link href="http://hongchaozhang.github.io/blog/2018/08/17/some-ideas-on-ar-usage/"/>
    <updated>2018-08-17T15:13:58+08:00</updated>
    <id>http://hongchaozhang.github.io/blog/2018/08/17/some-ideas-on-ar-usage</id>
    <content type="html"><![CDATA[<p>搜集一些有意思的AR应用。</p>

<!-- more -->


<p>主要参考：<a href="https://github.com/olucurious/Awesome-ARKit">Awesome ARKit</a>。这篇post里面还有很多实用的AR Tutorial和Resouces，如果自己动手，可以参考。</p>

<ul>
<li><a href="#app-store%E4%B8%8A%E9%9D%A2%E7%9A%84app">App Store上面的App</a>

<ul>
<li><a href="#%E7%A5%9E%E5%A5%87ar">神奇AR</a></li>
<li><a href="#ikea-place">IKEA Place</a></li>
<li><a href="#wallr">Wallr</a></li>
<li><a href="#horizon-explorer">Horizon Explorer</a></li>
<li><a href="#weare">WeAre</a></li>
<li><a href="#waazy---magic-ar-video-maker">Waazy - Magic AR Video Maker</a></li>
<li><a href="#human-anatomy-atlas-2019">Human Anatomy Atlas 2019</a></li>
</ul>
</li>
<li><a href="#github%E4%B8%8A%E9%9D%A2%E7%9A%84%E9%A1%B9%E7%9B%AE">Github上面的项目</a>

<ul>
<li><a href="#arkit-occlusion-demo">arkit-occlusion-demo</a></li>
<li><a href="#arvideokit">[ARVideoKit]()</a></li>
<li><a href="#arkit-smb-homage">arkit-smb-homage</a></li>
<li><a href="#arkit-corelocation">ARKit-CoreLocation</a></li>
<li><a href="#arkitnavigationdemo">ARKitNavigationDemo</a></li>
<li><a href="#fineme">FineMe</a></li>
<li><a href="#arkitspitfire">ARKitSpitfire</a></li>
</ul>
</li>
<li><a href="#resources">Resources</a>

<ul>
<li><a href="#poly">Poly</a></li>
</ul>
</li>
</ul>


<p><a id="markdown-app-store上面的app" name="app-store上面的app"></a></p>

<h2>App Store上面的App</h2>

<p><a id="markdown-神奇arhttpsitunesapplecomcnappar-arid1327719623mt8" name="神奇ar"></a></p>

<h3><a href="https://itunes.apple.com/cn/app/%E7%A5%9E%E5%A5%87ar-%E7%89%B9%E6%95%88ar%E7%9B%B8%E6%9C%BA%E5%92%8C%E7%9F%AD%E8%A7%86%E9%A2%91%E6%8B%8D%E6%91%84%E7%A5%9E%E5%99%A8/id1327719623?mt=8">神奇AR</a></h3>

<p>这个AR应用非常棒，号称“中国第一AR平台”。打开App，以为自己是打开了“抖音”呢。其模仿抖音的痕迹很重，但是神奇AR的视频不同于抖音里面的视频：都是真实世界和虚拟世界的深入互动。支持下载很多3D模型。</p>

<p><a href="https://itunes.apple.com/cn/app/%E7%A5%9E%E5%A5%87ar-%E7%89%B9%E6%95%88ar%E7%9B%B8%E6%9C%BA%E5%92%8C%E7%9F%AD%E8%A7%86%E9%A2%91%E6%8B%8D%E6%91%84%E7%A5%9E%E5%99%A8/id1327719623?mt=8">神奇AR</a>还有一个很好的应用：可以让你将照片在真实场景中打开，支持默认的排列方式，也可以自由摆放多张照片。但是图片一圈都会默认带有一圈白色的过渡带和阴影，无法去除。对于不是标准长方形的图片（比如圆形的图片，圆形之外都是透明的），显示效果不佳。</p>

<p>而且，是免费的。也许是因为其主打段视频社交吧，所以免费。</p>

<p>看官方介绍：</p>

<blockquote><p>AR视频:
用户可以利用AR模型、特效、图片、视频等拍摄一段30秒的短视频，发布在神奇AR的视频流中，或者分享到各大媒体平台，将自己的创意展现给更多的人，告诉大家如何使用AR。
玩转AR:
用户可以打开AR摄像头，通过简单的操作，将AR模型放在真实世界中，创造各种神奇的景象，用AR就能创造电影里才能出现的特技。
丰富的模型:
神奇AR是一个开放内容平台，直接对接优质的AR内容提供者，他可以通过神奇AR把自己的作品第一时间开放给用户，所以我们拥有全世界最丰富的AR内容。</p></blockquote>

<p><img src="/images/shenqiar.jpg" alt="神奇 AR" /></p>

<p><a id="markdown-ikea-placehttpsitunesapplecomusappikea-placeid1279244498mt8" name="ikea-place"></a></p>

<h3><a href="https://itunes.apple.com/us/app/ikea-place/id1279244498?mt=8">IKEA Place</a></h3>

<p>宜家的官方App，有丰富的宜家家具的3D模型，真实尺寸，可以提前放置到自己的房间，看看效果。</p>

<blockquote><p>IKEA Place lets you virtually &lsquo;place&rsquo; IKEA products in your space. The app includes 3D and true-to-scale models of everything from sofas and armchairs to footstools and coffee tables. IKEA Place gives you an accurate impression of the furniture’s size, design and functionality in your home so you can stop wondering and start doing.</p></blockquote>

<p><img src="/images/IKEAPlace.jpg" alt="ikea place" /></p>

<p><a id="markdown-wallrhttpsitunesapplecomusappwallrid1278372745" name="wallr"></a></p>

<h3><a href="https://itunes.apple.com/us/app/wallr/id1278372745">Wallr</a></h3>

<p>Wallr可以让你将图片放置到真实场景的墙面上。如果你想买画装饰墙面，可以试试。不过这个功能已经在<a href="https://itunes.apple.com/cn/app/%E7%A5%9E%E5%A5%87ar-%E7%89%B9%E6%95%88ar%E7%9B%B8%E6%9C%BA%E5%92%8C%E7%9F%AD%E8%A7%86%E9%A2%91%E6%8B%8D%E6%91%84%E7%A5%9E%E5%99%A8/id1327719623?mt=8">神奇AR</a>中实现了，而且是免费的。</p>

<p>同时放置多张图片需要花钱购买。</p>

<p><img src="/images/wallr.jpg" alt="wallr" /></p>

<p><a id="markdown-horizon-explorerhttpsitunesapplecomgbapphorizon-explorerid1326860431platformipadpreservescrollpositiontrueplatformipad" name="horizon-explorer"></a></p>

<h3><a href="https://itunes.apple.com/gb/app/horizon-explorer/id1326860431?platform=ipad&amp;preserveScrollPosition=true#platform/ipad">Horizon Explorer</a></h3>

<p>AR和地图、地理位置结合的一个应用。</p>

<p>展示你设备看到的地理位置的信息，包括距离、建筑物名称、地点名称等。并把路线在一张地图上面展示给你。</p>

<p>还可以将3D的地图展示给你，让你看看某个景点或者建筑物周围等地理信息。</p>

<p>来看官方介绍：</p>

<blockquote><p>Horizon Explorer shows you the horizon and skyline around you &amp; tells you what you&rsquo;re looking at.</p>

<p>Point your camera at a hill, village, lake or landmark and Horizon Explorer will tell you what you are looking at, how far away it is, and show you a map, and information about the point you&rsquo;re aiming at.</p>

<p>ARKit technology makes the labels and alignment much more stable than used to be possible.</p>

<p>Fly up high and see the terrain laid out below you to see what is over the hills around you, and get the lay of the land, then see the scale-model 3D map that you can walk around &amp; explore to find out what&rsquo;s behind hills, or investigate up close.</p>

<p>Tracking works best on top of a hill with an unobstructed view of your surroundings (close up trees, buildings, rocks etc. can confuse the tracking). You can drag the terrain with your finger to line up with the camera if the automatic tracking is not working very well. Or try waving your phone around in the air in a figure 8 to calibrate the compass.</p></blockquote>

<p><img src="/images/HorizonExplorer.jpg" alt="Horizon Explorer" /></p>

<p><a id="markdown-wearehttpsitunesapplecomcnappweareid1304227680platformiphonepreservescrollpositiontrueplatformiphoneplatformiphoneplatformiphone" name="weare"></a></p>

<h3><a href="https://itunes.apple.com/cn/app/weare/id1304227680?platform=iphone&amp;preserveScrollPosition=true&amp;platform=iphone#platform/iphone&amp;platform=iphone">WeAre</a></h3>

<p>这个应用可以让你选择一些照片，以设备为中心围成一圈，并缓慢移动。还可以播放视频、背景音乐和编辑3D文字。用作者的话说，“可以打造一个或温馨浪漫的回忆相册,或缥缈遥远的世界.”</p>

<p>同时还是源码可以参考：<a href="https://github.com/SherlockQi/HeavenMemoirs">HeavenMemoirs - AR相册</a></p>

<p><img src="/images/weare.jpg" alt="WeAre" /></p>

<p><a id="markdown-waazy---magic-ar-video-makerhttpsitunesapplecomusappwaazy-magic-ar-video-makerid1286992749" name="waazy---magic-ar-video-maker"></a></p>

<h3><a href="https://itunes.apple.com/us/app/waazy-magic-ar-video-maker/id1286992749">Waazy - Magic AR Video Maker</a></h3>

<p>没太看懂这个应用。感觉主要做社交视频分享。录制视频还需要AR Lens。直接看官方介绍吧：</p>

<blockquote><p>Waazy is an augmented reality short video clips shooting and sharing social network, making it possible to bring virtual characters and objects to the real world.</p>

<p>Features:
- Record cool moments with AR Lens
- Tons of free and awesome AR effects
- Can add multiple AR characters at the same time
- One tap to make all the characters dance together
- Easily move and rotate a virtual character with control pad
- Themes include fantasy, monster, fun, and landmarks
- Show your original AR videos to the world</p></blockquote>

<p><img src="/images/wazzy.jpg" alt="wazzy" /></p>

<p><a id="markdown-human-anatomy-atlas-2019httpsitunesapplecomappid1117998129" name="human-anatomy-atlas-2019"></a></p>

<h3><a href="https://itunes.apple.com/app/id1117998129">Human Anatomy Atlas 2019</a></h3>

<p>其实这个主要是展示人体内部结构的3D素材，借助AR技术投射到真实场景，没有很新鲜的AR应用场景。</p>

<p>这个应用非常专业，下载需要钱，App内还要购买。</p>

<blockquote><p>Human Anatomy Atlas offers thousands of models to help understand and communicate how the human body looks and works&ndash;and includes textbook-level definitions. Use it as a reference, instead of an anatomy textbook, or to create virtual lab experiences.
Includes over 10,000 anatomical models with descriptions in English, Spanish, French, German, Italian, Japanese, and Simplified Chinese.</p></blockquote>

<p><img src="/images/HumanAnatomyAtlas2019_1.jpg" alt="Human Anatomy Atlas 2019 1" /></p>

<p><img src="/images/HumanAnatomyAtlas2019_2.jpg" alt="Human Anatomy Atlas 2019 2" /></p>

<p><a id="markdown-github上面的项目" name="github上面的项目"></a></p>

<h2>Github上面的项目</h2>

<p><a id="markdown-arkit-occlusion-demohttpsgithubcombjarnelarkit-occlusion" name="arkit-occlusion-demo"></a></p>

<h3><a href="https://github.com/bjarnel/arkit-occlusion">arkit-occlusion-demo</a></h3>

<p>事先用一些虚拟平面将真实的墙面、柜子面、门等标记出来，就可以让虚拟的小球在房间里面来回反弹，就像撞到真实的墙上返回来一样。</p>

<p><img src="/images/occlusiongame.jpg" width="600" alt="occlusion game" /></p>

<p><a id="markdown-arvideokit" name="arvideokit"></a></p>

<h3><a href="">ARVideoKit</a></h3>

<p>一个用来录制AR视频的框架。</p>

<blockquote><p>An iOS Framework that enables developers to capture videos, photos, Live Photos, and GIFs with ARKit content.</p>

<p>In other words, you NO LONGER have to screen record/screenshot to capture videos and photos of your awesome ARKit apps!</p></blockquote>

<p>其实录屏/截屏不是也挺好的吗？！</p>

<blockquote><p>Key Features:</p>

<p>✅ Capture Photos from <code>ARSCNView</code>, <code>ARSKView</code>, and <code>SCNView</code></p>

<p>✅ Capture Live Photos &amp; GIFs from <code>ARSCNView</code>, <code>ARSKView</code>, and <code>SCNView</code></p>

<p>✅ Record Videos from <code>ARSCNView</code>, <code>ARSKView</code>, and <code>SCNView</code></p>

<p>✅ Pause/Resume video</p>

<p>✅ Allow device&rsquo;s Music playing in the background while recording a video</p>

<p>✅ Returns rendered and raw buffers in a protocol method for additional Image &amp; Video processing</p></blockquote>

<p><a id="markdown-arkit-smb-homagehttpsgithubcombjarnelarkit-smb-homage" name="arkit-smb-homage"></a></p>

<h3><a href="https://github.com/bjarnel/arkit-smb-homage">arkit-smb-homage</a></h3>

<p>在现实场景中玩超级玛丽。非常粗糙，但是创意还不错。</p>

<p><img src="/images/supermario_beginning.jpg" width="600" alt="super mario beginning" /></p>

<p><img src="/images/supermario_flag.jpg" width="600" alt="super mario flag" /></p>

<p><a id="markdown-arkit-corelocationhttpsgithubcomprojectdentarkit-corelocation" name="arkit-corelocation"></a></p>

<h3><a href="https://github.com/ProjectDent/ARKit-CoreLocation">ARKit-CoreLocation</a></h3>

<p>功能：</p>

<ol>
<li>这个库最主要的工作，是在试图整合ARKit和CoreLocation，以得到更加准确的定位，从而更好地应用于AR场景。</li>
<li>基于真实地理位置，标注出摄像头中某个建筑物或者景点的标注。这个功能类似于<a href="https://itunes.apple.com/gb/app/horizon-explorer/id1326860431?platform=ipad&amp;preserveScrollPosition=true#platform/ipad">Horizon Explorer</a>。</li>
</ol>


<p>TODO: 可以仔细看看此库附带的demo：</p>

<blockquote><p>The library and demo come with a bunch of additional features for configuration. It’s all fully documented to be sure to have a look around.</p></blockquote>

<p><img src="/images/arkit+corelocation.jpg" width="600" alt="arkit + corelocation" /></p>

<p><a id="markdown-arkitnavigationdemohttpsgithubcomchriswebb09arkitnavigationdemo" name="arkitnavigationdemo"></a></p>

<h3><a href="https://github.com/chriswebb09/ARKitNavigationDemo">ARKitNavigationDemo</a></h3>

<p>在地图上选择目的地，然后在真实场景中进行AR导航。</p>

<p>但是，这个项目也只是Demo一下，作者也很谦虚：</p>

<blockquote><p>When it loads to the map, tap a place on the map where you want to navigate to and press okay. The tap can be sluggish, so you might have to try once or twice before you get it. When the navigation screen loads, tap the screen, then give it a few seconds. You should see the nodes render.</p></blockquote>

<p>TODO: 这个项目中推荐的一些参考文献还是值得看一看的。</p>

<p><img src="/images/ARKitNavigationDemo.gif" alt="ARKitNavigationDemo" /></p>

<p><a id="markdown-finemehttpsgithubcommmoaayfindme" name="fineme"></a></p>

<h3><a href="https://github.com/mmoaay/Findme">FineMe</a></h3>

<p>可以让你的朋友根据你录制的路线图找到你：</p>

<ol>
<li>通过各种方法，记录你的起点，并让另一个人知道你的起点。比如可以通过分享起点照片，或者分享起点位置。</li>
<li>通过ARKit记录你走过的路线，并将路线分享给另一个人。</li>
<li>另一个人如果找到了你的起点，就可以根据你分享的路径找到你。</li>
</ol>


<p>但是，由于ARKit不稳定，此方法也不一定奏效。</p>

<p>作者试图通过定位和距离提高路线的稳定性，但不知效果如何，分别见于以下两个分支：</p>

<ul>
<li><a href="https://github.com/mmoaay/Findme/tree/feature/location_optimize">According to location</a></li>
<li><a href="https://github.com/mmoaay/Findme/tree/feature/distance_optimize">According to distance</a></li>
</ul>


<p><a id="markdown-arkitspitfirehttpsgithubcomchriswebb09arkitspitfire" name="arkitspitfire"></a></p>

<h3><a href="https://github.com/chriswebb09/ARKitSpitfire">ARKitSpitfire</a></h3>

<p>可以让一架3D飞机模型，根据提供的地理位置经纬度，调整姿态，并飞向那里。</p>

<p><img src="/images/ARKitSpitfire.gif" alt="ARKitSpitfire" /></p>

<p><a id="markdown-resources" name="resources"></a></p>

<h2>Resources</h2>

<p><a id="markdown-polyhttpsgithubcompiemontepoly" name="poly"></a></p>

<h3><a href="https://github.com/piemonte/Poly">Poly</a></h3>

<p><a href="https://github.com/piemonte/Poly">Poly</a>是一个iOS库，用来从<a href="https://developers.google.com/poly/">Google Poly</a>上下载3D模型，包含搜索、下载管理和缓存功能。</p>

<p><a href="https://developers.google.com/poly/develop/ios">iOS Quickstart</a>展示了如何在iOS中下载使用<a href="https://developers.google.com/poly/">Google Poly</a>上面的3D资源。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[What is new in ARKit 2]]></title>
    <link href="http://hongchaozhang.github.io/blog/2018/06/13/what-is-new-in-n-arkit-2/"/>
    <updated>2018-06-13T13:24:59+08:00</updated>
    <id>http://hongchaozhang.github.io/blog/2018/06/13/what-is-new-in-n-arkit-2</id>
    <content type="html"><![CDATA[<!-- more -->


<ul>
<li><a href="#overview">Overview</a></li>
<li><a href="#new-features-in-arkit-2">New Features in ARKit 2</a>

<ul>
<li><a href="#saving-and-loading-maps">Saving and Loading Maps</a>

<ul>
<li><a href="#world-tracking-recap">World Tracking Recap:</a></li>
<li><a href="#world-tracking-enhancement">World Tracking Enhancement:</a></li>
<li><a href="#saving-and-loading-maps">Saving and loading maps:</a></li>
<li><a href="#how-to-get-a-good-map">How to get a good map</a></li>
</ul>
</li>
<li><a href="#environment-texturing">Environment Texturing</a></li>
<li><a href="#image-tracking">Image Tracking</a></li>
<li><a href="#3d-object-detection">3D Object Detection</a></li>
<li><a href="#face-tracking-enhancements">Face Tracking Enhancements</a></li>
</ul>
</li>
<li><a href="#some-other-wwdc-sessions-related-to-ar">Some other WWDC Sessions Related to AR</a>

<ul>
<li><a href="#integrating-apps-and-content-with-ar-quick-look">Integrating Apps and Content with AR Quick Look</a></li>
<li><a href="#inside-swiftshot-creating-an-ar-game">Inside SwiftShot: Creating an AR Game</a></li>
<li><a href="#creating-great-ar-experiences">Creating Great AR Experiences</a></li>
<li><a href="#understanding-arkit-tracking-and-detection">Understanding ARKit Tracking and Detection</a></li>
</ul>
</li>
<li><a href="#some-other-materials-for-a-better-ar-app">Some other materials for a better AR app:</a>

<ul>
<li><a href="#building-your-first-ar-experience">Building Your First AR Experience</a></li>
<li><a href="#managing-session-lifecycle-and-tracking-quality">Managing Session Lifecycle and Tracking Quality</a></li>
<li><a href="#human-interface-guidelines---augmented-reality">Human Interface Guidelines - Augmented Reality</a></li>
<li><a href="#handling-3d-interaction-and-ui-controls-in-augmented-reality">Handling 3D Interaction and UI Controls in Augmented Reality</a></li>
<li><a href="#creating-a-multiuser-ar-experience">Creating a Multiuser AR Experience</a></li>
<li><a href="#swiftshot-creating-a-game-for-augmented-reality">SwiftShot: Creating a Game for Augmented Reality</a></li>
<li><a href="#recognizing-images-in-an-ar-experience">Recognizing Images in an AR Experience</a></li>
<li><a href="#scanning-and-detecting-3d-objects">Scanning and Detecting 3D Objects</a></li>
</ul>
</li>
</ul>


<p><a id="markdown-overview" name="overview"></a></p>

<h2>Overview</h2>

<p>In ARKit 1, we have:</p>

<ul>
<li>Device positioning from world tracking process</li>
<li>Horizontal and vertical plane detection from world tracking process</li>
<li>Lighting estimation</li>
<li>AR face tracking</li>
</ul>


<p>In ARKit 2, we have:</p>

<ul>
<li>Saving and loading maps</li>
<li>Environment Texturing</li>
<li>Image detection and tracking</li>
<li>3D object tracking</li>
<li>Improved face tracking</li>
</ul>


<p><a id="markdown-new-features-in-arkit-2" name="new-features-in-arkit-2"></a></p>

<h2>New Features in ARKit 2</h2>

<p><a id="markdown-saving-and-loading-maps" name="saving-and-loading-maps"></a></p>

<h3>Saving and Loading Maps</h3>

<p><a id="markdown-world-tracking-recap" name="world-tracking-recap"></a></p>

<h4>World Tracking Recap:</h4>

<ul>
<li>Position and orientation of the device.</li>
<li>Physical scale in the scene.</li>
<li>3D feature points.</li>
<li>Relocalization (iOS 11.3): we can relocalize objects when your AR session is interrupted, like phone coming or going from background. This feature is implemented by storing the mapping <code>ARWorldMap</code> between real world and the coordinate system. However the mapping is not exposed to developers.</li>
</ul>


<p><a id="markdown-world-tracking-enhancement" name="world-tracking-enhancement"></a></p>

<h4>World Tracking Enhancement:</h4>

<ul>
<li><strong>Saving and loading maps</strong>: expose the <code>ARWorldMap</code> to developers.</li>
<li>Faster initialization and plane detection</li>
<li>Robust tracking and plane detection</li>
<li>More accurate extent and boundary Continuous autofocus</li>
<li>New 4:3 video formats (iPad is also 4:3)</li>
</ul>


<p><a id="markdown-saving-and-loading-maps" name="saving-and-loading-maps"></a></p>

<h4>Saving and loading maps:</h4>

<p><code>ARWorldmap</code> contains:</p>

<ul>
<li>Mapping of physical 3D space: for representing 3D feature points in the coordinate system.</li>
<li>Mutable list of named anchors: for restoring previous 3D environment (like lighting node anchor), and relocalizing previously added virtual objects.</li>
<li>Raw feature points and extent: for debugging and visualization.</li>
<li>Serialization: for storing and recovering from an file.</li>
</ul>


<p><img src="/images/arkit2_arworldmap.jpg" width="500" alt="arkit arworldmap" /></p>

<p>We can use the map in two different ways:</p>

<ul>
<li>Persistent: Restore previous AR scene for a new AR session. For example, you go to another room and come back or close the AR app and open it some time later.</li>
<li>Multiuser experience: We can share the map among devices through WiFi or bluetooth.</li>
</ul>


<p>The SwiftShot is an multiuser experience AR game:</p>

<p><img src="/images/swiftshot.jpg" alt="arkit2 swiftshot" /></p>

<p>and the following is a small piece of the demo:</p>

<p><img src="/images/arkit2_multiuser_experience_demo.gif" alt="swift shot game" /></p>

<p><a id="markdown-how-to-get-a-good-map" name="how-to-get-a-good-map"></a></p>

<h4>How to get a good map</h4>

<p>In order to share or restore the map, we need to get a good one first. A good map should be:</p>

<!-- * Important for relocalization -->


<ul>
<li>Multiple points of view: If we record the mapping from one point of view, and try to restore the coordinate system from another point of view, it will fail.</li>
<li>Static, well-textured environment.</li>
<li>Dense feature points on the map.</li>
</ul>


<p>We can use the <code>WorldMappingStatus</code> status from <code>ARFrame</code> to decide if the current map is good enough for sharing or storing:</p>

<pre><code class="swift">public enum WorldMappingStatus : Int {
   case notAvailable
   case limited
   case extending
   case mapped 
}
</code></pre>

<p><a id="markdown-environment-texturing" name="environment-texturing"></a></p>

<h3>Environment Texturing</h3>

<p>With the help of Environment Texturing, AR scene objects can reflect the environment texture on the surface of themselves, just like:</p>

<p><img src="/images/arkit2_environment_texturing_demo.jpg" alt="arkit2 environment texturing demo" /></p>

<p><a id="markdown-image-tracking" name="image-tracking"></a></p>

<h3>Image Tracking</h3>

<p>Moving objects can not be positioned in ARKit 1. In ARKit 2, specified images can be tracked in AR scene.</p>

<p><img src="/images/arkit2_image_tracking.gif" alt="arkit 2 image tracking" /></p>

<p>The classes in ARKit 2 for image tracking are:</p>

<p><img src="/images/arkit2_image_tracking_classes.jpg" alt="arkit 2 image tracking classes" /></p>

<p>The detected <code>ARImageAnchor</code>s have properties like:</p>

<pre><code class="swift">open class ARImageAnchor : ARAnchor, ARTrackable { 
    public var isTracked: Bool { get }
    open var transform: simd_float4x4 { get }
    open var referenceImage: ARReferenceImage { get }
}
</code></pre>

<p>The specified image should:</p>

<ul>
<li>Histogram should be broad</li>
<li>Not have multiple uniform color regions</li>
<li>Not have repeated structures</li>
</ul>


<p>The following is the demo:</p>

<p><img src="/images/arkit2_image_tracking_demo.gif" alt="arkit 2 image tracking demo" /></p>

<p>The inputs of the above demo are:</p>

<ul>
<li>an static image of the cat, the same as it is in the picture frame</li>
<li>an video of the cat</li>
</ul>


<p>The video is played at the position of the specified picture frame, with the same orientation of the picture frame.</p>

<p>There are two classes related to image tracking:</p>

<table>
<thead>
<tr>
<th>ARImageTrackingConfiguration </th>
<th> ARWorldTrackingConfiguration</th>
</tr>
</thead>
<tbody>
<tr>
<td>Has No World Origin </td>
<td> Has World Origin</td>
</tr>
<tr>
<td>After detecting the image, only do things inside the place of the image. </td>
<td> After detecting the image, place some virtual objects outside the detected image plane.</td>
</tr>
</tbody>
</table>


<p><a id="markdown-3d-object-detection" name="3d-object-detection"></a></p>

<h3>3D Object Detection</h3>

<p>3D object detection workflow is:</p>

<p><img src="/images/arkit2_3D_object_tracking_classes.jpg" alt="arkit2 3D object tracking classes" /></p>

<p>The <code>ARObjectAnchor</code> contains properties like:</p>

<pre><code class="swift">open class ARObjectAnchor : ARAnchor {
    open var transform: simd_float4x4 { get }
    open var referenceObject: ARReferenceObject { get }
}
</code></pre>

<p>and <code>ARReferenceObject</code> is the scanned 3D object:</p>

<pre><code class="swift">open class ARReferenceObject
    : NSObject, NSCopying, NSSecureCoding {
    open var name: String?
    open var center: simd_float3 { get }
    open var extent: simd_float3 { get }
    open var rawFeaturePoints: ARPointCloud { get }
}
</code></pre>

<blockquote><p>An <code>ARReferenceObject</code> contains only the spatial feature information needed for ARKit to recognize the real-world object, and is not a displayable 3D reconstruction of that object.</p></blockquote>

<p>In order to get the <code>ARReferenceObject</code>, we should scan the real object, and store the result as an file (.arobject) or an xcode asset catalog for ARKit to use. Fortunately, Apple supplies a demo for scanning 3D object to get the <code>ARReferenceObject</code>. Refer to: <a href="https://developer.apple.com/documentation/arkit/scanning_and_detecting_3d_objects">Scanning and Detecting 3D Objects</a> for detail and the rough steps of object scanning are:</p>

<p><img src="/images/arkit2_3D_object_scan.jpg" alt="arkit2 3D object scan" /></p>

<p>For scanned object in the real world, we can dynamically add some info around it (Museum is a good use case.), like the demo does:</p>

<p><img src="/images/arkit2_3D_object_tracking_demo.gif" alt="arkit2 object tracking demo" /></p>

<p><a id="markdown-face-tracking-enhancements" name="face-tracking-enhancements"></a></p>

<h3>Face Tracking Enhancements</h3>

<p>With face tracking, we can place something on it or around it.</p>

<p>Enhancements in ARKit 2:</p>

<ul>
<li>Gaze tracking</li>
<li>Tongue support</li>
</ul>


<blockquote><p>Gaze and Tongue can be input of the AR app.</p></blockquote>

<p>New changes in one screenshot:</p>

<p><img src="/images/what-is-new-in-arkit-2.jpg" alt="what-is-new-in-arkit-2" /></p>

<p><a id="markdown-some-other-wwdc-sessions-related-to-ar" name="some-other-wwdc-sessions-related-to-ar"></a></p>

<h2>Some other WWDC Sessions Related to AR</h2>

<p><a id="markdown-integrating-apps-and-content-with-ar-quick-lookhttpsdeveloperapplecomvideosplaywwdc2018603" name="integrating-apps-and-content-with-ar-quick-lookhttpsdeveloperapplecomvideosplaywwdc2018603"></a></p>

<h3><a href="https://developer.apple.com/videos/play/wwdc2018/603/">Integrating Apps and Content with AR Quick Look</a></h3>

<p>A deeper dive into a new feature in iOS that provides a way to preview any AR object from a USDZ file.</p>

<p><img src="/images/QLPreviewController.png" alt="QLPreviewController" /></p>

<ul>
<li>There’s a great sequence diagram presented (see above) (I wish more sessions would have these!) for previewing USDZ objects, of which the <code>QLPreviewController</code> plays a central role.</li>
<li>For web developers, it covers HTML samples for how to preview USDZ objects in Safari.</li>
<li>Then it goes into a deep dive on how to create the actual USDZ objects, with more examples on new AR texturing capabilities.</li>
<li>There’s also a quick overview on how to optimize the files, to keep the size down, and there’s a breakdown of the files that make up the USDZ format.</li>
</ul>


<p><a id="markdown-inside-swiftshot-creating-an-ar-gamehttpsdeveloperapplecomvideosplaywwdc2018605" name="inside-swiftshot-creating-an-ar-gamehttpsdeveloperapplecomvideosplaywwdc2018605"></a></p>

<h3><a href="https://developer.apple.com/videos/play/wwdc2018/605/">Inside SwiftShot: Creating an AR Game</a></h3>

<p>Covers world map sharing, networking, and the physics of how to build an AR game, as well as some design insight (I have limited game dev experience so I’ll do the best I can below).</p>

<ul>
<li>Pointers to remember with designing an AR game, such as “encouraging” the user to slowly move the device for world mapping!</li>
<li>It demonstrates the usage of image &amp; object detection, world map sharing, and iBeacons for the game.</li>
<li>Integrating <code>ARKit</code> with <code>SceneKit</code> and <code>Metal</code>, including the translation of physics data between each — position, velocity, and orientation.</li>
<li>Performance enhancement with the <code>BitStreamCodable</code> protocol.</li>
<li>A small look at how audio was integrated into the game.</li>
</ul>


<p><a id="markdown-creating-great-ar-experienceshttpsdeveloperapplecomvideosplaywwdc2018805" name="creating-great-ar-experienceshttpsdeveloperapplecomvideosplaywwdc2018805"></a></p>

<h3><a href="https://developer.apple.com/videos/play/wwdc2018/805/">Creating Great AR Experiences</a></h3>

<p>Best practises mainly from a UX &amp; design perspective (there are no code samples in this session).</p>

<ul>
<li>Logical dos and don’ts that may be useful, if you need help with thought towards product and empathy towards the user.</li>
<li>They emphasize the importance of using transitions between AR scapes.</li>
<li>Why AR is a special combination of touch and movement.</li>
<li>They advise that minimal battery impact should be a huge focus! This is a challenge, given that they recommend to render the FPS at 60 to avoid latency.</li>
<li>There’s a lengthy demonstration of creating an AR fireplace, with complex texturing, etc. It looks great, but unfortunately there were no coding samples accompanying the demo.</li>
</ul>


<p><a id="markdown-understanding-arkit-tracking-and-detectionhttpsdeveloperapplecomvideosplaywwdc2018610" name="understanding-arkit-tracking-and-detectionhttpsdeveloperapplecomvideosplaywwdc2018610"></a></p>

<h3><a href="https://developer.apple.com/videos/play/wwdc2018/610/">Understanding ARKit Tracking and Detection</a></h3>

<p>A good broad overview of all of the main AR concepts.</p>

<ul>
<li>This is such a good intro into not only AR on iOS, but AR in general, that it should have been part of 2017’s sessions when ARKit was first introduced. Better late than never. If you’re only going to watch one session, watch this one!</li>
<li>It recaps the main features of ARKit — <strong>orientation</strong>, <strong>world tracking</strong>, and <strong>plane detection</strong>, and demos all of these in depth with coding samples.</li>
<li>It then demos the new features of ARKit 2 — <strong>shared world mapping</strong>, <strong>image tracking</strong>, and <strong>object detection</strong> (which has been available in the Vision framework recapped above, but is now also accessible in ARKit).</li>
<li>A good explanation on a core AR principle, <strong>Visual Inertial Odometry</strong>, is given. Short of going into the actual physics equations behind it, this should give you a great understanding of VIO.</li>
</ul>


<p><a id="markdown-some-other-materials-for-a-better-ar-app" name="some-other-materials-for-a-better-ar-app"></a></p>

<h2>Some other materials for a better AR app:</h2>

<p><a id="markdown-building-your-first-ar-experiencehttpsdeveloperapplecomdocumentationarkitbuilding_your_first_ar_experience" name="building-your-first-ar-experiencehttpsdeveloperapplecomdocumentationarkitbuilding_your_first_ar_experience"></a></p>

<h3><a href="https://developer.apple.com/documentation/arkit/building_your_first_ar_experience">Building Your First AR Experience</a></h3>

<p>This document demos an app for basic usage of ARKit.</p>

<p><a id="markdown-managing-session-lifecycle-and-tracking-qualityhttpsdeveloperapplecomdocumentationarkitmanaging_session_lifecycle_and_tracking_quality" name="managing-session-lifecycle-and-tracking-qualityhttpsdeveloperapplecomdocumentationarkitmanaging_session_lifecycle_and_tracking_quality"></a></p>

<h3><a href="https://developer.apple.com/documentation/arkit/managing_session_lifecycle_and_tracking_quality">Managing Session Lifecycle and Tracking Quality</a></h3>

<p>Make your AR experience more robust by</p>

<ul>
<li>providing clear feedback, using <code>ARCamera.TrackingState</code>.</li>
<li>recovering from interruptions, using <code>ARCamera.TrackingState.Reason.relocalizing</code>.</li>
<li>resuming previous sessions, using <code>ARWorldMap</code>.</li>
</ul>


<p><a id="markdown-human-interface-guidelines---augmented-realityhttpsdeveloperapplecomdesignhuman-interface-guidelinesiossystem-capabilitiesaugmented-reality" name="human-interface-guidelines---augmented-realityhttpsdeveloperapplecomdesignhuman-interface-guidelinesiossystem-capabilitiesaugmented-reality"></a></p>

<h3><a href="https://developer.apple.com/design/human-interface-guidelines/ios/system-capabilities/augmented-reality/">Human Interface Guidelines - Augmented Reality</a></h3>

<p>This post describes how to rendering virtual objects, how to interact with virtual objects, how to handling interruptions. It is for UX.</p>

<p><a id="markdown-handling-3d-interaction-and-ui-controls-in-augmented-realityhttpsdeveloperapplecomdocumentationarkithandling_3d_interaction_and_ui_controls_in_augmented_reality" name="handling-3d-interaction-and-ui-controls-in-augmented-realityhttpsdeveloperapplecomdocumentationarkithandling_3d_interaction_and_ui_controls_in_augmented_reality"></a></p>

<h3><a href="https://developer.apple.com/documentation/arkit/handling_3d_interaction_and_ui_controls_in_augmented_reality">Handling 3D Interaction and UI Controls in Augmented Reality</a></h3>

<p>This document describes the best practices for visual feedback, gesture interactions, and realistic rendering in AR experiences. And a demo app is supplied.</p>

<p><img src="/images/arkit_demo_screenshot.jpg" alt="arkit demo" /></p>

<p><a id="markdown-creating-a-multiuser-ar-experiencehttpsdeveloperapplecomdocumentationarkitcreating_a_multiuser_ar_experience" name="creating-a-multiuser-ar-experiencehttpsdeveloperapplecomdocumentationarkitcreating_a_multiuser_ar_experience"></a></p>

<h3><a href="https://developer.apple.com/documentation/arkit/creating_a_multiuser_ar_experience">Creating a Multiuser AR Experience</a></h3>

<p>This document demos an app (with source code) on how to transmit ARKit world-mapping data between nearby devices with the <a href="https://developer.apple.com/documentation/multipeerconnectivity">MultipeerConnectivity</a> framework (introduced in iOS 7.0) to create a shared basis for AR experiences. MultipeerConnectivity supports peer-to-peer connectivity and the discovery of nearby devices. With MultipeerConnectivity, you can not only share <code>ARWorldMap</code>, but also some actions. This makes multiuser AR game possible.</p>

<p>However:</p>

<ul>
<li>Recording and transmitting a world map and relocalizing to a world map are time-consuming, bandwidth-intensive operations. A good design is needed for better performance.</li>
<li>The persons received the world map data need to move their device so they see a similar perspective (also sent by the host) helps ARKit process the received map and establish a shared frame of reference for the multiuser experience.</li>
</ul>


<p><a id="markdown-swiftshot-creating-a-game-for-augmented-realityhttpsdeveloperapplecomdocumentationarkitswiftshot_creating_a_game_for_augmented_reality" name="swiftshot-creating-a-game-for-augmented-realityhttpsdeveloperapplecomdocumentationarkitswiftshot_creating_a_game_for_augmented_reality"></a></p>

<h3><a href="https://developer.apple.com/documentation/arkit/swiftshot_creating_a_game_for_augmented_reality">SwiftShot: Creating a Game for Augmented Reality</a></h3>

<p>This document demos the SwiftShot game shown on WWDC 2018, including:</p>

<ul>
<li>Designing Gameplay for AR</li>
<li>Using Local Multipeer Networking and Sharing World Maps</li>
<li>Synchronizing Gameplay Actions</li>
<li>Solving Multiplayer Physics</li>
</ul>


<p><a id="markdown-recognizing-images-in-an-ar-experiencehttpsdeveloperapplecomdocumentationarkitrecognizing_images_in_an_ar_experience" name="recognizing-images-in-an-ar-experiencehttpsdeveloperapplecomdocumentationarkitrecognizing_images_in_an_ar_experience"></a></p>

<h3><a href="https://developer.apple.com/documentation/arkit/recognizing_images_in_an_ar_experience">Recognizing Images in an AR Experience</a></h3>

<p>Detect known 2D images in the user’s environment, and use their positions to place AR content.</p>

<p><a id="markdown-scanning-and-detecting-3d-objectshttpsdeveloperapplecomdocumentationarkitscanning_and_detecting_3d_objects" name="scanning-and-detecting-3d-objectshttpsdeveloperapplecomdocumentationarkitscanning_and_detecting_3d_objects"></a></p>

<h3><a href="https://developer.apple.com/documentation/arkit/scanning_and_detecting_3d_objects">Scanning and Detecting 3D Objects</a></h3>

<p>Record spatial features of real-world objects, then use the results to find those objects in the user’s environment and trigger AR content.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[DR Project]]></title>
    <link href="http://hongchaozhang.github.io/blog/2018/01/02/dr-project/"/>
    <updated>2018-01-02T13:39:28+08:00</updated>
    <id>http://hongchaozhang.github.io/blog/2018/01/02/dr-project</id>
    <content type="html"><![CDATA[<p>Key words: ARKit, CoreML, SceneKit</p>

<!-- more -->


<p><a href="https://github.com/hongchaozhang/ProjectDataToReality">DR (Data to Reality)</a> is a demo for projecting data into reality: Using <strong>CoreML</strong> for object recognition, and then get the recognized object data and project the data to reality, just above the recognized object. In this process, <strong>ARKit</strong> helps us to get the real world object coordinate to put the data at, and <strong>SceneKit</strong> helps us to render the data in reality.</p>

<p>This is a screenshot in demo:</p>

<p><img src="/images/DR-Screenshot-1.jpg" alt="project chart to reality" /></p>

<p>Refer to github <a href="https://github.com/hongchaozhang/ProjectDataToReality">Project Data to Reality</a> for demo project. In the github page, the following are told:</p>

<ol>
<li>Requirement</li>
<li>How to Run the Project</li>
<li>How to Use the Demo

<ol>
<li>Project Chart to Reality</li>
<li>Face Detection</li>
<li>Face Recognition</li>
</ol>
</li>
</ol>


<h2>Related techniques used</h2>

<ol>
<li><a href="../../../../2017/12/28/arkit-usage/">ARKit</a></li>
<li><a href="../../../../2017/12/28/coreml-usage/">CoreML</a></li>
<li><a href="../../../../2018/01/02/scenekit-usage/">SceneKit</a></li>
</ol>


<h2>Notes on the Demo</h2>

<p>As this is a rough demo, it need some enhancements:</p>

<ol>
<li>Only four kinds of fruits are supported: banana, orange, cucumber and strawberry. But for anything recognized by Inceptionv3.mlmodel, we can add a sphere and the name just at the world position of the object. (Set <code>showRecognizedResultNearby</code> to <code>true</code>.)</li>
<li>The chart data of the four kinds of fruits are images exported from other apps.</li>
<li>For face detection on iphone, rotate the device to left by 90 degrees to make it work on landscape. This is an issue need to be fixed.</li>
<li>Face recognition needs a trained face recognition model, called FaceRecognition.mlmodel.</li>
<li>Face recognition request doesn&rsquo;t crop the image from camera according to the face detection result. This should be done to make face recognition more robust.</li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[ARKit Usage]]></title>
    <link href="http://hongchaozhang.github.io/blog/2017/12/28/arkit-usage/"/>
    <updated>2017-12-28T17:25:27+08:00</updated>
    <id>http://hongchaozhang.github.io/blog/2017/12/28/arkit-usage</id>
    <content type="html"><![CDATA[<!-- more -->




<p><!-- TOC -->autoauto- <a href="#cool-ar-apps-in-app-store">Cool AR Apps in App Store</a>auto    - <a href="#world-brush">World Brush</a>auto    - <a href="#ikea-place">IKEA Place</a>auto    - <a href="#ar-measurekit">AR MeasureKit</a>auto- <a href="#requirement">Requirement</a>auto- <a href="#arkit-usage">ARKit Usage</a>auto    - <a href="#arkit-related-techniques">ARKit Related Techniques</a>auto    - <a href="#arkit-in-ios-system">ARKit in iOS System</a>auto    - <a href="#arkit-key-classes">ARKit Key Classes</a>auto    - <a href="#arsession"><code>ARSession</code></a>auto    - <a href="#arconfiguration"><code>ARConfiguration</code></a>auto    - <a href="#more-on-arworldtrackingconfiguration">More on <code>ARWorldTrackingConfiguration</code></a>auto        - <a href="#tracking-quality">Tracking Quality</a>auto    - <a href="#arframe"><code>ARFrame</code></a>auto    - <a href="#hittest-for-real-world-position">HitTest for Real World Position</a>auto        - <a href="#existingplane"><code>existingPlane</code></a>auto        - <a href="#existingplaneusingextent"><code>existingPlaneUsingExtent</code></a>auto        - <a href="#estimatedhorizontalplane"><code>estimatedHorizontalPlane</code></a>auto        - <a href="#featurepoint"><code>featurePoint</code></a>auto    - <a href="#display-virtual-object-in-real-world">Display Virtual Object in Real World</a>auto        - <a href="#standard-view">Standard View</a>auto        - <a href="#custom-view">Custom View</a>auto- <a href="#best-practices-and-limitations">Best Practices and Limitations</a>auto    - <a href="#best-practices">Best Practices</a>auto    - <a href="#limitations">Limitations</a>auto- <a href="#ar-from-google">AR from Google</a>autoauto<!-- /TOC --></p>

<p><a id="markdown-cool-ar-apps-in-app-store" name="cool-ar-apps-in-app-store"></a></p>

<h2>Cool AR Apps in App Store</h2>

<p><a id="markdown-world-brush" name="world-brush"></a></p>

<h3>World Brush</h3>

<p><a href="https://itunes.apple.com/us/app/world-brush/id1277410449?mt=8">World Brush</a> is an AR experience where users can paint with brushes on the world around them. Every painting is saved at the approximate GPS location where it was created, and will be recommended to the user around.</p>

<p><img src="/images/ARKit_WorldBrush.png" width="300" alt="world brush" /></p>

<p><a id="markdown-ikea-place" name="ikea-place"></a></p>

<h3>IKEA Place</h3>

<p><a href="https://itunes.apple.com/us/app/ikea-place/id1279244498?mt=8">IKEA Place</a> lets you virtually &lsquo;place&rsquo; IKEA products in your space.</p>

<p><img src="/images/ARKit_IkeaPlace.png" width="300" alt="ikea place" /></p>

<p><a id="markdown-ar-measurekit" name="ar-measurekit"></a></p>

<h3>AR MeasureKit</h3>

<p><a href="https://itunes.apple.com/us/app/ar-measurekit/id1258270451?mt=8">AR MeasureKit</a> makes it really easy to measure different things in the world using your iPhone’s or iPad’s camera.</p>

<p><img src="/images/ARKit_MeasureKit.png" width="300" alt="ar measure kit" /></p>

<p><a id="markdown-requirement" name="requirement"></a></p>

<h2>Requirement</h2>

<ul>
<li>iOS 11 and above system.</li>
<li>iOS device with an A9 or later processor.</li>
</ul>


<p>To make your app available only on devices supporting ARKit, use the arkit key in the <code>UIRequiredDeviceCapabilities</code> section of your app&rsquo;s Info.plist. If augmented reality is a secondary feature of your app, use the <code>ARWorldTrackingSessionConfiguration.isSupported</code> property to determine whether the current device supports the session configuration you want to use.</p>

<p><a id="markdown-arkit-usage" name="arkit-usage"></a></p>

<h2>ARKit Usage</h2>

<p><a id="markdown-arkit-related-techniques" name="arkit-related-techniques"></a></p>

<h3>ARKit Related Techniques</h3>

<p><img src="/images/ARKitRelatedTechs.png" alt="arkit related techs" /></p>

<p><a id="markdown-arkit-in-ios-system" name="arkit-in-ios-system"></a></p>

<h3>ARKit in iOS System</h3>

<p><img src="/images/ARKitFramework.png" alt="arkit in ios system" /></p>

<p><a id="markdown-arkit-key-classes" name="arkit-key-classes"></a></p>

<h3>ARKit Key Classes</h3>

<p><img src="/images/ARKitUsage.png" alt="arkit usage" /></p>

<p><a id="markdown-arsession" name="arsession"></a></p>

<h3><code>ARSession</code></h3>

<p>An <a href="https://developer.apple.com/documentation/arkit/arsession"><code>ARSession</code></a> object coordinates the major processes that ARKit performs on your behalf to create an augmented reality experience. These processes include reading data from the device&rsquo;s motion sensing hardware, controlling the device&rsquo;s built-in camera, and performing image analysis on captured camera images. The session synthesizes all of these results to establish a correspondence between the real-world space the device inhabits and a virtual space where you model AR content.</p>

<p>Every AR experience built with ARKit requires a single <code>ARSession</code>object. If you use an <code>ARSCNView</code> or <code>ARSKView</code> object to easily build the visual part of your AR experience, the view object includes an <code>ARSession</code> instance. If you build your own renderer for AR content, you&rsquo;ll need to instantiate and maintain an ARSession object yourself.</p>

<p>Running a session requires a session configuration: an instance of the <code>ARConfiguration</code> class, or its subclass <code>ARWorldTrackingConfiguration</code>. These classes determine how ARKit tracks a device&rsquo;s position and motion relative to the real world, and thus affect the kinds of AR experiences you can create.</p>

<p><a id="markdown-arconfiguration" name="arconfiguration"></a></p>

<h3><code>ARConfiguration</code></h3>

<p><a href="https://developer.apple.com/documentation/arkit/arconfiguration"><code>ARConfiguration</code></a> is an abstract class; you do not create or work with instances of this class.</p>

<p>To run an AR session, create an instance of the concrete <code>ARConfiguration</code> subclass that provides the kind of augmented reality experience you want to use in your app or game. Then, set up the configuration object&rsquo;s properties and pass the configuration to your session&rsquo;s <code>run(_:options:)</code> method. ARKit includes the following concrete configuration classes:</p>

<ul>
<li><p><a href="https://developer.apple.com/documentation/arkit/arworldtrackingconfiguration"><code>ARWorldTrackingConfiguration</code></a>
Provides high-quality AR experiences that use the rear-facing camera precisely track a device&rsquo;s position and orientation and allow plane detection and hit testing. Creating and maintaining this correspondence between spaces requires tracking the device&rsquo;s motion. The <code>ARWorldTrackingConfiguration</code> class tracks the device&rsquo;s movement with six degrees of freedom (6DOF): specifically, the three rotation axes (roll, pitch, and yaw), and three translation axes (movement in x, y, and z).</p></li>
<li><p><a href="https://developer.apple.com/documentation/arkit/arorientationtrackingconfiguration"><code>AROrientationTrackingConfiguration</code></a>
Provides basic AR experiences that use the rear-facing camera and track only a device&rsquo;s orientation. Creating and maintaining this correspondence between spaces requires tracking the device&rsquo;s motion. The <code>AROrientationTrackingConfiguration</code> class tracks the device&rsquo;s movement with three degrees of freedom (3DOF): specifically, the three rotation axes (roll, pitch, and yaw).</p>

<p>  <code>AROrientationTrackingConfiguration</code> cannot track movement of the device, and 3DOF tracking does not support plane detection or hit testing.</p>

<p>  Use 3DOF tracking only as a fallback in situations where 6DOF tracking is temporarily unavailable.</p></li>
<li><p><a href="https://developer.apple.com/documentation/arkit/arfacetrackingconfiguration"><code>ARFaceTrackingConfiguration</code></a>
Provides AR experiences that use the front-facing camera and track the movement and expressions of the user&rsquo;s face.</p></li>
</ul>


<p>Face tracking is available only on iOS devices with a front-facing TrueDepth camera.</p>

<p>An official example <a href="https://developer.apple.com/documentation/arkit/creating_face_based_ar_experiences">Creating Face-Based AR Experiences</a> demonstrates that you can place and animate 3D content that follows the user’s face and matches facial expressions, using the TrueDepth camera on iPhone X.</p>

<p><a id="markdown-more-on-arworldtrackingconfiguration" name="more-on-arworldtrackingconfiguration"></a></p>

<h3>More on <code>ARWorldTrackingConfiguration</code></h3>

<p>Refer to <a href="https://developer.apple.com/documentation/arkit/about_augmented_reality_and_arkit">About Augmented Reality and ARKit</a> for an official explanation of the world tracking things.</p>

<p>World tracking process can be illustrated as:</p>

<p><img src="/images/ARKitTracking.gif" alt="arkit world tracking" /></p>

<blockquote><p>One question here: <em>How does ARKit know how long is 1 meter in the real world?</em> <a href="https://www.quora.com/How-can-Apple%E2%80%99s-ARKit-Augmented-Reality-do-precise-measurement-with-just-one-camera">How can Apple’s ARKit (Augmented Reality) do precise measurement with just one camera?</a> is trying to figure this out:</p>

<p>&ldquo;When an iPhone camera is turned on, it doesn’t have two different images with which to calculate distances. However, a moment after the first image is taken it does have a second image. <strong>Thanks to data from the iPhone accelerometer sensors, it can also estimate the difference - from the first image to the second - of the iPhone camera’s 3D position and aim.</strong> Now we go back to those “known features” being tracked. For each image the iPhone doesn’t just do this for a single feature, it maps as many features as it can. Aside from doing the triangulation on each of the features in the images, it also does a comparison between the differences in each feature’s relationship to other features in the image. So now, like your brain, the iPhone has two different views of something, knows the approximate angles of focus, knows the distance between the lens position, is tracking known features and their relationship to each other. From this, the iPhone can get a very good approximation of how each feature is positioned in space with relation to the other features, essentially producing a 3D mapping of the space.&rdquo;</p></blockquote>

<p><a id="markdown-tracking-quality" name="tracking-quality"></a></p>

<h4>Tracking Quality</h4>

<p>To get better tracking quality:</p>

<ol>
<li>Uninterrupted sensor data</li>
<li>Textured environments</li>
<li>Static scenes</li>
</ol>


<p>If tracking quality changes, the tracking state will also change:</p>

<p><img src="/images/ARKitTrackingState.png" alt="arkit tracing state transition" /></p>

<p>And the tracking state changes will be notified by:</p>

<pre><code class="swift">
func session(_ session: ARSession, cameraDidChangeTrackingState camera: ARCamera) { 
    if case .limited(let reason) = camera.trackingState {
        // Notify user of limited tracking state
        ...
    } 
}
</code></pre>

<p><a id="markdown-arframe" name="arframe"></a></p>

<h3><code>ARFrame</code></h3>

<p>After world tracking, we can get the 6 DOF of the camera, used for the upcoming rendering. These infos are stored in each <code>ARFrame</code>.</p>

<p><code>ARFrame</code> owns video image and position tracking information captured as part of an AR session. There are two ways to access <code>ARFrame</code> objects produced by an AR session, depending on whether your app favors a pull or a push design pattern.</p>

<ul>
<li><em>Pull Pattern</em>: get <code>currentFrame</code> from <code>ARSession</code>.</li>
<li><em>Push Pattern</em>: implement the <code>session(_:didUpdate:)</code> delegate method, and the session will call it once for each video frame it captures (at 60 frames per second by default).</li>
</ul>


<p>Key infos in <code>ARFrame</code>:</p>

<ol>
<li><p><strong><code>ARCamera</code></strong>: Information about the camera position and imaging characteristics for a captured video frame in an AR session. Get <code>camera</code> from <code>ARFrame</code>.</p></li>
<li><p><strong><code>ARLightEstimate</code></strong>: Estimated scene lighting information associated with a captured video frame in an AR session. Get <code>lightEstimate</code> from <code>ARFrame</code>.</p>

<p> Refer to <a href="https://blog.markdaws.net/arkit-by-example-part-4-realism-lighting-pbr-b9a0bedb013e">ARKit by Example — Part 4: Realism - Lighting &amp; PBR</a> for mimicing the environment light.</p></li>
</ol>


<p><a id="markdown-hittest-for-real-world-position" name="hittest-for-real-world-position"></a></p>

<h3>HitTest for Real World Position</h3>

<p>By calling the following method on <code>ARSCNView</code>,</p>

<pre><code class="swift">open func hitTest(_ point: CGPoint, types: ARHitTestResult.ResultType) -&gt; [ARHitTestResult]
</code></pre>

<p>we can get an array of <code>ARHitTestResult</code>, which stay at the very position point indicates. The <code>ARHitTestResult</code>s are sorted by distance. To call the method, you need to specify the <code>ARHitTestResult.ResultType</code>. There are four kinds of hitTest types:</p>

<p><a id="markdown-existingplane" name="existingplane"></a></p>

<h4><code>existingPlane</code></h4>

<p>Return the result type from intersecting with an existing plane anchor.</p>

<p><img src="/images/ARKitHitTestExistingPlane.gif" alt="arkit hittest existing plane" /></p>

<p><a id="markdown-existingplaneusingextent" name="existingplaneusingextent"></a></p>

<h4><code>existingPlaneUsingExtent</code></h4>

<p>Return the result type from intersecting with an existing plane anchor, taking into account the plane’s extent.</p>

<p><a id="markdown-estimatedhorizontalplane" name="estimatedhorizontalplane"></a></p>

<h4><code>estimatedHorizontalPlane</code></h4>

<p>Return the result type from intersecting a horizontal plane estimate, determined for the current frame.</p>

<p><img src="/images/ARKitHitTestEstimatedPlane.gif" alt="arkit hittest estimated plane" /></p>

<p><a id="markdown-featurepoint" name="featurepoint"></a></p>

<h4><code>featurePoint</code></h4>

<p>Return the result type from intersecting the nearest feature point.</p>

<p><img src="/images/ARKitHitTestFeaturePoints.gif" alt="" /></p>

<p><a id="markdown-display-virtual-object-in-real-world" name="display-virtual-object-in-real-world"></a></p>

<h3>Display Virtual Object in Real World</h3>

<p><a id="markdown-standard-view" name="standard-view"></a></p>

<h4>Standard View</h4>

<ul>
<li><strong><code>ARSCNView</code></strong>: A view for displaying AR experiences that augment the camera view with 3D SceneKit content.</li>
<li><strong><code>ARSKView</code></strong>: A view for displaying AR experiences that augment the camera view with 2D SpriteKit content.</li>
</ul>


<p><a id="markdown-custom-view" name="custom-view"></a></p>

<h4>Custom View</h4>

<p>To display your AR experience in a custom view, you’ll need to:</p>

<ol>
<li>Retrieve video frames and tracking information from the session.</li>
<li>Render those frame images as the backdrop for your view.</li>
<li>Use the tracking information to position and draw AR content atop the camera image.</li>
</ol>


<p>Refer to <a href="https://developer.apple.com/documentation/arkit/displaying_an_ar_experience_with_metal">Displaying an AR Experience with Metal</a>.</p>

<p><a id="markdown-best-practices-and-limitations" name="best-practices-and-limitations"></a></p>

<h2>Best Practices and Limitations</h2>

<p><a id="markdown-best-practices" name="best-practices"></a></p>

<h3>Best Practices</h3>

<p>World tracking is an inexact science. This process can often produce impressive accuracy, leading to realistic AR experiences. However, it relies on details of the device’s physical environment that are not always consistent or are difficult to measure in real time without some degree of error. To build high-quality AR experiences, be aware of these caveats and tips.</p>

<p>Refer to <a href="https://developer.apple.com/documentation/arkit/about_augmented_reality_and_arkit">About Augmented Reality and ARKit</a>.</p>

<p><strong>Design AR experiences for predictable lighting conditions.</strong></p>

<p> World tracking involves image analysis, which requires a clear image. Tracking quality is reduced when the camera can’t see details, such as when the camera is pointed at a blank wall or the scene is too dark.</p>

<p><strong>Use tracking quality information to provide user feedback.</strong></p>

<p> World tracking correlates image analysis with device motion. ARKit develops a better understanding of the scene if the device is moving, even if the device moves only subtly. Excessive motion—too far, too fast, or shaking too vigorously—results in a blurred image or too much distance for tracking features between video frames, reducing tracking quality. The ARCamera class provides tracking state reason information, which you can use to develop UI that tells a user how to resolve low-quality tracking situations.</p>

<p><strong>Allow time for plane detection to produce clear results, and disable plane detection when you have the results you need.</strong></p>

<p> Plane detection results vary over time—when a plane is first detected, its position and extent may be inaccurate. As the plane remains in the scene over time, ARKit refines its estimate of position and extent. When a large flat surface is in the scene, ARKit may continue changing the plane anchor’s position, extent, and transform after you’ve already used the plane to place content.</p>

<p><a id="markdown-limitations" name="limitations"></a></p>

<h3>Limitations</h3>

<ol>
<li>For a moving object, ARKit can not give an usable world position of it.</li>
<li>You can not put a virtual object behind a real object. This leads to some problems, like:

<ol>
<li>When an real object move in front of an virtual object, the virtual object will still be displayed in front of the real object.</li>
<li>You can not hold a virtual object coolly, as the virtual object you are trying to hold can not be behind your fingers.</li>
</ol>
</li>
</ol>


<p>Two thoughts that may be help on the second limitation:</p>

<ul>
<li>Segment the camera image based on the feature point with world position. Draw further real object, and then virtual object, and at last, the nearest real object. However, as the feature point is sparse (performance consideration), some edge detection algorithms are needed for accurate edges of objects.</li>
<li>Based on the dual camera, we can get depth of each pixel of the camera image. This will help on image segmentation.</li>
</ul>


<p><a id="markdown-ar-from-google" name="ar-from-google"></a></p>

<h2>AR from Google</h2>

<p><a href="https://developers.google.com/tango/?hl=zh-cn">Tango</a> is a platform that uses computer vision to give devices the ability to understand their position relative to the world around them. But Tango requires very special hardware to run on. So <a href="https://developers.google.com/ar/">ARCore</a> comes.</p>

<blockquote><p>The Tango project will be deprecated on March 1st, 2018. Google is continuing AR development with ARCore, a new platform designed for building augmented reality apps for a broad range of devices without the requirement for specialized hardware.</p></blockquote>

<p>ARCore is a platform for building augmented reality apps on Android. ARCore is designed to work on a wide variety of qualified Android phones running N and later. During the developer <strong>preview</strong>, ARCore supports the following devices:</p>

<ul>
<li>Google Pixel, Pixel XL, Pixel 2, Pixel 2 XL</li>
<li>Samsung Galaxy S8 (SM-G950U, SM-G950N, SM-G950F, SM-G950FD, SM-G950W, SM-G950U1)</li>
</ul>

]]></content>
  </entry>
  
</feed>
