---
layout: post
title: "程序员的数学基础课（黄申）"
date: 2022-05-31 15:48:14 +0800
comments: true
categories: [algorithm, machine learning, python]
---

<!-- more -->


## 01-开篇词 (1讲)
### 00丨开篇词丨作为程序员，为什么你应该学好数学？
## 02-导读 (1讲)
### 00丨导读：程序员应该怎么学数学？
## 03-基础思想篇 (18讲)
### 01丨二进制：不了解计算机的源头，你学什么编程
### 02丨余数：原来取余操作本身就是个哈希函数
### 03丨迭代法：不用编程语言的自带函数，你会如何计算平方根？
### 04丨数学归纳法：如何用数学归纳提升代码的运行效率？
### 05丨递归（上）：泛化数学归纳，如何将复杂问题简单化？
### 06丨递归（下）：分而治之，从归并排序到MapReduce
### 07丨排列：如何让计算机学会“田忌赛马”？
### 08丨组合：如何让计算机安排世界杯的赛程？
### 09丨动态规划（上）：如何实现基于编辑距离的查询推荐？
### 10丨动态规划（下）：如何求得状态转移方程并进行编程实现？
### 11丨树的深度优先搜索（上）：如何才能高效率地查字典？
### 12丨树的深度优先搜索（下）：如何才能高效率地查字典？
### 13丨树的广度优先搜索（上）：人际关系的六度理论是真的吗？
### 14丨树的广度优先搜索（下）：为什么双向广度优先搜索的效率更高？
### 15丨从树到图：如何让计算机学会看地图？
### 16丨时间和空间复杂度（上）：优化性能是否只是“纸上谈兵”？
### 17丨时间和空间复杂度（下）：如何使用六个法则进行复杂度分析？
### 18丨总结课：数据结构、编程语句和基础算法体现了哪些数学思想？
## 04-概率统计篇 (14讲)
### 19丨概率和统计：编程为什么需要概率和统计？
### 20丨概率基础（上）：一篇文章帮你理解随机变量、概率分布和期望值
#### 随机变量
#### 概率分布
1. 离散概率分布：伯努利分布、分类分布、二项分布、泊松分布
    1. 伯努利分布：二分类分布
    2. 分类分布：随机变量的取值空间为`n`个离散的值，`n=2`时就是伯努利分布。
2. 连续概率分布：正态分布、均匀分布、指数分布、拉普拉斯分布
    1. 正态分布：也叫高斯分布，有两个关键参数：均值和方差。

#### 期望值
均值是期望值的特例，即各个取值的概率相同。

### 21丨概率基础（下）：联合概率、条件概率和贝叶斯法则，这些概率公式究竟能做什么？
#### 联合概率
由多个随机变量决定的概率我们就叫联合概率，使用`P(x, y)`表示。
#### 边缘概率
联合概率和单个随机变量的概率之间有什么关联呢？对于离散型随机变量，我们可以通过通过联合概率`P(x, y)`在`y`上求和，就可以得到`P(x)`。对于连续型随机变量，我们可以通过联合概率`P(x, y)`在`y`上的积分，推导出概率`P(x)`。这个时候，我们称`P(x)`为**边缘概率**。
#### 条件概率
条件概率也是由多个随机变量决定，但是和联合概率不同的是，它计算了给定某个（或多个）随机变量的情况下，另一个（或多个）随机变量出现的概率，其概率分布叫做条件概率分布。给定随机变量`x`，随机变量`y`的条件概率使用`P(y|x)`表示。

#### 贝叶斯法则
条件概率、联合概率之间的关系如下：
`P(x,y) = P(x|y) * P(y)`

根据上面的关系，可以得到贝叶斯定理如下：

![20220531贝叶斯定理.png](/images/20220531贝叶斯定理.png)

1. 先验概率： 我们把`P(x)`称为先验概率。之所以称为“先验”，是因为它是从数据资料统计得到的，不需要经过贝叶斯定理的推算。
2. 条件概率（似然函数）：`P(y|x)`是给定`x`之后`y`出现的条件概率。在统计学中，我们也把`P(y|x)`写作似然函数`L(x|y)`。在数学里，似然函数和概率是有区别的。概率是指已经知道模型的参数来预测结果，而似然函数是根据观测到的结果数据，来预估模型的参数。不过，当`y`值给定的时候，两者在数值上是相等的，在应用中我们可以不用细究。
3. 边缘概率：我们没有必要事先知道`P(y)`。`P(y)`可以通过联合概率`P(x,y)`计算边缘概率得来，而联合概率`P(x,y)`可以由`P(y|x)`*`P(x)`推出。
4. 后验概率：`P(x|y)`是根据贝叶斯定理，通过先验概率`P(x)`、似然函数`P(y|x)`和边缘概率`P(y)`推算而来，因此我们把它称作后验概率。

如果有一定数量的标注数据，那么通过统计的方法，我们可以很方便地得到先验概率和似然函数，然后推算出后验概率，最后依据后验概率来做预测。这整个过程符合监督式机器学习的模型训练和新数据预测这两个阶段，因此朴素贝叶斯算法被广泛运用在机器学习的分类问题中。

#### 随机变量之间的独立性
如果随机变量`x`和`y`之间不相互影响，那么我们就说`x`和`y`相互独立。此时，有`P(x|y)=P(x)`，所以`P(x,y)=P(x)*P(y)`。

变量之间的独立性，可以帮我们简化计算。

举个例子，假设有6个随机变量，而每个变量有10种可能的取值，那么计算它们的联合概率`p(x1,x2,x3,x4,x5,x6)`，在实际中是非常困难的一件事情。

根据排列，可能的联合取值，会达到10的6次方，也就是100万这么多。那么使用实际的数据进行统计时，我们也至少需要这个数量级的样本，否则的话很多联合概率分布的值就是0，产生了数据稀疏的问题。但是，如果假设这些随机变量都是相互独立的，那么我们就可以将联合概率`p(x1,x2,x3,x4,x5,x6)`转换为`p(x1)*p(x2)*p(x3)*p(x4)*p(x5)*p(x6)`。如此一来，我们只需要计算`p(x1)`到`p(x6)`就行了。

### 22丨朴素贝叶斯：如何让计算机学会自动分类？
#### 训练样本
贝叶斯分类需要的训练样本如下：
![20220531训练样本](/images/20220531训练样本.png)

#### 训练
贝叶斯定理的核心思想：**用先验概率和条件概率估计后验概率**。

具体到这里的分类问题，贝叶斯公式可以写成这样：

![20220531贝叶斯分类公式.png](/images/20220531贝叶斯分类公式.png)

其中`c`表示一个分类（class）, `f`表示属性对应的数据字段（field）。如此一来，等号左边的`P(c|f)`就是待分类样本中，出现属性值`f`时，样本属于类别`c`的概率。而等号右边的`P(f|c)`是根据训练数据统计，得到分类`c`中出现属性`f`的概率。`P(c)`是分类`c`在训练数据中出现的概率，`P(f)`是属性`f`在训练样本中出现的概率。

这里的贝叶斯公式只描述了单个属性值属于某个分类的概率，可是我们要分析的水果每个都有很多属性，**朴素贝叶斯**在这里就要发挥作用了。这是基于一个简单假设建立的一种贝叶斯方法，并**假定数据对象的不同属性对其归类影响时是相互独立的**。此时若数据对象`o`中同时出现属性`fi`与`fj`，则对象`o`属于类别`c`的概率就是这样：

![20220531朴素贝叶斯分类公式.png](/images/20220531朴素贝叶斯分类公式.png)

现在，我们应该已经可以用10个水果的数据，来建立朴素贝叶斯模型了。

比如，苹果的分类中共包含3个数据实例，对于形状而言，出现2次不规则圆、1次圆形和0次椭圆形，因此各自的统计概率为0.67、0.33和0.00。我们将这些值称为，给定一个水果分类时，出现某个属性值的**条件概率**。以此类推，所有的统计结果就是下面这个表格中这样：

![20220531贝叶斯训练样本条件概率.png](/images/20220531贝叶斯训练样本条件概率.png)

> 对于上表中出现的0.00概率，在做贝叶斯公式中的乘积计算时，会出现结果为0的情况，因此我们通常取一个比这个数据集里最小统计概率还要小的极小值，来代替“零概率”。比如，我们这里取0.01。在填充训练数据中从来没有出现过的属性值的时候，我们就会使用这种技巧，我们给这种技巧起个名字就叫作**平滑**（Smoothing）。

#### 预测
有了这些条件概率，以及各类水果和各个属性出现的先验概率，我们已经建立起了朴素贝叶斯模型。现在，我们就可以用它进行朴素贝叶斯分类了。

假设我们有一个新的水果，它的形状是圆形，口感是甜的，那么根据朴素贝叶斯，它属于苹果、甜橙和西瓜的概率分别是多少呢？

我们先来计算一下，它属于苹果的概率有多大:

![20220531贝叶斯分类预测苹果概率.png](/images/20220531贝叶斯分类预测苹果概率.png)

其中，`apple`表示分类为苹果，`shape-2`表示形状属性的值为`2`（也就是圆形），`taste-2`表示口感属性的值为`2`。以此类推，我们还可计算该水果属于甜橙和西瓜的概率:

![20220531贝叶斯分类预测橙子西瓜概率.png](/images/20220531贝叶斯分类预测橙子西瓜概率.png)

比较这三个数值，`0.00198<0.00798<0.26934`，所以计算机可以得出的结论，该水果属于甜橙的可能性是最大的，或者说，这个水果最有可能是甜橙。

> 这几个公式里的概率乘积通常都非常小，在物品的属性非常多的时候，这个乘积可能就小到计算机无法处理的地步。因此，在实际运用中，我们还会采用一些数学手法进行转换（比如取`log`将小数转换为绝对值大于`1`的负数），原理都是一样的。

#### 总结
总结一次朴素贝叶斯分类的主要步骤：

1. 准备数据：针对水果分类这个案例，我们收集了若干水果的实例，并从水果的常见属性入手，将其转化为计算机所能理解的数据。这种数据也被称为**训练样本**。
2. 建立模型：通过手头上水果的实例，我们让计算机统计每种水果、属性出现的先验概率，以及在某个水果分类下某种属性出现的条件概率。这个过程也被称为基于样本的**训练**。
3. 分类新数据：对于一颗新水果的属性数据，计算机根据已经建立的模型进行推导计算，得到该水果属于每个分类的概率，实现了分类的目的。这个过程也被称为**预测**。

### 23丨文本分类：如何区分特定类型的新闻？
运用朴素贝叶斯原理，根据词频特征，对文章进行分类。清晰明了，值得一看。

### 24丨语言模型：如何使用链式法则和马尔科夫假设简化概率模型？
#### 语言模型
这里说的语言模型指的是基于概率和统计的语言模型。
##### 链式法则

![20220531链式法则.png](/images/20220531链式法则.png)

##### 马尔可夫假设

理解了链式法则，我们再来看看马尔可夫假设。这个假设的内容是：任何一个词`wi`出现的概率只和它前面的1个或若干个词有关。基于这个假设，我们可以提出多元文法（Ngram）模型。Ngram中的`N`很重要，它表示任何一个词出现的概率，只和它前面的`N-1`个词有关。

以二元文法模型为例，按照刚才的说法，二元文法表示，某个单词出现的概率只和它前面的1个单词有关。也就是说，即使某个单词出现在一个很长的句子中，我们也只需要看前面那1个单词。用公式来表示出来就是这样：

![20220531二元文法模型.png](/images/20220531二元文法模型.png)

假设我们有一个统计样本文本`d`，`s`表示某个有意义的句子，由一连串按照特定顺序排列的词`w1，w2,…,wn`组成，这里`n`是句子里单词的数量。现在，我们想知道根据文档`d`的统计数据，`s`在文本中出现的可能性，即`P(s|d)`，那么我们可以把它表示为`P(s|d)=P(w1,w2,…,wn|d)`。假设我们这里考虑的都是在集合`d`的情况下发生的概率，所以可以忽略`d`，写为`P(s)=P(w1,w2,…,wn)`。

`P(w1,w2,…,wn)`可以通过上面说的链式法则计算，通过文档集合`C`，你可以知道`P(w1)`，`P(w2|w1)`这种概率。但是，这会带来两个问题：

1. 概率为0的问题
    `P(w1)`大小还好，`P(w2|w1)`会小一些，再往后看，`P(w3|w1,w2)`出现概率更低，`P(w4|w1,w2,w3)`出现的概率就更低了。一直到`P(wn|w1,w2,…,wn−1)`，基本上又为0了。我们可以使用上一节提到的平滑技巧，减少0概率的出现。不过，如果太多的概率都是通过平滑的方式而得到的，那么模型和真实的数据分布之间的差距就会加大，最终预测的效果也会很差，所以平滑也不是解决0概率的最终办法。
2. 存储空间的问题
    为了统计现有文档集合中`P(w1,w2,…,wn)`这类值，我们就需要生成很多的计数器。我们假设文档集合中有`m`个不同的单词，那么从中挑出`n`个单词的可重复排列，数量就是`m^n`。此外，还有`m^(n−1)`,`m^(n−2)`等等。这也意味着，如果要统计并存储的所有`P(w1,w2,…,wn)`或`P(wn|w1,w2,…,wn−1)`这类概率，就需要大量的内存和磁盘空间。当然，你可以做一些简化，不考虑单词出现的顺序，那么问题就变成了可重复组合，但是数量仍然非常巨大。

在这两个问题上，马尔科夫假设和多元文法模型就能帮上大忙了。如果我们使用三元文法模型，上述公式可以改写为：

![20220531三元文法模型.png](/images/20220531三元文法模型.png)

这样，系统的复杂度大致在`(C(m,1)+C(m,2)+C(m,3))`这个数量级，而且`P(wn|wn−2,wn−1)`为0的概率也会大大低于`P(wn|w1,w2,…,wn−1)`为0的概率。

#### 语言模型的应用
基于概率的语言模型，本身不是新兴的技术。它已经在机器翻译、语音识别和中文分词中得到了成功应用。近几年来，人们也开始在信息检索领域中尝试语言模型。下面我就来讲讲语言模型在信息检索和中文分词这两个方面里是如何发挥作用的。

##### 信息检索
信息检索很关心的一个问题就是相关性，也就是说，给定一个查询，哪篇文档是更相关的呢？一种常见的做法是计算`P(d|q)`，其中`q`表示一个查询，`d`表示一篇文档。`P(d|q)`表示用户输入查询`q`的情况下，文档`d`出现的概率是多少？如果这个概率越高，我们就认为`q`和`d`之间的相关性越高。

通过我们手头的文档集合，并不能直接获得`P(d|q)`。好在我们已经学习过了贝叶斯定理，通过这个定理，我们可以将`P(d|q)`重写如下：

![20220531信息检索贝叶斯公式.png](/images/20220531信息检索贝叶斯公式.png)

让`k1,k2,…,kn`表示查询`q`里包含的`n`个关键词，就可以根据链式法则求解出`P(q|d)`，我们也使用马尔科夫假设和多元文法来提高算法效率。

最终，当用户输入一个查询`q`之后，对于每一篇文档`d`，我们都能获得`P(d|q)`的值。根据每篇文档所获得的`P(d|q)`这个值，由高到低对所有的文档进行排序。这就是语言模型在信息检索中的常见用法。

##### 中文分词
和拉丁语系不同，中文存在分词的问题。比如原句是“兵乓球拍卖完了”，分词结果可能是：

```
1. 兵乓球|拍卖|完了
2. 兵乓|球拍|卖完|了
3. ...
```

上面分词的例子，从字面来看都是合理的，所以这种歧义无法通过这句话本身来解决。那么这种情况下，语言模型能为我们做什么呢？我们知道，语言模型是基于大量的语料来统计的，所以我们可以使用这个模型来估算，哪种情况更合理。

假设整个文档集合是`D`，要分词的句子是`s`，分词结果为`w1,…wn`，如果使用三元文法模型，

![20220531中文分词三元文法模型.png](/images/20220531中文分词三元文法模型.png)

> 请注意，在信息检索中，我们关心的是每篇文章产生一个句子（也就是查询）的概率，而这里可以是整个文档集合`D`产生一个句子的概率。

语言模型可以帮我们估计某种分词结果，在文档集合中出现的概率。由于不同的分词方法，会导致`w1`到`wn`的不同，因此就会产生不同的`P(s)`。接下来，我们只要取最大的`P(s)`，并假设这种分词方式是最合理的，就可以在一定程度上解决歧义。

回到“兵乓球拍卖完了”这句话，如果文档集合都是讲述的有关体育用品的销售，而不是拍卖行，那么“兵乓|球拍|卖完|了”这种分词的可能性应该更高。

### 25丨马尔科夫模型：从PageRank到语音识别，背后是什么模型在支撑？
#### 马尔可夫模型
在介绍语言模型的时候，我们提到了马尔科夫假设，这个假设是说，每个词出现的概率和之前的一个或若干个词有关。我们换个角度思考就是，**每个词按照一定的概率转移到下一个词**。

如果把词抽象为一个状态，那么我们就可以认为，状态到状态之间是有关联的。前一个状态有一定的概率可以转移到到下一个状态。如果多个状态之间的随机转移满足马尔科夫假设，那么这类随机过程就是一个马尔科夫随机过程。而刻画这类随机过程的统计模型，就是**马尔科夫模型**（Markov Model）。

前面讲多元文法的时候，我提到了二元文法、三元文法。对于二元文法来说，某个词出现的概率只和前一个词有关。对应的，在马尔科夫模型中，如果一个状态出现的概率只和前一个状态有关，那么我们称它为**一阶马尔科夫模型**或者**马尔科夫链**。对应于三元、四元甚至更多元的文法，我们也有二阶、三阶等马尔科夫模型。

##### PageRank和马尔可夫链
Google公司最引以为傲的PageRank链接分析算法，它的核心思想就是基于马尔科夫链。这个算法假设了一个“随机冲浪者”模型，冲浪者从某张网页出发，根据Web图中的链接关系随机访问。在每个步骤中，冲浪者都会从当前网页的链出网页中随机选取一张作为下一步访问的目标。在整个Web图中，绝大部分网页节点都会有链入和链出。那么冲浪者就可以永不停歇地冲浪，持续在图中走下去。我们可以假设每张网页就是一个状态，而网页之间的链接表明了状态转移的方向。这样，我们很自然地就可以使用马尔科夫链来刻画“随机冲浪者”。

> 1. PageRank值：在随机访问的过程中，越是被频繁访问的链接，越是重要。可以看出，每个节点的PageRank值取决于Web图的链接结构。**假如一个页面节点有很多的链入链接，或者是链入的网页有较高的被访问率，那么它也将会有更高的被访问概率**。
> 2. PageRank在标准的马尔科夫链上，引入了随机的跳转操作，也就是假设冲浪者不按照Web图的拓扑结构走下去，只是随机挑选了一张网页进行跳转。这样的处理是类比人们打开一张新网页的行为，也是符合实际情况的，避免了信息孤岛的形成。
#### 隐马尔可夫模型
马尔可夫模型都是假设每个状态对我们都是已知的，比如在概率语言模型中，一个状态对应了单词“上学”，另一个状态对应了单词“书包”。可是，有没有可能某些状态我们是未知的呢？

在某些现实的应用场景中，我们是无法确定马尔科夫过程中某个状态的取值的。这种情况下，最经典的案例就是**语音识别**。使用概率对语音进行识别的过程，和语言模型类似，因此我们可以把每个等待识别的词对应为马尔科夫过程中的一个状态。

计算机只知道某个词的发音，而不知道它具体怎么写，对于这种情况，我们就认为计算机只能观测到每个状态的部分信息，而另外一些信息被“隐藏”了起来。这个时候，我们就需要用隐马尔科夫模型来解决这种问题。隐马尔科夫模型有两层，一层是我们可以观测到的数据，称为“输出层”，另一层则是我们无法直接观测到的状态，称为“隐藏状态层”。如下图：

![20220531隐马尔可夫模型图.jpeg](/images/20220531隐马尔可夫模型图.jpeg)

那么在这个两层模型示例中，“隐藏状态层”(x1，x2，x3)产生“输出层”(y1，y2，y3)的概率是:

![20220531隐马尔可夫模型输出层概率.png](/images/20220531隐马尔可夫模型输出层概率.png)

语音识别要做的，就是遍历所有可能的状态层，找出最可能产生已知“输出层”的状态层，即为语音识别结果。

### 26丨信息熵：如何通过几个问题，测出你对应的武侠人物？
### 27丨决策树：信息增益、增益比率和基尼指数的运用
### 28丨熵、信息增益和卡方：如何寻找关键特征？
### 29丨归一化和标准化：各种特征如何综合才是最合理的？
### 30丨统计意义（上）：如何通过显著性检验，判断你的A-B测试结果是不是巧合？
### 31丨统计意义（下）：如何通过显著性检验，判断你的A-B测试结果是不是巧合？
### 32丨概率统计篇答疑和总结：为什么会有欠拟合和过拟合？
## 05-线性代数篇 (13讲)
### 33丨线性代数：线性代数到底都讲了些什么？
### 34丨向量空间模型：如何让计算机理解现实事物之间的关系？
### 35丨文本检索：如何让计算机处理自然语言？
### 36丨文本聚类：如何过滤冗余的新闻？
### 37丨矩阵（上）：如何使用矩阵操作进行PageRank计算？
### 38丨矩阵（下）：如何使用矩阵操作进行协同过滤推荐？
### 39丨线性回归（上）：如何使用高斯消元求解线性方程组？
### 40丨线性回归（中）：如何使用最小二乘法进行直线拟合？
### 41丨线性回归（下）：如何使用最小二乘法进行效果验证？
### 42丨PCA主成分分析（上）：如何利用协方差矩阵来降维？
### 43丨PCA主成分分析（下）：为什么要计算协方差矩阵的特征值和特征向量？
### 44丨奇异值分解：如何挖掘潜在的语义关系？
### 45丨线性代数篇答疑和总结：矩阵乘法的几何意义是什么？
## 06-综合应用篇 (6讲)
### 46丨缓存系统：如何通过哈希表和队列实现高效访问？
### 47丨搜索引擎（上）：如何通过倒排索引和向量空间模型，打造一个简单的搜索引擎？
### 48丨搜索引擎（下）：如何通过查询的分类，让电商平台的搜索结果更相关？
### 49丨推荐系统（上）：如何实现基于相似度的协同过滤？
### 50丨推荐系统（下）：如何通过SVD分析用户和物品的矩阵？
### 51丨综合应用篇答疑和总结：如何进行个性化用户画像的设计？
## 07-加餐 (3讲)
### 数学专栏课外加餐（一）丨我们为什么需要反码和补码？
### 数学专栏课外加餐（三）：程序员需要读哪些数学书？
### 数学专栏课外加餐（二）丨位操作的三个应用实例
## 08-结束语 (1讲)
### 结束语丨从数学到编程，本身就是一个很长的链条

